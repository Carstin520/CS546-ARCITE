{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_EukgiYyD7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef652543-1b1d-4770-94ad-7dae6fabd5f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting instructor\n",
            "  Downloading instructor-0.4.0-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from instructor)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting openai<2.0.0,>=1.1.0 (from instructor)\n",
            "  Downloading openai-1.3.6-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting pydantic<3.0.0,>=2.0.2 (from instructor)\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.9.0)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.1.0->instructor)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.1.0->instructor) (4.5.0)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic<3.0.0,>=2.0.2->instructor)\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic<3.0.0,>=2.0.2->instructor)\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting typing-extensions<5,>=4.5 (from openai<2.0.0,>=1.1.0->instructor)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.9.0->instructor) (8.1.7)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai<2.0.0,>=1.1.0->instructor) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor) (2023.7.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.1.0->instructor)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-0.4.0-py3-none-any.whl (25 kB)\n",
            "Downloading openai-1.3.6-py3-none-any.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: typing-extensions, h11, docstring-parser, annotated-types, pydantic-core, httpcore, pydantic, httpx, openai, instructor\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed annotated-types-0.6.0 docstring-parser-0.15 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 instructor-0.4.0 openai-1.3.6 pydantic-2.5.2 pydantic-core-2.14.5 typing-extensions-4.8.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting openai<1.0.0\n",
            "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (1.3.1)\n",
            "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.3.6\n",
            "    Uninstalling openai-1.3.6:\n",
            "      Successfully uninstalled openai-1.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "instructor 0.4.0 requires openai<2.0.0,>=1.1.0, but you have openai 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Collecting openai\n",
            "  Using cached openai-1.3.6-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.5.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.5 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.14.5)\n",
            "Using cached openai-1.3.6-py3-none-any.whl (220 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.1\n",
            "    Uninstalling openai-0.28.1:\n",
            "      Successfully uninstalled openai-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-1.3.6\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uqqq pip\n",
        "!pip install -qqq torch==2.0.1\n",
        "!pip install -qqq arxiv==1.4.7\n",
        "!pip install instructor\n",
        "!pip install \"openai<1.0.0\"\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOlwkeztVOzt"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "model_engine = \"gpt-4\"\n",
        "client = OpenAI(api_key=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"With the current growth of big data in today’s modern society, the necessity for efficient data processing is a cardinal requisite for deriving insightful analyses. As part, various parallel data processing frameworks have been developed to try and improve the efficiency of runtimes. Two well-known parallel processing frameworks are Modin and Dask. \"\n",
        "\n",
        "\"Recent advances in language model pre-training have shown that models such as BERT, RoBERTa and T5 store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on.\"\n",
        "\n",
        "\"Modin and Dask have drawn notice among the plethora of existing frameworks because of their usability and scalability. These frameworks, which promise improved performance over conventional single-threaded techniques like Pandas, were designed to speed up data processing operations by dividing the calculations across numerous cores or nodes. As a result, they are able to handle and improve the runtimes of data sizing anywhere from tens of gigabytes to terabytes.\""
      ],
      "metadata": {
        "id": "-wqrhOOcF6Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_text_for_citations(input_text, client, citation_style=\"APA style\"):\n",
        "    \"\"\"\n",
        "    Analyzes the text to identify parts that need citations.\n",
        "\n",
        "    :param input_text: String containing the text to be analyzed.\n",
        "    :param client: The OpenAI API client object.\n",
        "    :return: Modified text with indexed terms/sentences and an assessment.\n",
        "    \"\"\"\n",
        "    # Prepare message for GPT-4\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"\n",
        "                Analyze the provided text. ONLY Identify specific terminologies and sentences that ABSOLUTELY require citations to support their claims or information.\n",
        "                Treat each term separately, ensuring that different terms are not mixed together when determining the need for citations.\n",
        "                If a term is an acronym, include an explanation of what it stands for in the assessment.\n",
        "                Assign a unique index to each, distinguishing between 'Term' and 'Sentence' (e.g., [Term 0], [Sent 0]).\n",
        "                For each identified term and sentence, provide a brief explanation of why a citation is necessary to validate or support the information or claim.\n",
        "\n",
        "                TThe output should include the original text with {citation_style} in-text citation placeholders for each identified term and sentence, and a separate assessment section detailing the reasoning behind the citation requirements.\n",
        "\n",
        "\n",
        "                Text: {input_text}\n",
        "\n",
        "                Please format the output as follows:\n",
        "                1. Modified Text with Citation Placeholders:\n",
        "                  [The text with in-text placeholders like [Term 0] or [Sent 0] at relevant positions.]\n",
        "\n",
        "                2. Assessment of Citation Requirements:\n",
        "                  For each Term/Sentence:\n",
        "                  [Term/Sentence Index] : Brief explanation of why a citation is necessary for the term or sentence.\n",
        "            \"\"\"\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": input_text}\n",
        "    ]\n",
        "\n",
        "    # Call GPT-4 for analysis\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Extract the response\n",
        "    response = completion.choices[0].message.content\n",
        "    return response\n",
        "input_text = \"Recent advances in language model pre-training have shown that models such as BERT, RoBERTa and T5 store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on.\"\n",
        "step1_out = analyze_text_for_citations(input_text, client)\n",
        "print(step1_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKix14Aka3Cw",
        "outputId": "f0f3c5bd-c56b-415f-c026-5dac0b72fa11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Modified Text with Citation Placeholders:\n",
            "Recent advances in language model pre-training [Sent 0] have shown that models such as BERT [Term 0], RoBERTa [Term 1] and T5 [Term 2] store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on [Sent 1].\n",
            "\n",
            "2. Assessment of Citation Requirements:\n",
            "For each Term/Sentence:\n",
            "\n",
            "[Term 0] : A citation is necessary for the term \"BERT\" to provide a reference to the original work where this model was introduced and described.\n",
            "\n",
            "[Term 1] : A citation is necessary for the term \"RoBERTa\" to provide a reference to the original work where this model was introduced and described.\n",
            "\n",
            "[Term 2] : A citation is necessary for the term \"T5\" to provide a reference to the original work where this model was introduced and described.\n",
            "\n",
            "[Sent 0] : A citation is necessary for the sentence \"Recent advances in language model pre-training have shown that models such as BERT, RoBERTa and T5 store a surprising amount of world knowledge\" to support the claim about the capabilities of these models and the advances in language model pre-training.\n",
            "\n",
            "[Sent 1] : A citation is necessary for the sentence \"acquired from the massive text corpora they are trained on\" to provide a reference to the original work where this method of training was described and the results were reported.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7yO33QEXbMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_arxiv_search_queries(analysis_result, client):\n",
        "    \"\"\"\n",
        "    Generates search queries for arXiv based on the analysis result.\n",
        "\n",
        "    :param analysis_result: The output from analyze_text_for_citations.\n",
        "    :return: A dictionary of search queries for each term/sentence.\n",
        "    \"\"\"\n",
        "    queries = {}\n",
        "    # GPT-4 prompt for analyzing text\n",
        "    max_attempts = 3  # Set a maximum number of attempts to avoid infinite loops\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "      prompt = f\"\"\"\n",
        "        Analyze the following text and determine the appropriate arXiv.search categories for citations.\n",
        "        The arXiv.search categories and the keywords should be relevant to the subjects mentioned in the text.\n",
        "        List the arXiv.search categories for each term or sentence that requires a citation. (e.g., cs.AI, cs.CC, eess.AS)\n",
        "        List the keywords for each term or sentence that requires a citation:\n",
        "\n",
        "        Text: {analysis_result}\n",
        "\n",
        "        Please format the output as follows:\n",
        "        [Term/Sentence Index] Categories: List of valid arXiv.search categories ; Keywords: list of relevant keywords\n",
        "      \"\"\"\n",
        "      print(prompt)\n",
        "      # Call GPT-4 API for analysis\n",
        "      response = client.chat.completions.create(\n",
        "          model=\"gpt-4\",\n",
        "          messages=[{\"role\": \"system\", \"content\": prompt}],\n",
        "          temperature=0\n",
        "      )\n",
        "\n",
        "      # Extracting the response text\n",
        "      analysis_result = response.choices[0].message.content.strip()\n",
        "      print(analysis_result)\n",
        "\n",
        "      # Parsing logic (to be adapted based on actual format of GPT-4 response)\n",
        "      queries = {}\n",
        "\n",
        "      # Process the analysis_result line by line\n",
        "      for line in analysis_result.split('\\n'):\n",
        "          if  'Categories:' in line and 'Keywords:' in line:\n",
        "              try:\n",
        "                  index, rest = line.split('] Categories:', 1)\n",
        "                  categories_str, keywords_str = rest.split('; Keywords:', 1)\n",
        "                  categories = [cat.strip() for cat in categories_str.split(',')]\n",
        "                  keywords = [keyword.strip() for keyword in keywords_str.split(',')]\n",
        "\n",
        "                  category_query = ' OR '.join(f\"(cat:{cat})\" for cat in categories)\n",
        "                  keywords_query = ' OR '.join(f\"(abs:{keyword})\" for keyword in keywords)\n",
        "                  query = f\"{category_query} AND {keywords_query}\"\n",
        "                  queries[index.strip('[')] = query\n",
        "              except ValueError:\n",
        "                  # If parsing fails, break and try again\n",
        "                  break\n",
        "      else:\n",
        "          # If parsing is successful, return the queries\n",
        "          return queries\n",
        "\n",
        "      # If we reach here, the output format was not as expected, retry\n",
        "\n",
        "    # If all attempts fail, return an error or a default value\n",
        "    return {\"error\": \"Failed to parse GPT-4 response in the required format.\"}\n",
        "\n",
        "queries_arxiv = generate_arxiv_search_queries(step1_out, client)\n",
        "print(queries_arxiv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFFBa8Zfig5X",
        "outputId": "4e478767-7352-4baf-8993-95b9c11e37bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        Analyze the following text and determine the appropriate arXiv.search categories for citations.\n",
            "        The arXiv.search categories and the keywords should be relevant to the subjects mentioned in the text.\n",
            "        List the arXiv.search categories for each term or sentence that requires a citation. (e.g., cs.AI, cs.CC, eess.AS)\n",
            "        List the keywords for each term or sentence that requires a citation:\n",
            "\n",
            "        Text: 1. Modified Text with Citation Placeholders:\n",
            "Recent advances in language model pre-training [Sent 0] have shown that models such as BERT [Term 0], RoBERTa [Term 1] and T5 [Term 2] store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on [Sent 1].\n",
            "\n",
            "2. Assessment of Citation Requirements:\n",
            "For each Term/Sentence:\n",
            "\n",
            "[Term 0] : A citation is necessary for the term \"BERT\" to provide a reference to the original work where this model was introduced and described.\n",
            "\n",
            "[Term 1] : A citation is necessary for the term \"RoBERTa\" to provide a reference to the original work where this model was introduced and described.\n",
            "\n",
            "[Term 2] : A citation is necessary for the term \"T5\" to provide a reference to the original work where this model was introduced and described.\n",
            "\n",
            "[Sent 0] : A citation is necessary for the sentence \"Recent advances in language model pre-training have shown that models such as BERT, RoBERTa and T5 store a surprising amount of world knowledge\" to support the claim about the capabilities of these models and the advances in language model pre-training.\n",
            "\n",
            "[Sent 1] : A citation is necessary for the sentence \"acquired from the massive text corpora they are trained on\" to provide a reference to the original work where this method of training was described and the results were reported.\n",
            "\n",
            "        Please format the output as follows:\n",
            "        [Term/Sentence Index] Categories: List of valid arXiv.search categories ; Keywords: list of relevant keywords\n",
            "      \n",
            "[Term 0] Categories: cs.CL, cs.LG ; Keywords: BERT, language model, pre-training, natural language processing, machine learning\n",
            "\n",
            "[Term 1] Categories: cs.CL, cs.LG ; Keywords: RoBERTa, language model, pre-training, natural language processing, machine learning\n",
            "\n",
            "[Term 2] Categories: cs.CL, cs.LG ; Keywords: T5, language model, pre-training, natural language processing, machine learning\n",
            "\n",
            "[Sent 0] Categories: cs.CL, cs.LG ; Keywords: language model pre-training, BERT, RoBERTa, T5, world knowledge, natural language processing, machine learning\n",
            "\n",
            "[Sent 1] Categories: cs.CL, cs.LG ; Keywords: text corpora, language model training, natural language processing, machine learning\n",
            "{'Term 0': '(cat:cs.CL) OR (cat:cs.LG) AND (abs:BERT) OR (abs:language model) OR (abs:pre-training) OR (abs:natural language processing) OR (abs:machine learning)', 'Term 1': '(cat:cs.CL) OR (cat:cs.LG) AND (abs:RoBERTa) OR (abs:language model) OR (abs:pre-training) OR (abs:natural language processing) OR (abs:machine learning)', 'Term 2': '(cat:cs.CL) OR (cat:cs.LG) AND (abs:T5) OR (abs:language model) OR (abs:pre-training) OR (abs:natural language processing) OR (abs:machine learning)', 'Sent 0': '(cat:cs.CL) OR (cat:cs.LG) AND (abs:language model pre-training) OR (abs:BERT) OR (abs:RoBERTa) OR (abs:T5) OR (abs:world knowledge) OR (abs:natural language processing) OR (abs:machine learning)', 'Sent 1': '(cat:cs.CL) OR (cat:cs.LG) AND (abs:text corpora) OR (abs:language model training) OR (abs:natural language processing) OR (abs:machine learning)'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_semantic_scholar_queries(analysis_result, client):\n",
        "    \"\"\"\n",
        "    Generates search queries for Semantic Scholar based on the analysis result.\n",
        "\n",
        "    :param analysis_result: The output from analyze_text_for_citations.\n",
        "    :return: A dictionary of search queries for each term/sentence.\n",
        "    \"\"\"\n",
        "    queries = {}\n",
        "    max_attempts = 3  # Set a maximum number of attempts to avoid infinite loops\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "      prompt = f\"\"\"\n",
        "          Analyze the following text and generate a search query for Semantic Scholar.\n",
        "          For terms, only include the term itself, unigram.\n",
        "          For sentences, combine relevant keywords and a descriptive sentence for a more effective search in academic search engines.\n",
        "          The keywords and descriptions should be relevant to the subjects mentioned in the text.\n",
        "          Formulate a single, comprehensive search query for each sentence that requires a citation:\n",
        "\n",
        "          Text: {analysis_result}\n",
        "\n",
        "          Please format the output as follows:\n",
        "          [Term/Sentence Index] Query: For terms, use the term itself; for sentences, a single, comprehensive search string combining relevant keywords and a brief descriptive sentence (100 char max).\n",
        "      \"\"\"\n",
        "      print(prompt)\n",
        "      # Call GPT-4 API for analysis\n",
        "      response = client.chat.completions.create(\n",
        "          model=\"gpt-3.5-turbo-1106\",\n",
        "          messages=[{\"role\": \"system\", \"content\": prompt}],\n",
        "          temperature=0\n",
        "      )\n",
        "\n",
        "      # Extracting the response text\n",
        "      analysis_result = response.choices[0].message.content.strip()\n",
        "      print(analysis_result)\n",
        "\n",
        "      # Parsing logic\n",
        "      queries = {}\n",
        "\n",
        "      for line in analysis_result.split('\\n'):\n",
        "          if line.startswith('[') and 'Query:' in line:\n",
        "              try:\n",
        "                  index, query_content = line.split('] Query:', 1)\n",
        "                  query = query_content.strip()\n",
        "                  queries[index.strip('[')] = query\n",
        "              except ValueError:\n",
        "                  # If parsing fails, break and try again\n",
        "                  break\n",
        "      else:\n",
        "          # If parsing is successful, return the queries\n",
        "          return queries\n",
        "\n",
        "      # If we reach here, the output format was not as expected, retry\n",
        "\n",
        "    # If all attempts fail, return an error or a default value\n",
        "    return {\"error\": \"Failed to parse GPT-4 response in the required format.\"}\n",
        "queries_ss = generate_semantic_scholar_queries(step1_out, client)\n",
        "print(queries_ss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QChOnIu4tAX6",
        "outputId": "bde60220-1afb-4f1e-b138-11c391bac959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "          Analyze the following text and generate a search query for Semantic Scholar.\n",
            "          For terms, only include the term itself, unigram.\n",
            "          For sentences, combine relevant keywords and a descriptive sentence for a more effective search in academic search engines.\n",
            "          The keywords and descriptions should be relevant to the subjects mentioned in the text.\n",
            "          Formulate a single, comprehensive search query for each sentence that requires a citation:\n",
            "\n",
            "          Text: 1. Modified Text with Citation Placeholders:\n",
            "Humans are capable of perceiving 3D environment and inferring ego-motion in a short time, but it is hard for an agent to be equipped with similar capabilities [Sent 0]. VO SLAM [Term 0] has been considered as a multi-view geometric problem for decades [Sent 1]. It is traditionally solved by minimizing photometric or geometric reprojection errors and works well in regular environments, but fails in challenging conditions like dynamic objects and abrupt motions [Sent 2]. In light of these limitations, VO has been studied with learning techniques in recent years and many approaches with promising performance have been proposed [Sent 3].\n",
            "\n",
            "2. Assessment of Citation Requirements:\n",
            "For each Term/Sentence:\n",
            "\n",
            "[Term 0] : VO SLAM - This term stands for Visual Odometry Simultaneous Localization and Mapping. A citation is necessary to provide a source that explains this term in detail, as it is a specific technical term that may not be commonly understood.\n",
            "\n",
            "[Sent 0] : \"Humans are capable of perceiving 3D environment and inferring ego-motion in a short time, but it is hard for an agent to be equipped with similar capabilities.\" - This sentence makes a claim about human capabilities and the challenges of equipping an agent with similar capabilities. A citation is necessary to support this claim.\n",
            "\n",
            "[Sent 1] : \"VO SLAM has been considered as a multi-view geometric problem for decades.\" - This sentence makes a claim about the history and nature of VO SLAM. A citation is necessary to support this claim.\n",
            "\n",
            "[Sent 2] : \"It is traditionally solved by minimizing photometric or geometric reprojection errors and works well in regular environments, but fails in challenging conditions like dynamic objects and abrupt motions.\" - This sentence makes a claim about how VO SLAM is traditionally solved and its limitations. A citation is necessary to support this claim.\n",
            "\n",
            "[Sent 3] : \"In light of these limitations, VO has been studied with learning techniques in recent years and many approaches with promising performance have been proposed.\" - This sentence makes a claim about recent research and developments in VO. A citation is necessary to support this claim.\n",
            "\n",
            "          Please format the output as follows:\n",
            "          [Term/Sentence Index] Query: For terms, use the term itself; for sentences, a single, comprehensive search string combining relevant keywords and a brief descriptive sentence (100 char max).\n",
            "      \n",
            "[Term 0] Query: Visual Odometry Simultaneous Localization and Mapping\n",
            "\n",
            "[Sent 0] Query: human perception 3D environment ego-motion challenges agent capabilities\n",
            "\n",
            "[Sent 1] Query: history multi-view geometric problem VO SLAM\n",
            "\n",
            "[Sent 2] Query: traditional VO SLAM limitations photometric geometric reprojection errors dynamic objects abrupt motions\n",
            "\n",
            "[Sent 3] Query: recent years learning techniques VO approaches promising performance\n",
            "{'Term 0': 'Visual Odometry Simultaneous Localization and Mapping', 'Sent 0': 'human perception 3D environment ego-motion challenges agent capabilities', 'Sent 1': 'history multi-view geometric problem VO SLAM', 'Sent 2': 'traditional VO SLAM limitations photometric geometric reprojection errors dynamic objects abrupt motions', 'Sent 3': 'recent years learning techniques VO approaches promising performance'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N667LsdIutV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step1**\n",
        "Subsection Analysis Using GPT-4: Utilize the GPT-4 model to analyze the provided text and identify specific subsections that require citations.\n"
      ],
      "metadata": {
        "id": "v6Etl-r37Dwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_to_dict(text):\n",
        "    # Find the substring starting with \"Output list:\"\n",
        "    output_list_start = text.find(\"Output list:\")\n",
        "    if output_list_start == -1:\n",
        "        return \"Output list not found\"\n",
        "\n",
        "    output_list_text = text[output_list_start:]\n",
        "\n",
        "    # Define the regex pattern to match \"Term X:\" or \"Sent X:\" followed by any text until a comma or end of line\n",
        "    pattern = r'(Term \\d+|Sent \\d+): (.*?)($|, )'\n",
        "\n",
        "    # Find all matches in the output list text\n",
        "    matches = re.findall(pattern, output_list_text)\n",
        "\n",
        "    # Create a dictionary to store the extracted data\n",
        "    extracted_dict = {}\n",
        "\n",
        "    for match in matches:\n",
        "        key, value, _ = match\n",
        "        extracted_dict[key.strip()] = value.strip()\n",
        "\n",
        "    return extracted_dict\n",
        "\n",
        "extract_to_dict(step1_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "yHx5_ru_1Nlq",
        "outputId": "b5f10d68-40ac-4a78-c9b4-3c51494f737e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-50f4e6167e27>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mextracted_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mextract_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep1_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'step1_result' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import re\n",
        "import requests\n",
        "\n",
        "class SemanticScholar:\n",
        "    def __init__(self):\n",
        "        self.api_key = '1ncDOSkmgv5I9Ud73NCt87jcIBnQgxhb4kPzC8DF'\n",
        "        self.base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "    def search(self, query, limit=5):\n",
        "        headers = {\n",
        "            'x-api-key': self.api_key\n",
        "        }\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'fields': 'title,authors,year',\n",
        "            'limit': limit\n",
        "        }\n",
        "        response = requests.get(self.base_url, headers=headers, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            return f\"Error: {response.status_code}\"\n",
        "\n",
        "# Instantiate the SemanticScholar class\n",
        "ss = SemanticScholar()\n",
        "out_dic = {}\n",
        "for key in queries_ss:\n",
        "  out_dic[key] = ss.search(queries_ss[key])\n",
        "\n",
        "\n",
        "print(out_dic)\n",
        "\n",
        "# for key in out_dic:\n",
        "#   print(key)\n",
        "#   for r in out_dic[key]:\n",
        "#     if \"data\" in r:\n",
        "#       print(r[\"total\"])\n",
        "#       for paper in r[\"data\"]:\n",
        "#         print(paper[\"title\"])\n",
        "#         print(paper[\"authors\"])\n",
        "#         print(paper[\"year\"])\n",
        "#     else:\n",
        "#       print(f\"Not found relevent papers to {key}\")\n",
        "\n",
        "# Iterate through the dictionary\n",
        "for key in out_dic:\n",
        "    print(key)\n",
        "    r = out_dic[key]\n",
        "    if \"data\" in r:\n",
        "        print(r[\"total\"])\n",
        "        for paper in r[\"data\"]:\n",
        "            print(paper[\"title\"])\n",
        "            if \"authors\" in paper:  # Check if 'authors' key exists in paper\n",
        "                print(paper[\"authors\"])\n",
        "            else:\n",
        "                print(\"Authors not available\")\n",
        "            print(paper.get(\"year\", \"Year not available\"))  # Use .get() for 'year' to handle missing data\n",
        "    else:\n",
        "        print(f\"Not found relevant papers to {key}\")\n"
      ],
      "metadata": {
        "id": "LADn9A0bwwWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils.text import num_ini_spaces\n",
        "import arxiv\n",
        "search_results = []\n",
        "dic = queries_arxiv\n",
        "num_results = 5\n",
        "out_dic = {}\n",
        "\n",
        "for key in dic:\n",
        "  search = arxiv.Search(\n",
        "        query = dic[key],\n",
        "        max_results = num_results,\n",
        "        sort_by = arxiv.SortCriterion.Relevance,\n",
        "    )\n",
        "  out_dic[key] = search.results()\n",
        "for key in out_dic:\n",
        "  print(key)\n",
        "  for r in out_dic[key]:\n",
        "    print(r.title)\n",
        "    print(r.pdf_url)\n",
        "    print(r.authors)\n",
        "    print(r.published)\n",
        "\n",
        "for r in out_dic['Term 0']:\n",
        "  author_names = [author.name for author in r.authors]\n",
        "  # Concatenating the names into a single string, separated by commas\n",
        "  author_names_str = ', '.join(author_names)\n",
        "  print(author_names_str)\n"
      ],
      "metadata": {
        "id": "up1eRpsR1G4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61b5c499-8fcb-4d57-e80e-cfc4055f4668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term 0\n",
            "Distilling Knowledge Learned in BERT for Text Generation\n",
            "http://arxiv.org/pdf/1911.03829v3\n",
            "[arxiv.Result.Author('Yen-Chun Chen'), arxiv.Result.Author('Zhe Gan'), arxiv.Result.Author('Yu Cheng'), arxiv.Result.Author('Jingzhou Liu'), arxiv.Result.Author('Jingjing Liu')]\n",
            "2019-11-10 02:12:38+00:00\n",
            "Dartmouth CS at WNUT-2020 Task 2: Informative COVID-19 Tweet Classification Using BERT\n",
            "http://arxiv.org/pdf/2012.04539v1\n",
            "[arxiv.Result.Author('Dylan Whang'), arxiv.Result.Author('Soroush Vosoughi')]\n",
            "2020-12-07 07:55:31+00:00\n",
            "BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model\n",
            "http://arxiv.org/pdf/1902.04094v2\n",
            "[arxiv.Result.Author('Alex Wang'), arxiv.Result.Author('Kyunghyun Cho')]\n",
            "2019-02-11 19:02:27+00:00\n",
            "RobBERT: a Dutch RoBERTa-based Language Model\n",
            "http://arxiv.org/pdf/2001.06286v2\n",
            "[arxiv.Result.Author('Pieter Delobelle'), arxiv.Result.Author('Thomas Winters'), arxiv.Result.Author('Bettina Berendt')]\n",
            "2020-01-17 13:25:44+00:00\n",
            "Comparing the Performance of NLP Toolkits and Evaluation measures in Legal Tech\n",
            "http://arxiv.org/pdf/2103.11792v1\n",
            "[arxiv.Result.Author('Muhammad Zohaib Khan')]\n",
            "2021-03-12 11:06:32+00:00\n",
            "Term 1\n",
            "UPB at SemEval-2020 Task 12: Multilingual Offensive Language Detection on Social Media by Fine-tuning a Variety of BERT-based Models\n",
            "http://arxiv.org/pdf/2010.13609v2\n",
            "[arxiv.Result.Author('Mircea-Adrian Tanase'), arxiv.Result.Author('Dumitru-Clementin Cercel'), arxiv.Result.Author('Costin-Gabriel Chiru')]\n",
            "2020-10-26 14:28:29+00:00\n",
            "Interpreting Language Models Through Knowledge Graph Extraction\n",
            "http://arxiv.org/pdf/2111.08546v1\n",
            "[arxiv.Result.Author('Vinitra Swamy'), arxiv.Result.Author('Angelika Romanou'), arxiv.Result.Author('Martin Jaggi')]\n",
            "2021-11-16 15:18:01+00:00\n",
            "GottBERT: a pure German Language Model\n",
            "http://arxiv.org/pdf/2012.02110v1\n",
            "[arxiv.Result.Author('Raphael Scheible'), arxiv.Result.Author('Fabian Thomczyk'), arxiv.Result.Author('Patric Tippmann'), arxiv.Result.Author('Victor Jaravine'), arxiv.Result.Author('Martin Boeker')]\n",
            "2020-12-03 17:45:03+00:00\n",
            "BERTuit: Understanding Spanish language in Twitter through a native transformer\n",
            "http://arxiv.org/pdf/2204.03465v2\n",
            "[arxiv.Result.Author('Javier Huertas-Tato'), arxiv.Result.Author('Alejandro Martin'), arxiv.Result.Author('David Camacho')]\n",
            "2022-04-07 14:28:51+00:00\n",
            "Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis\n",
            "http://arxiv.org/pdf/2308.05476v2\n",
            "[arxiv.Result.Author('Anusuya Krishnan')]\n",
            "2023-08-10 10:07:00+00:00\n",
            "Term 2\n",
            "There is No Big Brother or Small Brother: Knowledge Infusion in Language Models for Link Prediction and Question Answering\n",
            "http://arxiv.org/pdf/2301.04013v1\n",
            "[arxiv.Result.Author('Ankush Agarwal'), arxiv.Result.Author('Sakharam Gawade'), arxiv.Result.Author('Sachin Channabasavarajendra'), arxiv.Result.Author('Pushpak Bhattacharyya')]\n",
            "2023-01-10 14:59:33+00:00\n",
            "Bangla Grammatical Error Detection Using T5 Transformer Model\n",
            "http://arxiv.org/pdf/2303.10612v1\n",
            "[arxiv.Result.Author('H. A. Z. Sameen Shahgir'), arxiv.Result.Author('Khondker Salman Sayeed')]\n",
            "2023-03-19 09:24:48+00:00\n",
            "Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation\n",
            "http://arxiv.org/pdf/2311.00684v2\n",
            "[arxiv.Result.Author('Ta-Chung Chi'), arxiv.Result.Author('Ting-Han Fan'), arxiv.Result.Author('Alexander I. Rudnicky')]\n",
            "2023-11-01 17:43:35+00:00\n",
            "Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI\n",
            "http://arxiv.org/pdf/2212.02924v1\n",
            "[arxiv.Result.Author('Damith Chamalke Senadeera'), arxiv.Result.Author('Julia Ive')]\n",
            "2022-12-06 12:31:53+00:00\n",
            "Simple and Effective Gradient-Based Tuning of Sequence-to-Sequence Models\n",
            "http://arxiv.org/pdf/2209.04683v1\n",
            "[arxiv.Result.Author('Jared Lichtarge'), arxiv.Result.Author('Chris Alberti'), arxiv.Result.Author('Shankar Kumar')]\n",
            "2022-09-10 14:52:41+00:00\n",
            "Sent 0\n",
            "Interpreting Language Models Through Knowledge Graph Extraction\n",
            "http://arxiv.org/pdf/2111.08546v1\n",
            "[arxiv.Result.Author('Vinitra Swamy'), arxiv.Result.Author('Angelika Romanou'), arxiv.Result.Author('Martin Jaggi')]\n",
            "2021-11-16 15:18:01+00:00\n",
            "DynaBERT: Dynamic BERT with Adaptive Width and Depth\n",
            "http://arxiv.org/pdf/2004.04037v2\n",
            "[arxiv.Result.Author('Lu Hou'), arxiv.Result.Author('Zhiqi Huang'), arxiv.Result.Author('Lifeng Shang'), arxiv.Result.Author('Xin Jiang'), arxiv.Result.Author('Xiao Chen'), arxiv.Result.Author('Qun Liu')]\n",
            "2020-04-08 15:06:28+00:00\n",
            "MATE-KD: Masked Adversarial TExt, a Companion to Knowledge Distillation\n",
            "http://arxiv.org/pdf/2105.05912v1\n",
            "[arxiv.Result.Author('Ahmad Rashid'), arxiv.Result.Author('Vasileios Lioutas'), arxiv.Result.Author('Mehdi Rezagholizadeh')]\n",
            "2021-05-12 19:11:34+00:00\n",
            "Sequence-to-Sequence Spanish Pre-trained Language Models\n",
            "http://arxiv.org/pdf/2309.11259v1\n",
            "[arxiv.Result.Author('Vladimir Araujo'), arxiv.Result.Author('Maria Mihaela Trusca'), arxiv.Result.Author('Rodrigo Tufiño'), arxiv.Result.Author('Marie-Francine Moens')]\n",
            "2023-09-20 12:35:19+00:00\n",
            "Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA\n",
            "http://arxiv.org/pdf/2009.08257v1\n",
            "[arxiv.Result.Author('Ieva Staliūnaitė'), arxiv.Result.Author('Ignacio Iacobacci')]\n",
            "2020-09-17 13:00:13+00:00\n",
            "Sent 1\n",
            "Interpreting Language Models Through Knowledge Graph Extraction\n",
            "http://arxiv.org/pdf/2111.08546v1\n",
            "[arxiv.Result.Author('Vinitra Swamy'), arxiv.Result.Author('Angelika Romanou'), arxiv.Result.Author('Martin Jaggi')]\n",
            "2021-11-16 15:18:01+00:00\n",
            "Language ID in the Wild: Unexpected Challenges on the Path to a Thousand-Language Web Text Corpus\n",
            "http://arxiv.org/pdf/2010.14571v2\n",
            "[arxiv.Result.Author('Isaac Caswell'), arxiv.Result.Author('Theresa Breiner'), arxiv.Result.Author('Daan van Esch'), arxiv.Result.Author('Ankur Bapna')]\n",
            "2020-10-27 19:29:17+00:00\n",
            "Automatic Arabic Dialect Identification Systems for Written Texts: A Survey\n",
            "http://arxiv.org/pdf/2009.12622v1\n",
            "[arxiv.Result.Author('Maha J. Althobaiti')]\n",
            "2020-09-26 15:33:16+00:00\n",
            "RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use\n",
            "http://arxiv.org/pdf/2211.08192v1\n",
            "[arxiv.Result.Author('Pieter Delobelle'), arxiv.Result.Author('Thomas Winters'), arxiv.Result.Author('Bettina Berendt')]\n",
            "2022-11-15 14:55:53+00:00\n",
            "Système de traduction automatique statistique Anglais-Arabe\n",
            "http://arxiv.org/pdf/1802.02053v1\n",
            "[arxiv.Result.Author('Marwa Hadj Salah'), arxiv.Result.Author('Didier Schwab'), arxiv.Result.Author('Hervé Blanchon'), arxiv.Result.Author('Mounir Zrigui')]\n",
            "2018-02-06 16:36:44+00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IB1NmJzhwjPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_citations(input_string, found_papers):\n",
        "  out = \"\"\n",
        "  messages = []\n",
        "  for key in found_papers:\n",
        "    prompt = \"Base on the Assesment part of the input for XXXX, find the paper (Title + pdf_url + authors + published_date) which provide best support for XXXX.\\\n",
        "    For terms, focus on finding the term in the provided abstract (In the Abstract).\\\n",
        "    For sentences, focus on messuring the semantic similarity between the sentence and the provided documents.\\\n",
        "    Assure the format stays the same.\\\n",
        "    The results should have similar format, : \\\n",
        "    Title: xxxxxx \\n pdf_url: https:xxxxxx.pdf \\n authors: Abs \\n published_date: July 20, 2019 (replace with the actual published date)\"\n",
        "    prompt = prompt.replace(\"XXXX\", key)\n",
        "    messages.append({\"role\": \"system\", \"content\": prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": input_string})\n",
        "    papers = \"\"\n",
        "    for result in found_papers[key]:\n",
        "      author_names = [author.name for author in result.authors]\n",
        "      author_names_str = ', '.join(author_names)\n",
        "      papers += \"Title: \" + result.title + \"pdf_url: \" + result.pdf_url + \"\\n\" + \"authors: \" + author_names_str + \" \\n\" + \"published_date: \" + result.published.strftime(\"%Y-%m-%d\") + \"\\n\"\n",
        "    messages.append({\"role\": \"user\", \"content\": papers})\n",
        "    completion = client.chat.completions.create(\n",
        "      model= \"gpt-4\",\n",
        "      messages=messages,\n",
        "      temperature = 1\n",
        "    )\n",
        "    messages.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "    response = \"Part:\"+ key + \"\\n\" + completion.choices[0].message.content\n",
        "    print(response)\n",
        "    out += response + \"\\n\"\n",
        "\n",
        "  return out\n",
        "\n",
        "citations = find_citations(step1_out, out_dic)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GHuXyU5R1m8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5175570-b802-4294-e7dd-03cd55a63083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part:Term 0\n",
            "For Term 0 - BERT:\n",
            "Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
            "pdf_url: https://www.aclweb.org/anthology/N19-1423.pdf \n",
            "authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova \n",
            "published_date: June 19, 2019\n",
            "Part:Term 1\n",
            "For Term 1 - RoBERTa:\n",
            "Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
            "pdf_url: https://www.aclweb.org/anthology/P19-2048.pdf\n",
            "authors: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov\n",
            "published_date: July 28, 2019\n",
            "Part:Term 2\n",
            "For Term 2 - T5:\n",
            "Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "pdf_url: https://arxiv.org/pdf/1910.10683.pdf\n",
            "authors: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\n",
            "published_date: October 23, 2019\n",
            "Part:Sent 0\n",
            "For Sent 0:\n",
            "Title: Attention is All You Need\n",
            "pdf_url: https://arxiv.org/pdf/1706.03762.pdf\n",
            "authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin\n",
            "published_date: June 12, 2017\n",
            "Part:Sent 1\n",
            "For Sent 1:\n",
            "Title: Language Models are Few-Shot Learners\n",
            "pdf_url: https://arxiv.org/pdf/2005.14165.pdf\n",
            "authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\n",
            "published_date: May 28, 2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def match_style(original_input, step1_result, citations, citation_style = \"APA style citation\"):\n",
        "  messages = []\n",
        "  messages.append({\"role\": \"system\", \"content\": \"original text: \" + original_input})\n",
        "  messages.append({\"role\": \"system\", \"content\": \"step1 result: \" + step1_result})\n",
        "  messages.append({\"role\": \"system\", \"content\": \"selected paper: \" + citations})\n",
        "  prompt = f\"For the corresponding Term/Sent in the step1_result, complete the {citation_style} in-text citation to the original text. Also generate a reference list. \"\n",
        "  messages.append({\"role\": \"system\", \"content\": prompt})\n",
        "  completion = client.chat.completions.create(\n",
        "    model= \"gpt-3.5-turbo-1106\",\n",
        "    messages = messages,\n",
        "    temperature = 0\n",
        "  )\n",
        "  messages.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "  response = completion.choices[0].message.content\n",
        "  print(response)\n",
        "\n",
        "match_style(input_text, step1_out, citations)"
      ],
      "metadata": {
        "id": "c_cYTa3O7OtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b998c32a-ab99-441e-dbd6-879471018411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recent advances in language model pre-training (Vaswani et al., 2017) have shown that models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel et al., 2019) store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on (Brown et al., 2020).\n",
            "\n",
            "References:\n",
            "\n",
            "Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Retrieved from https://www.aclweb.org/anthology/N19-1423.pdf\n",
            "\n",
            "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., & Zettlemoyer, L. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. Retrieved from https://www.aclweb.org/anthology/P19-2048.pdf\n",
            "\n",
            "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Retrieved from https://arxiv.org/pdf/1910.10683.pdf\n",
            "\n",
            "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is All You Need. Retrieved from https://arxiv.org/pdf/1706.03762.pdf\n",
            "\n",
            "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., & Sutskever, I. (2020). Language Models are Few-Shot Learners. Retrieved from https://arxiv.org/pdf/2005.14165.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA4GsMrqAA6y"
      },
      "source": [
        "Recent advances in language model pre-training have\n",
        "shown that models such as BERT,\n",
        "RoBERTa and T5 store a surprising amount of world knowledge, acquired from the massive text corpora they are trained\n",
        "on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "1. Accuarcy & Precision of inserted postion\n",
        "2. Auccarcy & Precision of the selected papers"
      ],
      "metadata": {
        "id": "AJ6sCepgImvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "A11QDeeQImgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "vx6PFVq0OYqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_space_before_citations(text):\n",
        "    text = re.sub(r', \\[\\d+\\]', lambda m: m.group(0).replace(', ', ''), text)\n",
        "    text = re.sub(r' \\[\\d+\\]', lambda m: m.group(0).replace(' ', ''), text)\n",
        "    return text\n",
        "# usually the [#] tag is separated by comma and space. Remove them.\n",
        "text = \"There are many existing studies about outlier detection in water treatment\\\n",
        " networks [2], [4], [5], [35], [36]. For instance, Adepu et al. studied the impact\\\n",
        "  of cyber attacks on water distribution systems [37]. Goh et al. designed an unsupervised\\\n",
        "   learning approach that regards Recurrent Neural Networks as a temporal predictor\\\n",
        "    to detect attacks [1].\"\n",
        "processed_text = remove_space_before_citations(text)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "id": "KTxBjPZAjTTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# this function return a dictionary\n",
        "# each key are the citaiton tag number\n",
        "# each value are the absolute citaion tag positional value\n",
        "#\n",
        "def extract_citation_indices(text):\n",
        "    citation_indices = {}\n",
        "    offset = 0\n",
        "    for match in re.finditer(r'\\[\\d+\\]', text):\n",
        "        positional_index = match.start() - offset\n",
        "        citation_index = int(match.group()[1:-1])\n",
        "        citation_indices[citation_index] = positional_index\n",
        "        offset += len(match.group())\n",
        "    return citation_indices\n",
        "\n",
        "# text1 = \"Hello[0] World[1][2].\"\n",
        "# text2 = \"A significant fraction[0] of local galaxies show evidence of nuclear activity. I\\\n",
        "# argue that the bulk of this[1] activity, while energetically not remarkable,\\\n",
        "# derives from accretion onto a central massive black hole[2]. The statistics of\\\n",
        "# nearby active galactic nuclei thus provide an effective probe of black hole\\\n",
        "# demography[3].\"\n",
        "citation_indices = extract_citation_indices(processed_text)\n",
        "print(citation_indices)"
      ],
      "metadata": {
        "id": "VnWbI33CLQx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performance(ground_truth, predicted):\n",
        "    matches = 0\n",
        "    mismatches = 0\n",
        "    overmatches = 0\n",
        "    undermatches = 0\n",
        "    positional_differences = 0\n",
        "\n",
        "    for key, value in ground_truth.items():\n",
        "        if key in predicted:\n",
        "            if value == predicted[key]:\n",
        "                matches += 1\n",
        "            else:\n",
        "                mismatches += 1\n",
        "                positional_differences += abs(value - predicted[key])\n",
        "        else:\n",
        "            undermatches += 1\n",
        "\n",
        "    for key in predicted.keys():\n",
        "        if key not in ground_truth:\n",
        "            overmatches += 1\n",
        "\n",
        "    return {\n",
        "        'matches': matches,\n",
        "        'mismatches': mismatches,\n",
        "        'overmatches': overmatches,\n",
        "        'undermatches': undermatches,\n",
        "        'positional_differences': positional_differences\n",
        "    }\n",
        "\n",
        "ground_truth = {0: 5, 1: 11, 2: 11}\n",
        "predicted = {0: 5, 1: 11, 2: 13, 3: 19}\n",
        "performance = evaluate_performance(ground_truth, predicted)\n",
        "print(performance)"
      ],
      "metadata": {
        "id": "kbctpmTnSoRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_comprehensive_score(performance):\n",
        "    total = sum(performance.values())\n",
        "    match_rate = performance['matches'] / total\n",
        "    mismatch_rate = performance['mismatches'] / total\n",
        "    overmatch_rate = performance['overmatches'] / total\n",
        "    undermatch_rate = performance['undermatches'] / total\n",
        "    positional_accuracy = performance['positional_differences'] / performance['mismatches'] if performance['mismatches'] != 0 else 0\n",
        "\n",
        "    comprehensive_score = 1 * match_rate - 1 * mismatch_rate - 1 * overmatch_rate - 1 * undermatch_rate - 0.1 * positional_accuracy\n",
        "    return comprehensive_score\n",
        "\n",
        "comprehensive_score = calculate_comprehensive_score(performance)\n",
        "print(f'Comprehensive Score: {comprehensive_score}')"
      ],
      "metadata": {
        "id": "xNk-Wg_pTMf5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}