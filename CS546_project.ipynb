{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_EukgiYyD7m",
        "outputId": "01af8486-e1aa-4f21-e569-69ac398e4bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.0/990.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uqqq pip\n",
        "!pip install -qqq bitsandbytes==0.39.0\n",
        "!pip install -qqq torch==2.0.1\n",
        "!pip install -qqq arxiv==1.4.7\n",
        "!pip install -qqq langchain==0.0.193\n",
        "!pip install -qqq openai\n",
        "!pip install -qqq tiktoken\n",
        "!pip install -qqq cohere\n",
        "!pip install -qqq pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5kZ3TMiPHuJ",
        "outputId": "77efb02f-1b0b-43ce-9f25-3880cbbe72cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting openai<1.0.0\n",
            "  Using cached openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai<1.0.0) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai<1.0.0) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai<1.0.0) (1.3.1)\n",
            "Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.2.3\n",
            "    Uninstalling openai-1.2.3:\n",
            "      Successfully uninstalled openai-1.2.3\n",
            "Successfully installed openai-0.28.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Collecting openai\n",
            "  Using cached openai-1.2.3-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Using cached openai-1.2.3-py3-none-any.whl (220 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.1\n",
            "    Uninstalling openai-0.28.1:\n",
            "      Successfully uninstalled openai-0.28.1\n",
            "Successfully installed openai-1.2.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uqqq pip\n",
        "!pip install \"openai<1.0.0\"\n",
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOlwkeztVOzt"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "model_engine = \"gpt-4\"\n",
        "client = OpenAI(api_key=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step1**\n",
        "Subsection Analysis Using GPT-4: Utilize the GPT-4 model to analyze the provided text and identify specific subsections that require citations.\n"
      ],
      "metadata": {
        "id": "v6Etl-r37Dwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Step1(input_text):\n",
        "  messages= [{\"role\":\"system\", \"content\": \"Step 1\\\n",
        "  Conduct a thorough analysis of the provided text. \\\n",
        "  Search for specific sentences or terms that need further citations to support, and assign an index to each, starting from 0.\\\n",
        "  For each selected sentence/term (e.g., Term 0, Term 1, Sent 0 etc.), \\\n",
        "  identify and highlight parts that lack citations. \\\n",
        "  Assess the content critically to determine where additional references are necessary to substantiate the claims or information. \\\n",
        "  This approach aims to ensure that every significant claim or statement in each selected sentence/term is adequately supported by relevant and reliable sources\\\n",
        "  The output should also include the original input with in-text citation position. \\\n",
        "  Example input: Emerging strategies, such as the utilization of ChatGPT Plugins, offer promise.\\\n",
        "  While these plugins are undeniably beneficial, they primarily source citations from expansive paper data APIs, encompassing millions of documents. \\\n",
        "  Consequently, while the generated citations may exhibit accuracy, they may not invariably align with the specific references the author intends.\\\n",
        "  Example output : Emerging strategies, such as the utilization of ChatGPT [Term 0] Plugins, offer promise.\\\n",
        "  While these plugins are undeniably beneficial, they primarily source citations from expansive paper data APIs,[Sent 1] encompassing millions of documents. \\\n",
        "  Consequently, while the generated citations may exhibit accuracy, they may not invariably align with the specific references the author intends.[Sent 2]\\\n",
        "  Assesement: [Term 0]: The reason why term 0 needs citations, [Sent 0]: The reason why sentence 0 needs citations, [Sent 1]: The reason why sentence 1 needs citations, [Sent 2]: The reason why sentence 2 needs citations \\\n",
        "  Step 2\\\n",
        "  Combine the results from the following two analysis steps to formulate a python list (code) of comprehensive query strings for each subsection from step1 for arXiv searches.\\\n",
        "  The query string should be formatted using 'abs' for abstract keywords and 'cat' for category.\\\n",
        "  Ensure the string is concise and accurately reflects the key elements derived from each step: \\\n",
        "  From Citation Category Determination (Index 2.1): Extract the main category or categories identified as relevant to the desired citations (arXiv categories like cs.CL).\\\n",
        "  Use the 'cat' parameter to represent these categories in the query string.\\\n",
        "  From Possible Keywords Abstracts (Index 2.2): Compile a list of essential keywords and phrases identified from the abstracts of potential resources.\\\n",
        "  These keywords should be incorporated into the query string using the 'abs' parameter.\\\n",
        "  The final output should be a single, well-structured query string that synergizes these elements, facilitating targeted and efficient searches on arXiv.\\\n",
        "  The string should be clear, precise, and tailored to yield the most relevant and authoritative academic papers pertinent to the subject matter of the text.\\\n",
        "  Don't use quote marks/double quote marks in the query string. Present the output in the similar format as below:\\\n",
        "  Output list: Term 0: (cat:cs.CL) AND (abs: New-Bing), Term 1:(cat:cs.CL) AND (abs: BARD)\\\n",
        "  The number of terms/sentences in step one should align with the number of elements in the output list of step2\"}]\n",
        "\n",
        "\n",
        "\n",
        "  messages.append({\"role\":\"user\", \"content\": input_text})\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "      model= model_engine,\n",
        "      messages=messages,\n",
        "      temperature = 0\n",
        "    )\n",
        "  response = completion.choices[0].message.content\n",
        "  messages.append({\"role\":\"assistant\", \"content\": response})\n",
        "  return response\n",
        "\n",
        "input_text = \"Recent advances in language model pre-training have shown that models such as BERT, RoBERTa and T5 store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on.\"\n",
        "step1_result = Step1(input_text)\n",
        "print(step1_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdu2EBOk7C-X",
        "outputId": "adfc17e4-b707-41c6-d820-a96dcea0ed95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1:\n",
            "\n",
            "Recent advances in language model pre-training have shown that models such as BERT [Term 0], RoBERTa [Term 1] and T5 [Term 2] store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on [Sent 0].\n",
            "\n",
            "Assessment: \n",
            "\n",
            "[Term 0]: The term BERT needs citations to provide evidence of its existence and its role in language model pre-training.\n",
            "\n",
            "[Term 1]: The term RoBERTa needs citations to provide evidence of its existence and its role in language model pre-training.\n",
            "\n",
            "[Term 2]: The term T5 needs citations to provide evidence of its existence and its role in language model pre-training.\n",
            "\n",
            "[Sent 0]: The sentence needs citations to support the claim that these models store a surprising amount of world knowledge and that they acquire this knowledge from the massive text corpora they are trained on.\n",
            "\n",
            "Step 2:\n",
            "\n",
            "Output list: \n",
            "\n",
            "Term 0: (cat:cs.CL) AND (abs: BERT), \n",
            "\n",
            "Term 1: (cat:cs.CL) AND (abs: RoBERTa), \n",
            "\n",
            "Term 2: (cat:cs.CL) AND (abs: T5), \n",
            "\n",
            "Sent 0: (cat:cs.CL) AND (abs: language model pre-training) AND (abs: world knowledge) AND (abs: text corpora)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_to_dict(text):\n",
        "    # Find the substring starting with \"Output list:\"\n",
        "    output_list_start = text.find(\"Output list:\")\n",
        "    if output_list_start == -1:\n",
        "        return \"Output list not found\"\n",
        "\n",
        "    output_list_text = text[output_list_start:]\n",
        "\n",
        "    # Define the regex pattern to match \"Term X:\" or \"Sent X:\" followed by any text until a comma or end of line\n",
        "    pattern = r'(Term \\d+|Sent \\d+): (.*?)($|, )'\n",
        "\n",
        "    # Find all matches in the output list text\n",
        "    matches = re.findall(pattern, output_list_text)\n",
        "\n",
        "    # Create a dictionary to store the extracted data\n",
        "    extracted_dict = {}\n",
        "\n",
        "    for match in matches:\n",
        "        key, value, _ = match\n",
        "        extracted_dict[key.strip()] = value.strip()\n",
        "\n",
        "    return extracted_dict\n",
        "\n",
        "extract_to_dict(step1_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHx5_ru_1Nlq",
        "outputId": "c146eabb-cbc4-4e35-b78c-bb0c9e2a24a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Term 0': '(cat:cs.CL) AND (abs: BERT)',\n",
              " 'Term 1': '(cat:cs.CL) AND (abs: RoBERTa)',\n",
              " 'Term 2': '(cat:cs.CL) AND (abs: T5)',\n",
              " 'Sent 0': '(cat:cs.CL) AND (abs: language model pre-training) AND (abs: world knowledge) AND (abs: text corpora)'}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils.text import num_ini_spaces\n",
        "import arxiv\n",
        "search_results = []\n",
        "dic = extract_to_dict(step1_result)\n",
        "num_results = 15\n",
        "out_dic = {}\n",
        "\n",
        "for key in dic:\n",
        "  search = arxiv.Search(\n",
        "        query = dic[key],\n",
        "        max_results = num_results,\n",
        "        sort_by = arxiv.SortCriterion.Relevance,\n",
        "    )\n",
        "  out_dic[key] = search.results()\n",
        "for r in out_dic['Term 0']:\n",
        "  author_names = [author.name for author in r.authors]\n",
        "  # Concatenating the names into a single string, separated by commas\n",
        "  author_names_str = ', '.join(author_names)\n",
        "  print(author_names_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "up1eRpsR1G4M",
        "outputId": "d03db0df-5c90-4972-ebf5-d12f6194bc19"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zhebin Zhang, Sai Wu, Dawei Jiang, Gang Chen\n",
            "Matej Ulčar, Aleš Žagar, Carlos S. Armendariz, Andraž Repar, Senja Pollak, Matthew Purver, Marko Robnik-Šikonja\n",
            "Nora Kassner, Hinrich Schütze\n",
            "Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, Ion Androutsopoulos\n",
            "Muhammad AL-Qurishi, Sarah AlQaseemi, Riad Soussi\n",
            "Anton Golubev, Natalia Loukachevitch\n",
            "Aadarsh Singh, Priyanshu Kumar, Aman Sinha\n",
            "Jekaterina Novikova\n",
            "Leonardo Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto\n",
            "Ehsan Tavan, Ali Rahmati, Maryam Najafi, Saeed Bibak, Zahed Rahmati\n",
            "Wietse de Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, Malvina Nissim\n",
            "Pham Quang Nhat Minh\n",
            "Ashutosh Adhikari, Achyudh Ram, Raphael Tang, Jimmy Lin\n",
            "Claudia Kittask, Kirill Milintsevich, Kairit Sirts\n",
            "Yongjie Lin, Yi Chern Tan, Robert Frank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_citations(input_string, found_papers):\n",
        "  out = \"\"\n",
        "  messages = []\n",
        "  for key in found_papers:\n",
        "    prompt = \"Base on the Assesment part of the input for XXXX, find the paper (Title + pdf_url + authors + published_date) which provide best support for XXXX.\\\n",
        "    For terms, focus on finding the term in the provided abstract (In the Abstract).\\\n",
        "    For sentences, focus on messuring the semantic similarity between the sentence and the provided documents.\\\n",
        "    Assure the format stays the same.\\\n",
        "    The results should have similar format, : \\\n",
        "    Title: xxxxxx \\n pdf_url: https:xxxxxx.pdf \\n authors: Abs \\n published_date: July 20, 2019 (replace with the actual published date)\"\n",
        "    prompt = prompt.replace(\"XXXX\", key)\n",
        "    messages.append({\"role\": \"system\", \"content\": prompt})\n",
        "    messages.append({\"role\": \"user\", \"content\": input_string})\n",
        "    papers = \"\"\n",
        "    for result in found_papers[key]:\n",
        "      author_names = [author.name for author in result.authors]\n",
        "      author_names_str = ', '.join(author_names)\n",
        "      papers += \"Title: \" + result.title + \"pdf_url: \" + result.pdf_url + \"\\n\" + \"Abstract: \" + result.summary + \"\\n\" + \"authors: \" + author_names_str + \" \\n\" + \"published_date: \" + result.published.strftime(\"%Y-%m-%d\") + \"\\n\"\n",
        "    messages.append({\"role\": \"user\", \"content\": papers})\n",
        "    completion = client.chat.completions.create(\n",
        "      model= model_engine,\n",
        "      messages=messages,\n",
        "      temperature = 1\n",
        "    )\n",
        "    messages.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "    response = \"Part:\"+ key + \"\\n\" + completion.choices[0].message.content\n",
        "    print(response)\n",
        "    out += response + \"\\n\"\n",
        "\n",
        "  return out\n",
        "\n",
        "citations = find_citations(step1_result, out_dic)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHuXyU5R1m8L",
        "outputId": "dde99d81-f098-4ded-ddc7-bf849717da76"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part:Term 0\n",
            "Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n",
            "pdf_url: https://arxiv.org/pdf/1810.04805.pdf\n",
            "authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova\n",
            "published_date: October 11, 2018\n",
            "Part:Term 1\n",
            "Title: RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
            "pdf_url: https://arxiv.org/pdf/1907.11692.pdf\n",
            "authors: Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov\n",
            "published_date: July 26, 2019\n",
            "Part:Term 2\n",
            "Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n",
            "pdf_url: https://arxiv.org/pdf/1910.10683.pdf\n",
            "authors: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\n",
            "published_date: October 24, 2019\n",
            "Part:Sent 0\n",
            "Title: Language Models are Few-Shot Learners\n",
            "pdf_url: https://arxiv.org/pdf/2005.14165.pdf\n",
            "authors: Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei\n",
            "published_date: May 28, 2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def match_style(original_input, step1_result, citations):\n",
        "  messages = []\n",
        "  messages.append({\"role\": \"system\", \"content\": \"original text: \" + original_input})\n",
        "  messages.append({\"role\": \"system\", \"content\": \"step1 result: \" + step1_result})\n",
        "  messages.append({\"role\": \"system\", \"content\": \"selected paper: \" + citations})\n",
        "  prompt = \"For the corresponding Term/Sent in the step1_result, complete the APA style in-text citation to the original text. Also generate a reference list. \"\n",
        "  messages.append({\"role\": \"system\", \"content\": prompt})\n",
        "  completion = client.chat.completions.create(\n",
        "    model= model_engine,\n",
        "    messages = messages,\n",
        "    temperature = 0\n",
        "  )\n",
        "  messages.append({\"role\": \"assistant\", \"content\": completion.choices[0].message.content})\n",
        "  response = completion.choices[0].message.content\n",
        "  print(response)\n",
        "\n",
        "match_style(input_text, step1_result, citations)"
      ],
      "metadata": {
        "id": "c_cYTa3O7OtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b92daf0-7019-427e-acb1-ca02b57e573b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Revised Text: Recent advances in language model pre-training have shown that models such as BERT (Devlin, Chang, Lee, & Toutanova, 2018), RoBERTa (Liu et al., 2019) and T5 (Raffel et al., 2019) store a surprising amount of world knowledge, acquired from the massive text corpora they are trained on (Brown et al., 2020).\n",
            "\n",
            "References:\n",
            "\n",
            "Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. (2020). Language Models are Few-Shot Learners. Retrieved from https://arxiv.org/pdf/2005.14165.pdf\n",
            "\n",
            "Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Retrieved from https://arxiv.org/pdf/1810.04805.pdf\n",
            "\n",
            "Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. Retrieved from https://arxiv.org/pdf/1907.11692.pdf\n",
            "\n",
            "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Retrieved from https://arxiv.org/pdf/1910.10683.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA4GsMrqAA6y"
      },
      "source": [
        "Recent advances in language model pre-training have\n",
        "shown that models such as BERT,\n",
        "RoBERTa and T5 store a surprising amount of world knowledge, acquired from the massive text corpora they are trained\n",
        "on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "1. Accuarcy & Precision of inserted postion\n",
        "2. Auccarcy & Precision of the selected papers"
      ],
      "metadata": {
        "id": "AJ6sCepgImvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A11QDeeQImgm",
        "outputId": "6a91a61d-fdf8-49ce-9a06-d63b6d2c1326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx6PFVq0OYqU",
        "outputId": "6a8e36d8-369d-48e8-d41d-f88bb2b3bfbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_space_before_citations(text):\n",
        "    text = re.sub(r', \\[\\d+\\]', lambda m: m.group(0).replace(', ', ''), text)\n",
        "    text = re.sub(r' \\[\\d+\\]', lambda m: m.group(0).replace(' ', ''), text)\n",
        "    return text\n",
        "# usually the [#] tag is separated by comma and space. Remove them.\n",
        "text = \"There are many existing studies about outlier detection in water treatment\\\n",
        " networks [2], [4], [5], [35], [36]. For instance, Adepu et al. studied the impact\\\n",
        "  of cyber attacks on water distribution systems [37]. Goh et al. designed an unsupervised\\\n",
        "   learning approach that regards Recurrent Neural Networks as a temporal predictor\\\n",
        "    to detect attacks [1].\"\n",
        "processed_text = remove_space_before_citations(text)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTxBjPZAjTTP",
        "outputId": "855f11da-7047-4e4d-f123-7ca2740c7740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are many existing studies about outlier detection in water treatment networks[2][4][5][35][36]. For instance, Adepu et al. studied the impact  of cyber attacks on water distribution systems[37]. Goh et al. designed an unsupervised   learning approach that regards Recurrent Neural Networks as a temporal predictor    to detect attacks[1].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# this function return a dictionary\n",
        "# each key are the citaiton tag number\n",
        "# each value are the absolute citaion tag positional value\n",
        "#\n",
        "def extract_citation_indices(text):\n",
        "    citation_indices = {}\n",
        "    offset = 0\n",
        "    for match in re.finditer(r'\\[\\d+\\]', text):\n",
        "        positional_index = match.start() - offset\n",
        "        citation_index = int(match.group()[1:-1])\n",
        "        citation_indices[citation_index] = positional_index\n",
        "        offset += len(match.group())\n",
        "    return citation_indices\n",
        "\n",
        "# text1 = \"Hello[0] World[1][2].\"\n",
        "# text2 = \"A significant fraction[0] of local galaxies show evidence of nuclear activity. I\\\n",
        "# argue that the bulk of this[1] activity, while energetically not remarkable,\\\n",
        "# derives from accretion onto a central massive black hole[2]. The statistics of\\\n",
        "# nearby active galactic nuclei thus provide an effective probe of black hole\\\n",
        "# demography[3].\"\n",
        "citation_indices = extract_citation_indices(processed_text)\n",
        "print(citation_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnWbI33CLQx-",
        "outputId": "50d53dc0-8ec2-472a-d896-3fded39db977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{2: 83, 4: 83, 5: 83, 35: 83, 36: 83, 37: 178, 1: 319}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performance(ground_truth, predicted):\n",
        "    matches = 0\n",
        "    mismatches = 0\n",
        "    overmatches = 0\n",
        "    undermatches = 0\n",
        "    positional_differences = 0\n",
        "\n",
        "    for key, value in ground_truth.items():\n",
        "        if key in predicted:\n",
        "            if value == predicted[key]:\n",
        "                matches += 1\n",
        "            else:\n",
        "                mismatches += 1\n",
        "                positional_differences += abs(value - predicted[key])\n",
        "        else:\n",
        "            undermatches += 1\n",
        "\n",
        "    for key in predicted.keys():\n",
        "        if key not in ground_truth:\n",
        "            overmatches += 1\n",
        "\n",
        "    return {\n",
        "        'matches': matches,\n",
        "        'mismatches': mismatches,\n",
        "        'overmatches': overmatches,\n",
        "        'undermatches': undermatches,\n",
        "        'positional_differences': positional_differences\n",
        "    }\n",
        "\n",
        "ground_truth = {0: 5, 1: 11, 2: 11}\n",
        "predicted = {0: 5, 1: 11, 2: 13, 3: 19}\n",
        "performance = evaluate_performance(ground_truth, predicted)\n",
        "print(performance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbctpmTnSoRM",
        "outputId": "36dd40b0-6fa1-4b78-cbbe-0f498be0265e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'matches': 2, 'mismatches': 1, 'overmatches': 1, 'undermatches': 0, 'positional_differences': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_comprehensive_score(performance):\n",
        "    total = sum(performance.values())\n",
        "    match_rate = performance['matches'] / total\n",
        "    mismatch_rate = performance['mismatches'] / total\n",
        "    overmatch_rate = performance['overmatches'] / total\n",
        "    undermatch_rate = performance['undermatches'] / total\n",
        "    positional_accuracy = performance['positional_differences'] / performance['mismatches'] if performance['mismatches'] != 0 else 0\n",
        "\n",
        "    comprehensive_score = 1 * match_rate - 1 * mismatch_rate - 1 * overmatch_rate - 1 * undermatch_rate - 0.1 * positional_accuracy\n",
        "    return comprehensive_score\n",
        "\n",
        "comprehensive_score = calculate_comprehensive_score(performance)\n",
        "print(f'Comprehensive Score: {comprehensive_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNk-Wg_pTMf5",
        "outputId": "a245c4bf-a740-4e4f-a69f-b56a901a5e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comprehensive Score: -0.2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}