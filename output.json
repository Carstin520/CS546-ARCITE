[
    {
        "aid": "1908.08704",
        "mid": "2969244993",
        "abstract": "We propose a self-supervised learning framework for visual odometry (VO) that incorporates correlation of consecutive frames and takes advantage of adversarial learning. Previous methods tackle self-supervised VO as a local structure from motion (SfM) problem that recovers depth from single image and relative poses from image pairs by minimizing photometric loss between warped and captured images. As single-view depth estimation is an ill-posed problem, and photometric loss is incapable of discriminating distortion artifacts of warped images, the estimated depth is vague and pose is inaccurate. In contrast to previous methods, our framework learns a compact representation of frame-to-frame correlation, which is updated by incorporating sequential information. The updated representation is used for depth estimation. Besides, we tackle VO as a self-supervised image generation task and take advantage of Generative Adversarial Networks (GAN). The generator learns to estimate depth and pose to generate a warped target image. The discriminator evaluates the quality of generated image with high-level structural perception that overcomes the problem of pixel-wise loss in previous methods. Experiments on KITTI and Cityscapes datasets show that our method obtains more accurate depth with details preserved and predicted pose outperforms state-of-the-art self-supervised methods significantly.",
        "related_work": "Humans are capable of perceiving 3D environment and inferring ego-motion in a short time, but it is hard for an agent to be equipped with similar capabilities. VO SLAM has been considered as a multi-view geometric problem for decades. It is traditionally solved by minimizing photometric @cite_13 or geometric @cite_2 reprojection errors and works well in regular environments, but fails in challenging conditions like dynamic objects and abrupt motions. In light of these limitations, VO has been studied with learning techniques in recent years and many approaches with promising performance have been proposed.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "612478963",
                "abstract": "We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on ( sim (3) ), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.",
                "doi": "https://doi.org/10.1007/978-3-319-10605-2_54",
                "title": "LSD-SLAM: Large-Scale Direct Monocular SLAM",
                "publication_year": 2014
            },
            "@cite_2": {
                "mid": "1612997784",
                "abstract": "This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.",
                "doi": "https://doi.org/10.1109/tro.2015.2463671",
                "title": "ORB-SLAM: A Versatile and Accurate Monocular SLAM System",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1908.06851",
        "mid": "2968588162",
        "abstract": "Fingerprinting techniques, which are a common method for indoor localization, have been recently applied with success into outdoor settings. Particularly, the communication signals of Low Power Wide Area Networks (LPWAN) such as Sigfox, have been used for localization. In this rather recent field of study, not many publicly available datasets, which would facilitate the consistent comparison of different positioning systems, exist so far. In the current study, a published dataset of RSSI measurements on a Sigfox network deployed in Antwerp, Belgium is used to analyse the appropriate selection of preprocessing steps and to tune the hyperparameters of a kNN fingerprinting method. Initially, the tuning of hyperparameter k for a variety of distance metrics, and the selection of efficient data transformation schemes, proposed by relevant works, is presented. In addition, accuracy improvements are achieved in this study, by a detailed examination of the appropriate adjustment of the parameters of the data transformation schemes tested, and of the handling of out of range values. With the appropriate tuning of these factors, the achieved mean localization error was 298 meters, and the median error was 109 meters. To facilitate the reproducibility of tests and comparability of results, the code and train validation test split used in this study are available.",
        "related_work": "The proliferation of Low Power Wide Area Networks (LPWAN), such as Sigfox and LoRaWAN, has brought a new domain of application of the fingerprinting methods. A recent study @cite_10 has experimentally verified the intuitive assumption that fingerprinting methods outperform, in terms of accuracy, proximity or ranging positioning methods, in a Sigfox setting.",
        "ref_abstract": {
            "@cite_10": {
                "mid": "2902629385",
                "abstract": "Location-based services play an important role in Internet of Things (IoT) applications. However, a trade-off has to be made between the location estimation error and the battery lifetime of an IoT device. As IoT devices communicate over Low Power Wide Area Networks (LPWAN), signal strength localization methods can use the existing communication link to estimate their location. In this paper, we present a comparison of three proximity methods, one fingerprinting method and three ranging methods using Sigfox communication messages. To evaluate these methods, we use a ground truth Sigfox dataset which we collected in a large urban environment, as well as new evaluation data that was collected in the same urban area. With a mean estimation error of 586 m, our fingerprinting method achieves the best result compared to other signal strength localization methods.",
                "doi": "https://doi.org/10.1109/wpnc.2018.8555743",
                "title": "A Comparison of Signal Strength Localization Methods with Sigfox",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1908.04686",
        "mid": "2967058823",
        "abstract": "We show how to build several data structures of central importance to string processing, taking as input the Burrows-Wheeler transform (BWT) and using small extra working space. Let @math be the text length and @math be the alphabet size. We first provide two algorithms that enumerate all LCP values and suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. Using these algorithms as building blocks, for any parameter @math we show how to build the PLCP bitvector and the balanced parentheses representation of the suffix tree topology in @math time using at most @math bits of working space on top of the input BWT and the output. In particular, this implies that we can build a compressed suffix tree from the BWT using just succinct working space (i.e. @math bits) and any time in @math . This improves the previous most space-efficient algorithms, which worked in @math bits and @math time. We also consider the problem of merging BWTs of string collections, and provide a solution running in @math time and using just @math bits of working space. An efficient implementation of our LCP construction and BWT merge algorithms use (in RAM) as few as @math bits on top of a packed representation of the input output and process data as fast as @math megabases per second.",
        "related_work": "As far as the CSA is concerned, this component can be easily built from the BWT using small space as it is formed (in its simplest design) by just a BWT with rank select functionality enhanced with a suffix array sampling, see also @cite_21 .",
        "ref_abstract": {
            "@cite_21": {
                "mid": "1988110322",
                "abstract": "We show that the compressed suffix array and the compressed suffix tree for a string of length n over an integer alphabet of size \u03c3 \u2264 n can both be built in O(n) (randomized) time using only O(n log \u03c3) bits of working space. The previously fastest construction algorithms that used O(n log \u03c3) bits of space took times O(n log log \u03c3) and O(n loge n) respectively (where e is any positive constant smaller than 1).",
                "doi": "https://doi.org/10.1145/2591796.2591885",
                "title": "Linear time construction of compressed text indices in compact space",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1908.01823",
        "mid": "2965547769",
        "abstract": "We propose a general approach for change-point detection in dynamic networks. The proposed method is model-free and covers a wide range of dynamic networks. The key idea behind our approach is to effectively utilize the network structure in designing change-point detection algorithms. This is done via an initial step of graphon estimation, where we propose a modified neighborhood smoothing (MNBS) algorithm for estimating the link probability matrices of a dynamic network. Based on the initial graphon estimation, we then develop a screening and thresholding algorithm for multiple change-point detection in dynamic networks. The convergence rate and consistency for the change-point detection procedure are derived as well as those for MNBS. When the number of nodes is large (e.g., exceeds the number of temporal points), our approach yields a faster convergence rate in detecting change-points comparing with an algorithm that simply employs averaged information of the dynamic network across time. Numerical experiments demonstrate robust performance of the proposed algorithm for change-point detection under various types of dynamic networks, and superior performance over existing methods is observed. A real data example is provided to illustrate the effectiveness and practical impact of the procedure.",
        "related_work": "@cite_5 proposes a novel estimator for estimating the link probability matrix @math of an undirected network by neighborhood smoothing (NBS). The essential idea consists of the following: Given an adjacent matrix @math , the link probability @math between node @math and @math is estimated by where @math is a certain set of neighboring nodes of node @math , which consists of the nodes that exhibit similar connection patterns as node @math . With a well-designed neighborhood adaptive to the network structure, the smoothing achieves an accurate estimation for @math . NBS in @cite_5 estimates @math with a single adjacency matrix @math . For a dynamic network, a sequence of adjacency matrices @math is available, which provides extra information of the network. By aggregating information from repeated observations across time, in Section , we propose a modified NBS by carefully shrinking the neighborhood size, which yields a better convergence rate in estimating the link probability matrix @math and thus an improved rate in change-point detection.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2962719258",
                "abstract": "SummaryThe estimation of probabilities of network edges from the observed adjacency matrix has important applications to the prediction of missing links and to network denoising. It is usually addressed by estimating the graphon, a function that determines the matrix of edge probabilities, but this is ill-defined without strong assumptions on the network structure. Here we propose a novel computationally efficient method, based on neighbourhood smoothing, to estimate the expectation of the adjacency matrix directly, without making the structural assumptions that graphon estimation requires. The neighbourhood smoothing method requires little tuning, has a competitive mean squared error rate and outperforms many benchmark methods for link prediction in simulated and real networks.",
                "doi": "https://doi.org/10.1093/biomet/asx042",
                "title": "Estimating network edge probabilities by neighbourhood smoothing",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1907.12352",
        "mid": "2966538158",
        "abstract": "We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levels---the DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms out---instead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.",
        "related_work": "On a high level, our work relates to the use of abstraction in creating effective visual representations, , the use of . Viola and Isenberg @cite_39 describe this concept as a process, which removes detail when transitioning from a lower-level to a higher-level representation, yet which preserves the overall concept. While they attribute the removed detail to natural variation, noise, etc.'' in the investigated multi-scale representation we actually deal with a different data scenario: DNA assemblies at different levels of scale. We thus technically do not deal with a concept-preserving transformation'' @cite_39 , but with a process in which the underlying representational concept (or parts of it) can change. Nonetheless, their view of abstraction as an interactive process that allows viewers to relate one representation (at one scale) to another one (at a different scale) is essential to our work.",
        "ref_abstract": {
            "@cite_39": {
                "mid": "2751478023",
                "abstract": "We explore the concept of abstraction as it is used in visualization, with the ultimate goal of understanding and formally defining it. Researchers so far have used the concept of abstraction largely by intuition without a precise meaning. This lack of specificity left questions on the characteristics of abstraction, its variants, its control, or its ultimate potential for visualization and, in particular, illustrative visualization mostly unanswered. In this paper we thus provide a first formalization of the abstraction concept and discuss how this formalization affects the application of abstraction in a variety of visualization scenarios. Based on this discussion, we derive a number of open questions still waiting to be answered, thus formulating a research agenda for the use of abstraction for the visual representation and exploration of data. This paper, therefore, is intended to provide a contribution to the discussion of the theoretical foundations of our field, rather than attempting to provide a completed and final theory.",
                "doi": "https://doi.org/10.1109/tvcg.2017.2747545",
                "title": "Pondering the Concept of Abstraction in (Illustrative) Visualization",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1907.11397",
        "mid": "2966209912",
        "abstract": "Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes), by sharing information of attributes between different objects. Attributes are artificially annotated for objects and are treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impact on the ZSL system performance. This paper first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting key attributes set can improve the generalization performance of the original ZSL model which uses all the attributes. Unfortunately, previous attribute selection methods are conducted based on the seen data, their selected attributes have poor generalization capability to the unseen data, which is unavailable in training stage for ZSL tasks. Inspired by learning from pseudo relevance feedback, this paper introduces the out-of-the-box data, which is pseudo data generated by an attribute-guided generative model, to mimic the unseen data. After that, we present an iterative attribute selection (IAS) strategy which iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance.",
        "related_work": "ZSL can recognize new objects using attributes as the intermediate semantic representation. Some researchers adopt the probability-prediction strategy to transfer information. @cite_12 proposed a popular baseline, i.e. direct attribute prediction (DAP). DAP learns probabilistic attribute classifiers using the seen data and infers the label of the unseen data by combining the results of pre-trained classifiers. Most recent works adopt the label-embedding strategy that directly learns a mapping function from the input features space to the semantic embedding space. One line of works is to learn linear compatibility functions. For example, @cite_0 presented an attribute label embedding (ALE) model which learns a compatibility function combined with ranking loss. Romera- @cite_16 proposed an approach that models the relationships among features, attributes and classes as a two linear layers network. Another direction is to learn nonlinear compatibility functions. @cite_30 presented a nonlinear embedding model that augments bilinear compatibility model by incorporating latent variables. @cite_15 proposed a first general kronecker product kernel-based learning model for ZSL tasks. In addition to the classification task, @cite_38 proposed an attribute network for zero-shot hashing retrieval task.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "2334493732",
                "abstract": "We present a novel latent embedding model for learning a compatibility function between image and class embeddings, in the context of zero-shot classification. The proposed method augments the state-of-the-art bilinear compatibility model by incorporating latent variables. Instead of learning a single bilinear map, it learns a collection of maps with the selection, of which map to use, being a latent variable for the current image-class pair. We train the model with a ranking based objective function which penalizes incorrect rankings of the true class for a given image. We empirically demonstrate that our model improves the state-of-the-art for various class embeddings consistently on three challenging publicly available datasets for the zero-shot setting. Moreover, our method leads to visually highly interpretable results with clear clusters of different fine-grained object properties that correspond to different latent variable maps.",
                "doi": "https://doi.org/10.1109/cvpr.2016.15",
                "title": "Latent Embeddings for Zero-Shot Classification",
                "publication_year": 2016
            },
            "@cite_38": {
                "mid": "2963340196",
                "abstract": "Zero-shot hashing (ZSH) aims at learning a hashing model that is trained only by instances from seen categories but can generate well to those of unseen categories. Typically, it is achieved by utilizing a semantic embedding space to transfer knowledge from seen domain to unseen domain. Existing efforts mainly focus on single-modal retrieval task, especially image-based image retrieval (IBIR). However, as a highlighted research topic in the field of hashing, cross-modal retrieval is more common in real-world applications. To address the cross-modal ZSH (CMZSH) retrieval task, we propose a novel attribute-guided network (AgNet), which can perform not only IBIR but also text-based image retrieval (TBIR). In particular, AgNet aligns different modal data into a semantically rich attribute space, which bridges the gap caused by modality heterogeneity and zero-shot setting. We also design an effective strategy that exploits the attribute to guide the generation of hash codes for image and text within the same network. Extensive experimental results on three benchmark data sets (AwA, SUN, and ImageNet) demonstrate the superiority of AgNet on both cross-modal and single-modal zero-shot image retrieval tasks.",
                "doi": "https://doi.org/10.1109/tnnls.2019.2904991",
                "title": "Attribute-Guided Network for Cross-Modal Zero-Shot Hashing",
                "publication_year": 2020
            },
            "@cite_0": {
                "mid": "2171061940",
                "abstract": "Attributes act as intermediate representations that enable parameter sharing between classes, a must when training data is scarce. We propose to view attribute-based image classification as a label-embedding problem: each class is embedded in the space of attribute vectors. We introduce a function that measures the compatibility between an image and a label embedding. The parameters of this function are learned on a training set of labeled samples to ensure that, given an image, the correct classes rank higher than the incorrect ones. Results on the Animals With Attributes and Caltech-UCSD-Birds datasets show that the proposed framework outperforms the standard Direct Attribute Prediction baseline in a zero-shot learning scenario. Label embedding enjoys a built-in ability to leverage alternative sources of information instead of or in addition to attributes, such as, e.g., class hierarchies or textual descriptions. Moreover, label embedding encompasses the whole range of learning settings from zero-shot learning to regular learning with a large number of labeled examples.",
                "doi": "https://doi.org/10.1109/tpami.2015.2487986",
                "title": "Label-Embedding for Image Classification",
                "publication_year": 2016
            },
            "@cite_15": {
                "mid": "2964038500",
                "abstract": "Kronecker product kernel provides the standard approach in the kernel methods\u2019 literature for learning from graph data, where edges are labeled and both start and end vertices have their own feature representations. The methods allow generalization to such new edges, whose start and end vertices do not appear in the training data, a setting known as zero-shot or zero-data learning. Such a setting occurs in numerous applications, including drug-target interaction prediction, collaborative filtering, and information retrieval. Efficient training algorithms based on the so-called vec trick that makes use of the special structure of the Kronecker product are known for the case where the training data are a complete bipartite graph. In this paper, we generalize these results to noncomplete training graphs. This allows us to derive a general framework for training Kronecker product kernel methods, as specific examples we implement Kronecker ridge regression and support vector machine algorithms. Experimental results demonstrate that the proposed approach leads to accurate models, while allowing order of magnitude improvements in training and prediction time.",
                "doi": "https://doi.org/10.1109/tnnls.2017.2727545",
                "title": "Fast Kronecker Product Kernel Methods via Generalized Vec Trick",
                "publication_year": 2018
            },
            "@cite_16": {
                "mid": "652269744",
                "abstract": "Zero-shot learning consists in learning how to recognise new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalisation error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17 .",
                "doi": "https://doi.org/10.1007/978-3-319-50077-5_2",
                "title": "An Embarrassingly Simple Approach to Zero-Shot Learning",
                "publication_year": 2017
            },
            "@cite_12": {
                "mid": "2128532956",
                "abstract": "We study the problem of object recognition for categories for which we have no training examples, a task also called zero--data or zero-shot learning. This situation has hardly been studied in computer vision research, even though it occurs frequently; the world contains tens of thousands of different object classes, and image collections have been formed and suitably annotated for only a few of them. To tackle the problem, we introduce attribute-based classification: Objects are identified based on a high-level description that is phrased in terms of semantic attributes, such as the object's color or shape. Because the identification of each such property transcends the specific learning task at hand, the attribute classifiers can be prelearned independently, for example, from existing image data sets unrelated to the current task. Afterward, new classes can be detected based on their attribute representation, without the need for a new training phase. In this paper, we also introduce a new data set, Animals with Attributes, of over 30,000 images of 50 animal classes, annotated with 85 semantic attributes. Extensive experiments on this and two more data sets show that attribute-based classification indeed is able to categorize images without access to any training images of the target classes.",
                "doi": "https://doi.org/10.1109/tpami.2013.140",
                "title": "Attribute-Based Classification for Zero-Shot Visual Object Categorization",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1901.11467",
        "mid": "2951047368",
        "abstract": "An obstacle to the development of many natural language processing products is the vast amount of training examples necessary to get satisfactory results. The generation of these examples is often a tedious and time-consuming task. This paper this paper proposes a method to transform the sentiment of sentences in order to limit the work necessary to generate more training data. This means that one sentence can be transformed to an opposite sentiment sentence and should reduce by half the work required in the generation of text. The proposed pipeline consists of a sentiment classifier with an attention mechanism to highlight the short phrases that determine the sentiment of a sentence. Then, these phrases are changed to phrases of the opposite sentiment using a baseline model and an autoencoder approach. Experiments are run on both the separate parts of the pipeline as well as on the end-to-end model. The sentiment classifier is tested on its accuracy and is found to perform adequately. The autoencoder is tested on how well it is able to change the sentiment of an encoded phrase and it was found that such a task is possible. We use human evaluation to judge the performance of the full (end-to-end) pipeline and that reveals that a model using word vectors outperforms the encoder model. Numerical evaluation shows that a success rate of 54.7 is achieved on the sentiment change.",
        "related_work": "Sentiment analysis is a task in NLP that aims to predict the sentiment of a sentence @cite_26 . The task can range from a binary classification task where the aim is to predict whether a document is positive or negative to a fine-grained task with multiple classes. In sentiment analysis, state-of-the-art results have been achieved using neural network architectures such as convolutional neural networks @cite_9 and recurrent neural networks @cite_1 . Variants of RNNs; LSTMs and GRUs, have also been used to great success @cite_6 .",
        "ref_abstract": {
            "@cite_9": {
                "mid": "2949541494",
                "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.",
                "doi": "https://doi.org/10.48550/arxiv.1408.5882",
                "title": "Convolutional Neural Networks for Sentence Classification",
                "publication_year": 2014
            },
            "@cite_26": {
                "mid": "2108646579",
                "abstract": "Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Lecture slides are also available online.",
                "doi": "https://doi.org/10.1007/978-1-4899-7687-1_907",
                "title": "Sentiment Analysis and Opinion Mining",
                "publication_year": 2017
            },
            "@cite_1": {
                "mid": "2250966211",
                "abstract": "Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document. To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion. The model first learns sentence representation with convolutional neural network or long short-term memory. Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network. We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge. Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification. 1",
                "doi": "https://doi.org/10.18653/v1/d15-1167",
                "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification",
                "publication_year": 2015
            },
            "@cite_6": {
                "mid": "2950635152",
                "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
                "doi": "https://doi.org/10.48550/arxiv.1406.1078",
                "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1907.09387",
        "mid": "2963297137",
        "abstract": "Industry 4.0 is becoming more and more important for manufacturers as the developments in the area of Internet of Things advance. Another technology gaining more attention is data stream processing systems. Although such streaming frameworks seem to be a natural fit for Industry 4.0 scenarios, their application in this context is still low. The contributions in this paper are threefold. Firstly, we present industry findings that we derived from site inspections with a focus on Industry 4.0. Moreover, our view on Industry 4.0 and important related aspects is elaborated. As a third contribution, we illustrate our opinion on why data stream processing technologies could act as an enabler for Industry 4.0 and point out possible obstacles on this way.",
        "related_work": "A recent work developed a framework called Production Assessment 4.0, which aims to support enterprises developing Industry 4.0 use cases. For doing so, they made use of the design thinking approach. After elaborating on the framework and its processes, a section about its evaluation is presented. Production Assessment 4.0 was evaluated in several consulting projects with enterprises. However, no details about, e.g., their data characteristics or their state of Industry 4.0 adoption progress are given @cite_17 .",
        "ref_abstract": {
            "@cite_17": {
                "mid": "2811438692",
                "abstract": "Facing a wide range of new technologies and best practices within Industry 4.0, companies are seeking a systematic approach to identify potential application scenarios in their production. Best practices are often customized use cases \u2013 generally too specific to apply in a different manufacturing setting right away. The Production Assessment 4.0 presents a pragmatic approach to support companies to develop Industry 4.0 use cases in their factories. The approach follows the Design Thinking method and focus especially on the human role as a key aspect in the use cases designing process.",
                "doi": "https://doi.org/10.1007/978-3-319-94196-7_46",
                "title": "Production Assessment 4.0 \u2013 Methods for the Development and Evaluation of Industry 4.0 Use Cases",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1901.08707",
        "mid": "2963863924",
        "abstract": "We investigate the effectiveness of a simple solution to the common problem of deep learning in medical image analysis with limited quantities of labeled training data. The underlying idea is to assign artificial labels to abundantly available unlabeled medical images and, through a process known as surrogate supervision, pre-train a deep neural network model for the target medical image analysis task lacking sufficient labeled training data. In particular, we employ 3 surrogate supervision schemes, namely rotation, reconstruction, and colorization, in 4 different medical imaging applications representing classification and segmentation for both 2D and 3D medical images. 3 key findings emerge from our research: 1) pre-training with surrogate supervision is effective for small training sets; 2) deep models trained from initial weights pre-trained through surrogate supervision outperform the same models when trained from scratch, suggesting that pre-training with surrogate supervision should be considered prior to training any deep 3D models; 3) pre-training models in the medical domain with surrogate supervision is more effective than transfer learning from an unrelated domain (e.g., natural images), indicating the practical value of abundant unlabeled medical image data.",
        "related_work": "Self-supervised learning with surrogate supervision is a relatively new trend in computer vision, with promising schemes appearing only in recent years. Consequently, the literature on the effectiveness of surrogate supervision in medical imaging is meager. @cite_5 proposed longitudinal relationships between medical images as the surrogate task to pre-train model weights. To generate surrogate supervision, they assign a label of 1 if two longitudinal studies belong to the same patient and 0 otherwise. @cite_4 used noise removal in small image patches as the surrogate task, wherein the surrogate supervision was created by mapping the patches with user-injected noise to the original clean image patches. @cite_18 used image colorization as the surrogate task, wherein color colonoscopy images are converted to gray-scale and then recovered using a conditional Generative Adversarial Network (GAN).",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2742126485",
                "abstract": "A significant proportion of patients scanned in a clinical setting have follow-up scans. We show in this work that such longitudinal scans alone can be used as a form of \u201cfree\u201d self-supervision for training a deep network. We demonstrate this self-supervised learning for the case of T2-weighted sagittal lumbar Magnetic Resonance Images (MRIs). A Siamese convolutional neural network (CNN) is trained using two losses: (i) a contrastive loss on whether the scan is of the same person (i.e. longitudinal) or not, together with (ii) a classification loss on predicting the level of vertebral bodies. The performance of this pre-trained network is then assessed on a grading classification task. We experiment on a dataset of 1016 subjects, 423 possessing follow-up scans, with the end goal of learning the disc degeneration radiological gradings attached to the intervertebral discs. We show that the performance of the pre-trained CNN on the supervised classification task is (i) superior to that of a network trained from scratch; and (ii) requires far fewer annotated training samples to reach an equivalent performance to that of the network trained from scratch.",
                "doi": "https://doi.org/10.1007/978-3-319-67558-9_34",
                "title": "Self-supervised Learning for Spinal MRIs",
                "publication_year": 2017
            },
            "@cite_18": {
                "mid": "2962936819",
                "abstract": "Purpose Surgical data science is a new research field that aims to observe all aspects of the patient treatment process in order to provide the right assistance at the right time. Due to the breakthrough successes of deep learning-based solutions for automatic image annotation, the availability of reference annotations for algorithm training is becoming a major bottleneck in the field. The purpose of this paper was to investigate the concept of self-supervised learning to address this issue.",
                "doi": "https://doi.org/10.1007/s11548-018-1772-0",
                "title": "Exploiting the potential of unlabeled endoscopic video data with self-supervised learning",
                "publication_year": 2018
            },
            "@cite_4": {
                "mid": "2558464646",
                "abstract": "The work explores the use of denoising autoencoders (DAEs) for brain lesion detection, segmentation, and false-positive reduction. Stacked denoising autoencoders (SDAEs) were pretrained using a large number of unlabeled patient volumes and fine-tuned with patches drawn from a limited number of patients (n=20, 40, 65). The results show negligible loss in performance even when SDAE was fine-tuned using 20 labeled patients. Low grade glioma (LGG) segmentation was achieved using a transfer learning approach in which a network pretrained with high grade glioma data was fine-tuned using LGG image patches. The networks were also shown to generalize well and provide good segmentation on unseen BraTS 2013 and BraTS 2015 test data. The manuscript also includes the use of a single layer DAE, referred to as novelty detector (ND). ND was trained to accurately reconstruct nonlesion patches. The reconstruction error maps of test data were used to localize lesions. The error maps were shown to assign unique error distributions to various constituents of the glioma, enabling localization. The ND learns the nonlesion brain accurately as it was also shown to provide good segmentation performance on ischemic brain lesions in images from a different database.",
                "doi": "https://doi.org/10.1117/1.jmi.4.4.041311",
                "title": "Semisupervised learning using denoising autoencoders for brain lesion detection and segmentation",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1901.08201",
        "mid": "2911583982",
        "abstract": "Abstract To improve the performance of Intensive Care Units (ICUs), the field of bio-statistics has developed scores which try to predict the likelihood of negative outcomes. These help evaluate the effectiveness of treatments and clinical practice, and also help to identify patients with unexpected outcomes. However, they have been shown by several studies to offer sub-optimal performance. Alternatively, Deep Learning offers state of the art capabilities in certain prediction tasks and research suggests deep neural networks are able to outperform traditional techniques. Nevertheless, a main impediment for the adoption of Deep Learning in healthcare is its reduced interpretability, for in this field it is crucial to gain insight into the why of predictions, to assure that models are actually learning relevant features instead of spurious correlations. To address this, we propose a deep multi-scale convolutional architecture trained on the Medical Information Mart for Intensive Care III (MIMIC-III) for mortality prediction, and the use of concepts from coalitional game theory to construct visual explanations aimed to show how important these inputs are deemed by the network. Results show our model attains a ROC AUC of 0.8735 ( \u00b1 0.0025) which is competitive with the state of the art of Deep Learning mortality models trained on MIMIC-III data, while remaining interpretable. Supporting code can be found at https: github.com williamcaicedo ISeeU .",
        "related_work": "Although the most natural application of Deep Learning algorithms to medical diagnosis is automated medical image diagnosis @cite_38 , the usage of Physiological Time Series (PTS) and Electronic Medical Record (EMR) data, is a more general source of data on which machine learning models can be trained. EMRs are very attractive as a potential data source since their use is widespread, which makes them abundant and accessible electronically. However, there are certain challenges associated with their \u201csecondary use\u201d in Machine Learning @cite_39 . Despite this, several works have reported the successful use of EMRs and PTS to train Machine Learning Deep Learning based models for diagnosis.",
        "ref_abstract": {
            "@cite_38": {
                "mid": "2533800772",
                "abstract": "This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.",
                "doi": "https://doi.org/10.1146/annurev-bioeng-071516-044442",
                "title": "Deep Learning in Medical Image Analysis",
                "publication_year": 2017
            },
            "@cite_39": {
                "mid": "2277786047",
                "abstract": "Clinical data management systems typically provide caregiver teams with useful information, derived from large, sometimes highly heterogeneous, data sources that are often changing dynamically. Over the last decade there has been a significant surge in interest in using these data sources, from simply reusing the standard clinical databases for event prediction or decision support, to including dynamic and patient-specific information into clinical monitoring and prediction problems. However, in most cases, commercial clinical databases have been designed to document clinical activity for reporting, liability, and billing reasons, rather than for developing new algorithms. With increasing excitement surrounding \u201csecondary use of medical records\u201d and \u201cBig Data\u201d analytics, it is important to understand the limitations of current databases and what needs to change in order to enter an era of \u201cprecision medicine.\u201d This review article covers many of the issues involved in the collection and preprocessing of critical care data. The three challenges in critical care are considered: compartmentalization, corruption, and complexity. A range of applications addressing these issues are covered, including the modernization of static acuity scoring; online patient tracking; personalized prediction and risk assessment; artifact detection; state estimation; and incorporation of multimodal data sources such as genomic and free text data.",
                "doi": "https://doi.org/10.1109/jproc.2015.2501978",
                "title": "Machine Learning and Decision Support in Critical Care",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1901.07822",
        "mid": "2911565143",
        "abstract": "This paper presents a new method for medical diagnosis of neurodegenerative diseases, such as Parkinson's, by extracting and using latent information from trained Deep convolutional, or convolutional-recurrent Neural Networks (DNNs). In particular, our approach adopts a combination of transfer learning, k-means clustering and k-Nearest Neighbour classification of deep neural network learned representations to provide enriched prediction of the disease based on MRI and or DaT Scan data. A new loss function is introduced and used in the training of the DNNs, so as to perform adaptation of the generated learned representations between data from different medical environments. Results are presented using a recently published database of Parkinson's related information, which was generated and evaluated in a hospital environment.",
        "related_work": "A Parkinson's database comprising MRI and DaT Scan data from 78 subjects, 55 patients with Parkinson's and 23 non patients, has been recently released @cite_14 ; it includes, in total 41528 MRI data (31147 from patients and 10381 from non patients) and 925 DaT scans (595 and 330 respectively). Our developments next are based on this database.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2789037848",
                "abstract": "Neurodegenerative disorders, such as Alzheimer\u2019s and Parkinson\u2019s, constitute a major factor in long-term disability and are becoming more and more a serious concern in developed countries. As there...",
                "doi": "https://doi.org/10.1142/s0218213018500112",
                "title": "Machine Learning for Neurodegenerative Disorder Diagnosis \u2014 Survey of Practices and Launch of Benchmark Dataset",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1901.07786",
        "mid": "2952145720",
        "abstract": "Headline generation is a special type of text summarization task. While the amount of available training data for this task is almost unlimited, it still remains challenging, as learning to generate headlines for news articles implies that the model has strong reasoning about natural language. To overcome this issue, we applied recent Universal Transformer architecture paired with byte-pair encoding technique and achieved new state-of-the-art results on the New York Times Annotated corpus with ROUGE-L F1-score 24.84 and ROUGE-2 F1-score 13.48. We also present the new RIA corpus and reach ROUGE-L F1-score 36.81 and ROUGE-2 F1-score 22.15 on it.",
        "related_work": "In the recent work of Hayashi @cite_1 , an encoder-decoder approach was presented, where the first sentence was reformulated to a headline. Our Encoder-Decoder baseline (see section ) follows their setup.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2787752238",
                "abstract": "Automatic headline generation is related to automatic text summarization and it is useful to solve information flood problems. This paper aims at generating a headline using a recurrent neural network which is based on a machine translation approach. Our headline generator consists of an encoder and a decoder and they are constructed with Long Short Term Memory, which is one of recurrent neural networks. The encoder constructs distributed representation from the first sentence in an article and the decoder generated headlines from the distributed representation. In our experiments, we confirmed that our proposed method could generate appropriate headlines but in some articles this method generates meaningless headlines. The results show that our proposed method is superior to another approach, statistical machine translation from the viewpoint of ROUGE, which is an evaluation score of automatic text summarization. Furthermore, we could find that using an input sentence in reverse order improves the quality of headline generation.",
                "doi": "https://doi.org/10.1007/978-3-319-70636-8_6",
                "title": "Headline Generation with Recurrent Neural Network",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1901.07440",
        "mid": "2913669491",
        "abstract": "Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10 of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.",
        "related_work": "One of the most related studies is the one by Xia et al @cite_33 . They investigated what developers search for on the Web, and found that developers search for explanations of unknown terminology, explanations for exceptions error messages (e.g., HTTP 404), reusable code snippets, solutions to common programming bugs, and suitable third-party libraries services. Furthermore, they found that searching for solutions to performance bugs, solutions to multi-threading bugs, public datasets to test newly developed algorithms or systems, reusable code snippets, best industrial practices, database optimization solutions, solutions to security bugs, and solutions to software configuration bugs are the most difficult search tasks that developers consider.",
        "ref_abstract": {
            "@cite_33": {
                "mid": "2604794021",
                "abstract": "Developers commonly make use of a web search engine such as Google to locate online resources to improve their productivity. A better understanding of what developers search for could help us understand their behaviors and the problems that they meet during the software development process. Unfortunately, we have a limited understanding of what developers frequently search for and of the search tasks that they often find challenging. To address this gap, we collected search queries from 60 developers, surveyed 235 software engineers from more than 21 countries across five continents. In particular, we asked our survey participants to rate the frequency and difficulty of 34 search tasks which are grouped along the following seven dimensions: general search, debugging and bug fixing, programming, third party code reuse, tools, database, and testing. We find that searching for explanations for unknown terminologies, explanations for exceptions error messages (e.g., HTTP 404), reusable code snippets, solutions to common programming bugs, and suitable third-party libraries services are the most frequent search tasks that developers perform, while searching for solutions to performance bugs, solutions to multi-threading bugs, public datasets to test newly developed algorithms or systems, reusable code snippets, best industrial practices, database optimization solutions, solutions to security bugs, and solutions to software configuration bugs are the most difficult search tasks that developers consider. Our study sheds light as to why practitioners often perform some of these tasks and why they find some of them to be challenging. We also discuss the implications of our findings to future research in several research areas, e.g., code search engines, domain-specific search engines, and automated generation and refinement of search queries.",
                "doi": "https://doi.org/10.1007/s10664-017-9514-4",
                "title": "What do developers search for on the web?",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1907.07066",
        "mid": "2959448612",
        "abstract": "In a steady-state evolution, tournament selection traditionally uses the fitness function to select the parents, and negative selection chooses an individual to be replaced with an offspring. This contribution focuses on analyzing the behavior, in terms of performance, of different heuristics when used instead of the fitness function in tournament selection. The heuristics analyzed are related to measuring the similarity of the individuals in the semantic space. In addition, the analysis includes random selection and traditional tournament selection. These selection functions were implemented on our Semantic Genetic Programming system, namely EvoDAG, which is inspired by the geometric genetic operators and tested on 30 classification problems with a variable number of samples, variables, and classes. The result indicated that the combination of accuracy and the random selection, in the negative tournament, produces the best combination, and the difference in performances between this combination and the tournament selection is statistically significant. Furthermore, we compare EvoDAG's performance using the selection heuristics against 18 classifiers that included traditional approaches as well as auto-machine-learning techniques. The results indicate that our proposal is competitive with state-of-art classifiers. Finally, it is worth to mention that EvoDAG is available as open source software.",
        "related_work": "Let us recall that Semantic GP uses the information in the target behavior, i.e., @math , to guide the search. Notably, Krawiec @cite_29 affirmed that aware semantic methods make search algorithms better informed. For example, Nguyen @cite_28 proposed Fitness Sharing, a technique that promotes dispersion and diversity of individuals. Their proposal consisted of calculating the individual fitness as @math , where @math is approximately equal to the number of individuals that behave similarly to individual @math .",
        "ref_abstract": {
            "@cite_28": {
                "mid": "2113192631",
                "abstract": "This paper investigates the efficiency of using semantic and syntactic distance metrics in fitness sharing with Genetic Programming (GP). We modify the implementation of fitness sharing to speed up its execution, and used two distance metrics in calculating the distance between individuals in fitness sharing: semantic distance and syntactic distance. We applied fitness sharing with these two distance metrics to a class of real-valued symbolic regression. Experimental results show that using semantic distance in fitness sharing helps to significantly improve the performance of GP more frequently, and results in faster execution times than with the syntactic distance. Moreover, we also analyse the impact of the fitness sharing parameters on GP performance helping to indicate appropriate values for fitness sharing using a semantic distance metric.",
                "doi": "https://doi.org/10.1007/978-3-642-29139-5_10",
                "title": "An Investigation of Fitness Sharing with Semantic and Syntactic Distance Metrics",
                "publication_year": 2012
            },
            "@cite_29": {
                "mid": "2043686990",
                "abstract": "Semantic genetic programming is a recent, rapidly growing trend in Genetic Programming (GP) that aims at opening the 'black box' of the evaluation function and make explicit use of more information on program behavior in the search. In the most common scenario of evaluating a GP program on a set of input-output examples (fitness cases), the semantic approach characterizes program with a vector of outputs rather than a single scalar value (fitness). The past research on semantic GP has demonstrated that the additional information obtained in this way facilitates designing more effective search operators. In particular, exploiting the geometric properties of the resulting semantic space leads to search operators with attractive properties, which have provably better theoretical characteristics than conventional GP operators. This in turn leads to dramatic improvements in experimental comparisons. The aim of the tutorial is to give a comprehensive overview of semantic methods in genetic programming, illustrate in an accessible way a formal geometric framework for program semantics to design provably good mutation and crossover operators for traditional GP problem domains, and to analyze rigorously their performance (runtime analysis). A number of real-world applications of this framework will be also presented. Other promising emerging approaches to semantics in GP will be reviewed. In particular, the recent developments in the behavioral programming, which aims at characterizing the entire program behavior (and not only program outputs) will be covered as well. Current challenges and future trends in semantic GP will be identified and discussed. Selected methods and concepts will be accompanied with live software demonstrations. Also, efficient implementation of semantic search operators may be challenging. We will illustrate very efficient, concise and elegant implementations of these operators, which are available for download from the web.",
                "doi": "https://doi.org/10.1145/2739482.2756587",
                "title": "Semantic Genetic Programming",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1901.05573",
        "mid": "2911027578",
        "abstract": "A key property underlying the success of evolutionary algorithms (EAs) is their global search behavior, which allows the algorithms to jump' from a current state to other parts of the search space, thereby avoiding to get stuck in local optima. This property is obtained through a random choice of the radius at which offspring are sampled from previously evaluated solutions. It is well known that, thanks to this global search behavior, the probability that an EA using standard bit mutation finds a global optimum of an arbitrary function @math tends to one as the number of function evaluations grows. This advantage over heuristics using a fixed search radius, however, comes at the cost of using non-optimal step sizes also in those regimes in which the optimal rate is stable for a long time. This downside results in significant performance losses for many standard benchmark problems. We introduce in this work a simple way to interpolate between the random global search of EAs and their deterministic counterparts which sample from a fixed radius only. To this end, we introduce , in which the binomial choice of the search radius is replaced by a normal distribution. Normalized standard bit mutation allows a straightforward way to control its variance, and hence the degree of randomness involved. We experiment with a self-adjusting choice of this variance, and demonstrate its effectiveness for the two classic benchmark problems LeadingOnes and OneMax. Our work thereby also touches a largely ignored question in discrete evolutionary computation: multi-dimensional parameter control.",
        "related_work": "As reasoned above, normalized standard bit mutation offers an elegant way to interpolate between deterministic mutation strengths and regular standard bit mutation, thus showing that Randomized Local Search (RLS) variants with their deterministic search radii and the (1+1) EA with mutation rate @math are essentially just different instantiations of the same meta-algorithm. Similar results also extend to population-based @math EAs. Note that normalized standard bit mutation also allows other degrees of randomization, thereby offering a wide range for further experimentation. In this context we note that for the special case of standard RLS (i.e., the greedy (1+1) hill climber that flips in each iteration exactly one uniformly chosen bit) a similar meta-model allowing to interpolate between the (1+1) EA and RLS is the (1+1) EA @math introduced in @cite_2 @cite_5 . This model, however, is much less flexible, and does not allow, for example, deterministic search radii greater than one.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2888223476",
                "abstract": "The idea to recombine two or more search points into a new solution is one of the main design principles of evolutionary computation (EC). Its usefulness in the combinatorial optimization context, however, is subject to a highly controversial discussion between EC practitioners and the broader Computer Science research community. While the former, naturally, report significant speedups procured by crossover, the belief that sexual reproduction cannot advance the search for high-quality solutions seems common, for example, amongst theoretical computer scientists. Examples that help understand the role of crossover in combinatorial optimization are needed to promote an intensified discussion on this subject.",
                "doi": "https://doi.org/10.1007/978-3-319-99259-4_3",
                "title": "A Simple Proof for the Usefulness of Crossover in Black-Box Optimization",
                "publication_year": 2018
            },
            "@cite_2": {
                "mid": "2012697403",
                "abstract": "Analyzing the computational complexity of evolutionary algorithms has become an accepted and important branch in evolutionary computation theory. This is usually done by analyzing the (expected) optimization time measured by means of the number of function evaluations and describing its growth as a function of a measure for the size of the search space. Most often asymptotic results describing only the order of growth are derived. This corresponds to classical analysis of (randomized) algorithms in algorithmics. Recently, the emerging field of algorithm engineering has demonstrated that for practical purposes this analysis can be too coarse and more details of the algorithm and its implementation have to be taken into account in order to obtain results that are valid in practice. Using a very recent analysis of a simple evolutionary algorithm as starting point it is shown that the same holds for evolutionary algorithms. Considering this example it is demonstrated that counting function evaluations more precisely can lead to results contradicting actual run times. Motivated by these limitations of computational complexity analysis an algorithm engineering-like approach is presented.",
                "doi": "https://doi.org/10.1145/1967654.1967656",
                "title": "Analysis of evolutionary algorithms",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1901.05375",
        "mid": "2963550527",
        "abstract": "Recent research on face detection, which is focused primarily on improving accuracy of detecting smaller faces, attempt to develop new anchor design strategies to facilitate increased overlap between anchor boxes and ground truth faces of smaller sizes. In this work, we approach the problem of small face detection with the motivation of enriching the feature maps using a density map estimation module. This module, inspired by recent crowd counting density estimation techniques, performs the task of estimating the per pixel density of people faces present in the image. Output of this module is employed to accentuate the feature maps from the backbone network using a feature enrichment module before being used for detecting smaller faces. The proposed approach can be used to complement recent anchor-design based novel methods to further improve their results. Experiments conducted on different datasets such as WIDER, FDDB and Pascal-Faces demonstrate the effectiveness of the proposed approach.",
        "related_work": "Zhang al @cite_0 proposed a single image-based method that involved multi-column network to extract features at different scales. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people head size due to perspective effect or image resolution. Onoro-Rubio and L 'o pez-Sastre in @cite_62 addressed the scale issue by proposing a scale aware counting model called Hydra CNN to estimate the object density maps. Sam al @cite_16 trained a Switching-CNN network to automatically choose the most optimal regressor among several independent regressors for a particular input patch. More recently, Sindagi and Patel @cite_3 proposed Contextual Pyramid CNN (CP-CNN), where they demonstrated significant improvements by fusing local and global context through classification networks.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2463631526",
                "abstract": "This paper aims to develop a method than can accurately estimate the crowd count from an individual image with arbitrary crowd density and arbitrary perspective. To this end, we have proposed a simple but effective Multi-column Convolutional Neural Network (MCNN) architecture to map the image to its crowd density map. The proposed MCNN allows the input image to be of arbitrary size or resolution. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people head size due to perspective effect or image resolution. Furthermore, the true density map is computed accurately based on geometry-adaptive kernels which do not need knowing the perspective map of the input image. Since exiting crowd counting datasets do not adequately cover all the challenging situations considered in our work, we have collected and labelled a large new dataset that includes 1198 images with about 330,000 heads annotated. On this challenging new dataset, as well as all existing datasets, we conduct extensive experiments to verify the effectiveness of the proposed model and method. In particular, with the proposed simple MCNN model, our method outperforms all existing methods. In addition, experiments show that our model, once trained on one dataset, can be readily transferred to a new dataset.",
                "doi": "https://doi.org/10.1109/cvpr.2016.70",
                "title": "Single-Image Crowd Counting via Multi-Column Convolutional Neural Network",
                "publication_year": 2016
            },
            "@cite_62": {
                "mid": "2519281173",
                "abstract": "In this paper we address the problem of counting objects instances in images. Our models are able to precisely estimate the number of vehicles in a traffic congestion, or to count the humans in a very crowded scene. Our first contribution is the proposal of a novel convolutional neural network solution, named Counting CNN (CCNN). Essentially, the CCNN is formulated as a regression model where the network learns how to map the appearance of the image patches to their corresponding object density maps. Our second contribution consists in a scale-aware counting model, the Hydra CNN, able to estimate object densities in different very crowded scenarios where no geometric information of the scene can be provided. Hydra CNN learns a multiscale non-linear regression model which uses a pyramid of image patches extracted at multiple scales to perform the final density prediction. We report an extensive experimental evaluation, using up to three different object counting benchmarks, where we show how our solutions achieve a state-of-the-art performance.",
                "doi": "https://doi.org/10.1007/978-3-319-46478-7_38",
                "title": "Towards Perspective-Free Object Counting with Deep Learning",
                "publication_year": 2016
            },
            "@cite_3": {
                "mid": "2963035940",
                "abstract": "We present a novel method called Contextual Pyramid CNN (CP-CNN) for generating high-quality crowd density and count estimation by explicitly incorporating global and local contextual information of crowd images. The proposed CP-CNN consists of four modules: Global Context Estimator (GCE), Local Context Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN). GCE is a VGG-16 based CNN that encodes global context and it is trained to classify input images into different density classes, whereas LCE is another CNN that encodes local context information and it is trained to perform patch-wise classification of input images into different density classes. DME is a multi-column architecture-based CNN that aims to generate high-dimensional feature maps from the input image which are fused with the contextual information estimated by GCE and LCE using F-CNN. To generate high resolution and high-quality density maps, F-CNN uses a set of convolutional and fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end fashion using a combination of adversarial loss and pixellevel Euclidean loss. Extensive experiments on highly challenging datasets show that the proposed method achieves significant improvements over the state-of-the-art methods.",
                "doi": "https://doi.org/10.1109/iccv.2017.206",
                "title": "Generating High-Quality Crowd Density Maps Using Contextual Pyramid CNNs",
                "publication_year": 2017
            },
            "@cite_16": {
                "mid": "2741077351",
                "abstract": "We propose a novel crowd counting model that maps a given crowd scene to its density. Crowd analysis is compounded by myriad of factors like inter-occlusion between people due to extreme crowding, high similarity of appearance between people and background elements, and large variability of camera view-points. Current state-of-the art approaches tackle these factors by using multi-scale CNN architectures, recurrent networks and late fusion of features from multi-column CNN with different receptive fields. We propose switching convolutional neural network that leverages variation of crowd density within an image to improve the accuracy and localization of the predicted crowd count. Patches from a grid within a crowd scene are relayed to independent CNN regressors based on crowd count prediction quality of the CNN established during training. The independent CNN regressors are designed to have different receptive fields and a switch classifier is trained to relay the crowd scene patch to the best CNN regressor. We perform extensive experiments on all major crowd counting datasets and evidence better performance compared to current state-of-the-art methods. We provide interpretable representations of the multichotomy of space of crowd scene patches inferred from the switch. It is observed that the switch relays an image patch to a particular CNN column based on density of crowd.",
                "doi": "https://doi.org/10.1109/cvpr.2017.429",
                "title": "Switching Convolutional Neural Network for Crowd Counting",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1901.04805",
        "mid": "2964234547",
        "abstract": "The widespread adoption of Internet of Things has led to many security issues. Recently, there have been malware attacks on IoT devices, the most prominent one being that of Mirai. IoT devices such as IP cameras, DVRs and routers were compromised by the Mirai malware and later large-scale DDoS attacks were propagated using those infected devices (bots) in October 2016. In this research, we develop a network-based algorithm which can be used to detect IoT bots infected by Mirai or similar malware in large-scale networks (e.g. ISP network). The algorithm particularly targets bots scanning the network for vulnerable devices since the typical scanning phase for botnets lasts for months and the bots can be detected much before they are involved in an actual attack. We analyze the unique signatures of the Mirai malware to identify its presence in an IoT device. Further, to optimize the usage of computational resources, we use a two-dimensional (2D) packet sampling approach, wherein we sample the packets transmitted by IoT devices both across time and across the devices. Leveraging the Mirai signatures identified and the 2D packet sampling approach, a bot detection algorithm is proposed. We use testbed measurements and simulations to study the relationship between bot detection delays and the sampling frequencies for device packets. Subsequently, we derive insights from the obtained results and use them to design our proposed bot detection algorithm. Finally, we discuss the deployment of our bot detection algorithm and the countermeasures which can be taken post detection.",
        "related_work": "There has also been some research on intrusion detection and anomaly detection systems for IoT. A whitelist-based intrusion detection system for IoT devices (Heimdall) has been presented in @cite_1 . Heimdall is based on dynamic profile learning and is designed to work on routers acting as gateways for IoT devices. The authors in @cite_2 propose an intrusion detection model for IoT backbone networks leveraging two-layer dimension reduction and two-tier classification techniques to detect U2R (User-to-Root) and R2L (Remote-to-Local) attacks. In a recently published paper @cite_11 , deep-autoencoders based anomaly detection has been used to detect attacks launched from IoT botnets. The method consists of extraction of statistical features from behavioral snapshots of normal IoT device traffic captures, training of a deep learning-based autoencoder (for each IoT device) on the extracted features and comparison of the reconstruction error for traffic observations with a threshold for normal-anomalous classification. The proposed detection method was evaluated on Mirai and BASHLITE botnets formed using commercial IoT devices.",
        "ref_abstract": {
            "@cite_11": {
                "mid": "2799758613",
                "abstract": "The proliferation of IoT devices that can be more easily compromised than desktop computers has led to an increase in IoT-based botnet attacks. To mitigate this threat, there is a need for new methods that detect attacks launched from compromised IoT devices and that differentiate between hours- and milliseconds-long IoT-based attacks. In this article, we propose a novel network-based anomaly detection method for the IoT called N-BaIoT that extracts behavior snapshots of the network and uses deep autoencoders to detect anomalous network traffic from compromised IoT devices. To evaluate our method, we infected nine commercial IoT devices in our lab with two widely known IoT-based botnets, Mirai and BASHLITE. The evaluation results demonstrated our proposed methods ability to accurately and instantly detect the attacks as they were being launched from the compromised IoT devices that were part of a botnet.",
                "doi": "https://doi.org/10.1109/mprv.2018.03367731",
                "title": "N-BaIoT\u2014Network-Based Detection of IoT Botnet Attacks Using Deep Autoencoders",
                "publication_year": 2018
            },
            "@cite_1": {
                "mid": "2614230424",
                "abstract": "The Internet of Things (IoT) is built of many small smart objects continuously connected to the Internet. This makes these devices an easy target for attacks exploiting vulnerabilities at the network, application, and mobile level. With that it comes as no surprise that distributed denial of service attacks leveraging these vulnerable devices have become a new standard for effective botnets. In this paper, we propose Heimdall, a whitelist-based intrusion detection technique tailored to IoT devices. Heimdall operates on routers acting as gateways for IoT as a homogeneous defense for all devices behind the router. Our experimental results show that our defense mechanism is effective and has minimal overhead.",
                "doi": "https://doi.org/10.1109/jiot.2017.2704093",
                "title": "Heimdall: Mitigating the Internet of Insecure Things",
                "publication_year": 2017
            },
            "@cite_2": {
                "mid": "2557450880",
                "abstract": "With increasing reliance on Internet of Things (IoT) devices and services, the capability to detect intrusions and malicious activities within IoT networks is critical for resilience of the network infrastructure. In this paper, we present a novel model for intrusion detection based on two-layer dimension reduction and two-tier classification module, designed to detect malicious activities such as User to Root (U2R) and Remote to Local (R2L) attacks. The proposed model is using component analysis and linear discriminate analysis of dimension reduction module to spate the high dimensional dataset to a lower one with lesser features. We then apply a two-tier classification module utilizing Naive Bayes and Certainty Factor version of K-Nearest Neighbor to identify suspicious behaviors. The experiment results using NSL-KDD dataset shows that our model outperforms previous models designed to detect U2R and R2L attacks.",
                "doi": "https://doi.org/10.1109/tetc.2016.2633228",
                "title": "A Two-Layer Dimension Reduction and Two-Tier Classification Model for Anomaly-Based Intrusion Detection in IoT Backbone Networks",
                "publication_year": 2019
            }
        }
    },
    {
        "aid": "1907.06143",
        "mid": "2962163524",
        "abstract": "In common real-world robotic operations, action and state spaces can be vast and sometimes unknown, and observations are often relatively sparse. How do we learn the full topology of action and state spaces when given only few and sparse observations? Inspired by the properties of grid cells in mammalian brains, we build a generative model that enforces a normalized pairwise distance constraint between the latent space and output space to achieve data-efficient discovery of output spaces. This method achieves substantially better results than prior generative models, such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs). Prior models have the common issue of mode collapse and thus fail to explore the full topology of output space. We demonstrate the effectiveness of our model on various datasets both qualitatively and quantitatively.",
        "related_work": "Recent works have made substantial progress in imposing diversity constraint on the latent space of a generative model. In particular, Liu et. al. @cite_49 proposes the normalized diversification technique that effectively solves the problem of mode collapsing. Building on top of their prior work, we use a similar technique to learn an accurate encoding of action and state spaces in physical manipulation tasks. To our knowledge, our model is the first to use normalized diversification in these applications.",
        "ref_abstract": {
            "@cite_49": {
                "mid": "2931659475",
                "abstract": "Generating diverse yet specific data is the goal of the generative adversarial network (GAN), but it suffers from the problem of mode collapse. We introduce the concept of normalized diversity which force the model to preserve the normalized pairwise distance between the sparse samples from a latent parametric distribution and their corresponding high-dimensional outputs. The normalized diversification aims to unfold the manifold of unknown topology and non-uniform distribution, which leads to safe interpolation between valid latent variables. By alternating the maximization over the pairwise distance and updating the total distance (normalizer), we encourage the model to actively explore in the high-dimensional output space. We demonstrate that by combining the normalized diversity loss and the adversarial loss, we generate diverse data without suffering from mode collapsing. Experimental results show that our method achieves consistent improvement on unsupervised image generation, conditional image generation and hand pose estimation over strong baselines.",
                "doi": "https://doi.org/10.48550/arxiv.1904.03608",
                "title": "Normalized Diversification",
                "publication_year": 2019
            }
        }
    },
    {
        "aid": "1907.06358",
        "mid": "2963432486",
        "abstract": "Due to the high resolution of pathological images, the automated semantic segmentation in the medical pathological images has shown greater challenges than that in natural images. Sliding Window method has shown its effect on solving problem caused by the high resolution of whole slide images (WSI). However, owing to its localization, Sliding Window method also suffers from lack of global information. In this paper, a dual input semantic segmentation network based on attention is proposed, in which, one input provides small-scale fine information, the other input provides large-scale coarse information. Compared with single input methods, our method based on dual inputs and attention: DA-RefineNet exhibits a dramatic performance improvement on ICIAR2018 breast cancer segmentation task.",
        "related_work": "In this paper we adopt Refinenet @cite_14 as the baseline. The main difference between Refinenet and Unet @cite_26 lies in the unique block \"Refine Block\". The Refine Block is a unique feature fusion block, which can be divided into three parts. (1) Residual Convolution Unit (RCU). This is a convolutional module based on the residual connection design. Compared with the original Resnet @cite_6 , the BN layer is removed, and the parameter amount is reduced to be used as a feature extractor. (2) Multi-size fusion. Our task is semantic segmentation, with the output and the input in the same size, the blocks except for Block No.4 being dual input, and the two input in different scales. Thus multi-size fusion is applied for upsampleing and feature fusion. (3) Chain residual pooling(CRP). The module efficiently fuses features through convolution pooling operations of different window sizes. Through this chained pooling operation, the receptive field is expanded. At the same time, multi-scale information is merged through short jump connections, which let gradient go to directly from one module to another.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2563705555",
                "abstract": "Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense classification problems such as semantic segmentation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular, we achieve an intersection-over-union score of 83.4 on the challenging PASCAL VOC 2012 dataset, which is the best reported result to date.",
                "doi": "https://doi.org/10.1109/cvpr.2017.549",
                "title": "RefineNet: Multi-path Refinement Networks for High-Resolution Semantic Segmentation",
                "publication_year": 2017
            },
            "@cite_6": {
                "mid": "2194775991",
                "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\u20148\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57 error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                "doi": "https://doi.org/10.1109/cvpr.2016.90",
                "title": "Deep Residual Learning for Image Recognition",
                "publication_year": 2016
            },
            "@cite_26": {
                "mid": "1901129140",
                "abstract": "There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http: lmb.informatik.uni-freiburg.de people ronneber u-net .",
                "doi": "https://doi.org/10.1007/978-3-319-24574-4_28",
                "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1907.06032",
        "mid": "2956802405",
        "abstract": "Subspace segmentation or subspace learning is a challenging and complicated task in machine learning. This paper builds a primary frame and solid theoretical bases for the minimal subspace segmentation (MSS) of finite samples. Existence and conditional uniqueness of MSS are discussed with conditions generally satisfied in applications. Utilizing weak prior information of MSS, the minimality inspection of segments is further simplified to the prior detection of partitions. The MSS problem is then modeled as a computable optimization problem via self-expressiveness of samples. A closed form of representation matrices is first given for the self-expressiveness, and the connection of diagonal blocks is then addressed. The MSS model uses a rank restriction on the sum of segment ranks. Theoretically, it can retrieve the minimal sample subspaces that could be heavily intersected. The optimization problem is solved via a basic manifold conjugate gradient algorithm, alternative optimization and hybrid optimization, taking into account of solving both the primal MSS problem and its pseudo-dual problem. The MSS model is further modified for handling noisy data, and solved by an ADMM algorithm. The reported experiments show the strong ability of the MSS method on retrieving minimal sample subspaces that are heavily intersected.",
        "related_work": "As shown in Theorem , the LRR gets the MSDR if and only if the subspaces are independent, that is, @math . This condition implies that each subspace does not intersect with the sum of other subspaces, or equivalently, @math , which is much stricter than that given in Theorem . It is proven by @cite_2 that the iPursuit can separate two subspaces ( @math ) with a high probability. This is one of the special cases shown in Corollary . The condition for LRSSC is similar with that of SSC in the same form and is stricter. We omit the comparison the condition with that of LRSSC, but a detailed comparison with SSC is given below.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "2185081213",
                "abstract": "In subspace clustering, a group of data points belonging to a union of subspaces are assigned membership to their respective subspaces. This paper presents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of subspace clustering using a new geometrical idea whereby subspaces are identified based on their relative novelties. We present two frameworks in which the idea of innovation pursuit is used to distinguish the subspaces. Underlying the first framework is an iterative method that finds the subspaces consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data potentially orthogonal to all subspaces except for the one to be identified in one step of the algorithm. A detailed mathematical analysis is provided establishing sufficient conditions for iPursuit to correctly cluster the data. The proposed approach can provably yield exact clustering even when the subspaces have significant intersections. It is shown that the complexity of the iterative approach scales only linearly in the number of data points and subspaces, and quadratically in the dimension of the subspaces. The second framework integrates iPursuit with spectral clustering to yield a new variant of spectral-clustering-based algorithms. The numerical simulations with both real and synthetic data demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms, more so for subspaces with significant intersections, and that it significantly improves the state-of-the-art result for subspace-segmentation-based face clustering.",
                "doi": "https://doi.org/10.1109/tsp.2017.2749206",
                "title": "Innovation Pursuit: A New Approach to Subspace Clustering",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1907.05944",
        "mid": "2956358468",
        "abstract": "We study various discrete nonlinear combinatorial optimization problems in an online learning framework. In the first part, we address the question of whether there are negative results showing that getting a vanishing (or even vanishing approximate) regret is computational hard. We provide a general reduction showing that many (min-max) polynomial time solvable problems not only do not have a vanishing regret, but also no vanishing approximation @math -regret, for some @math (unless @math ). Then, we focus on a particular min-max problem, the min-max version of the vertex cover problem which is solvable in polynomial time in the offline case. The previous reduction proves that there is no @math -regret online algorithm, unless Unique Game is in @math ; we prove a matching upper bound providing an online algorithm based on the online gradient descent method. Then, we turn our attention to online learning algorithms that are based on an offline optimization oracle that, given a set of instances of the problem, is able to compute the optimum static solution. We show that for different nonlinear discrete optimization problems, it is strongly @math -hard to solve the offline optimization oracle, even for problems that can be solved in polynomial time in the static case (e.g. min-max vertex cover, min-max perfect matching, etc.). On the positive side, we present an online algorithm with vanishing regret that is based on the follow the perturbed leader algorithm for a generalized knapsack problem.",
        "related_work": "Online Learning, or Online Convex Optimization, is an active research domain. In this section, we only summarize works which are directly related to ours. We refer the reader to comprehensive books @cite_15 @cite_1 and references therein for a more complete overview. The first no-regret algorithm has been given by . Subsequently, and gave improved algorithms with regret @math where @math is the size of the action space. However, these algorithms have running-time @math which is exponential in the size of the input for many applications, in particular for combinatorial optimization problems. An intriguing question is whether there exists a no-regret online algorithm with running-time polynomial in @math . proved that no such algorithm exists in general settings without any assumption on the structure. Designing online polynomial-time algorithms with approximation and vanishing regret guarantees for combinatorial optimization problems is a major research agenda.",
        "ref_abstract": {
            "@cite_15": {
                "mid": "2077723394",
                "abstract": "Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.",
                "doi": "https://doi.org/10.1561/9781601985477",
                "title": "Online Learning and Online Convex Optimization",
                "publication_year": 2011
            },
            "@cite_1": {
                "mid": "2513180554",
                "abstract": "This monograph portrays optimization as a process. In many practical applications the environment is so complex that it is infeasible to lay out a comprehensive theoretical model and use classical algorithmic theory and mathematical optimization. It is necessary as well as beneficial to take a robust approach, by applying an optimization method that learns as one goes along, learning from experience as more aspects of the problem are observed. This view of optimization as a process has become prominent in varied fields and has led to some spectacular success in modeling and systems that are now part of our daily lives.",
                "doi": "https://doi.org/10.1561/9781680831719",
                "title": "Introduction to Online Convex Optimization",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1901.02936",
        "mid": "2909346791",
        "abstract": "Linear mixed models (LMMs) are widely used for heritability estimation in genome-wide association studies (GWAS). In standard approaches to heritability estimation with LMMs, a genetic relationship matrix (GRM) must be specified. In GWAS, the GRM is frequently a correlation matrix estimated from the study population's genotypes, which corresponds to a normalized Euclidean distance kernel. In this paper, we show that reliance on the Euclidean distance kernel contributes to several unresolved modeling inconsistencies in heritability estimation for GWAS. These inconsistencies can cause biased heritability estimates in the presence of linkage disequilibrium (LD), depending on the distribution of causal variants. We show that these biases can be resolved (at least at the modeling level) if one adopts a Mahalanobis distance-based GRM for LMM analysis. Additionally, we propose a new definition of partitioned heritability -- the heritability attributable to a subset of genes or single nucleotide polymorphisms (SNPs) -- using the Mahalanobis GRM, and show that it inherits many of the nice consistency properties identified in our original analysis. Partitioned heritability is a relatively new area for GWAS analysis, where inconsistency issues related to LD have previously been known to be especially pernicious.",
        "related_work": "Recently, in independent work, @cite_0 proposed using the Mahalanobis kernel in a similar way for heritability esitmation with GWAS data. 's paper primarily focuses on empirical analysis, using both simulated and real datasets to illsutrate advantages of the Mahalanobis kernel. The present work contains more precise mathematical and statistical justification for much of the work in @cite_0 , and introduces statistical principles (e.g. @math -heritability in Section ) that can be extended to other targeted application areas and genetics (like partitioning heritability).",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2773459529",
                "abstract": "Single nucleotide polymorphism (SNP)-heritability estimation is an important topic in several research fields, including animal, plant and human genetics, as well as in ecology. Linear mixed model estimation of SNP-heritability uses the structures of genomic relationships between individuals, which is constructed from genome-wide sets of SNP-markers that are generally weighted equally in their contributions. Proposed methods to handle dependence between SNPs include, \u201cthinning\u201d the marker set by linkage disequilibrium (LD)-pruning, the use of haplotype-tagging of SNPs, and LD-weighting of the SNP-contributions. For improved estimation, we propose a new conceptual framework for genomic relationship matrix, in which Mahalanobis distance-based LD-correction is used in a linear mixed model estimation of SNP-heritability. The superiority of the presented method is illustrated and compared to mixed-model analyses using a VanRaden genomic relationship matrix, a matrix used by GCTA and a matrix employing LD-weighting (as implemented in the LDAK software) in simulated (using real human, rice and cattle genotypes) and real (maize, rice and mice) datasets. Despite of the computational difficulties, our results suggest that by using the proposed method one can improve the accuracy of SNP-heritability estimates in datasets with high LD.",
                "doi": "https://doi.org/10.1038/s41437-017-0023-4",
                "title": "A novel linkage-disequilibrium corrected genomic relationship matrix for SNP-heritability estimation and genomic prediction",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1812.11852",
        "mid": "2908069757",
        "abstract": "The vast majority of photos taken today are by mobile phones. While their quality is rapidly growing, due to physical limitations and cost constraints, mobile phone cameras struggle to compare in quality with DSLR cameras. This motivates us to computationally enhance these images. We extend upon the results of , where they are able to translate images from compact mobile cameras into images with comparable quality to high-resolution photos taken by DSLR cameras. However, the neural models employed require large amounts of computational resources and are not lightweight enough to run on mobile devices. We build upon the prior work and explore different network architectures targeting an increase in image quality and speed. With an efficient network architecture which does most of its processing in a lower spatial resolution, we achieve a significantly higher mean opinion score (MOS) than the baseline while speeding up the computation by 6.3 times on a consumer-grade CPU. This suggests a promising direction for neural-network-based photo enhancement using the phone hardware of the future.",
        "related_work": "A considerable body of work is dedicated to automatic photo enhancement. However, it traditionally only focused on a specific subproblem, such as super-resolution, denoising, deblurring, or colorization. All of these subproblems are tackled simultaneously when we generate plausible high-quality photos from low-end ones. Furthermore, these older works commonly train with artifacts that have been artificially applied to the target image dataset. Recreating and simulating all the flaws in one camera given a picture from another is close to impossible, therefore in order to achieve real-world photo enhancement we use the photos simultaneously captured by a capture rig from Ignatov @cite_7 . Despite their limitations, the related works contain many useful ideas, which we briefly review in this section.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "2607202125",
                "abstract": "Despite a rapid rise in the quality of built-in smartphone cameras, their physical limitations \u2013 small sensor size, compact lenses and the lack of specific hardware, \u2013 impede them to achieve the quality results of DSLR cameras. In this work we present an end-to-end deep learning approach that bridges this gap by translating ordinary photos into DSLR-quality images. We propose learning the translation function using a residual convolutional neural network that improves both color rendition and image sharpness. Since the standard mean squared loss is not well suited for measuring perceptual image quality, we introduce a composite perceptual error function that combines content, color and texture losses. The first two losses are defined analytically, while the texture loss is learned in an adversarial fashion. We also present DPED, a large-scale dataset that consists of real photos captured from three different phones and one high-end reflex camera. Our quantitative and qualitative assessments reveal that the enhanced image quality is comparable to that of DSLR-taken photos, while the methodology is generalized to any type of digital camera.",
                "doi": "https://doi.org/10.1109/iccv.2017.355",
                "title": "DSLR-Quality Photos on Mobile Devices with Deep Convolutional Networks",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1812.10668",
        "mid": "2906853528",
        "abstract": "Abstract Maximizing resource utilization by performing an efficient resource provisioning is a key factor for any cloud provider: commercial actors can maximize their revenues, whereas scientific and non-commercial providers can maximize their infrastructure utilization. Traditionally, batch systems have allowed data centers to fill their resources as much as possible by using backfilling and similar techniques. However, in an IaaS cloud, where virtual machines are supposed to live indefinitely, or at least as long as the user is able to pay for them, these policies are not easily implementable. In this work we present a new scheduling algorithm for IaaS providers that is able to support preemptible instances, that can be stopped by higher priority requests without introducing large modifications in the current cloud schedulers. This scheduler enables the implementation of new cloud usage and payment models that allow more efficient usage of the resources and potential new revenue sources for commercial providers. We also study the correctness and the performance overhead of the proposed scheduler against existing solutions.",
        "related_work": "The resource provisioning from cloud computing infrastructures using Spot Instances or similar mechanisms has been addressed profusely in the scientific literature in the last years @cite_10 . However, the vast majority of this work has been done from the users' perspective when using and consuming Spot Instances @cite_31 and few works tackle the problem from the resource provider standpoint.",
        "ref_abstract": {
            "@cite_31": {
                "mid": "1646360816",
                "abstract": "An important feature of most cloud computing solutions is auto-scaling, an operation that enables dynamic changes on resource capacity. Auto-scaling algorithms generally take into account aspects such as system load and response time to determine when and by how much a resource pool capacity should be extended or shrunk. In this article, we propose a scheduling algorithm and auto-scaling triggering strategies that explore user patience, a metric that estimates the perception end-users have from the Quality of Service (QoS) delivered by a service provider based on the ratio between expected and actual response times for each request. The proposed strategies help reduce costs with resource allocation while maintaining perceived QoS at adequate levels. Results show reductions on resource-hour consumption by up to approximately 9 compared to traditional approaches. Mechanisms for resource auto-scaling in clouds considering users' patience.Methods for determining the step size of scaling operations under bound and unbounded maximum capacity.Users patience model inspired in prospect theory.",
                "doi": "https://doi.org/10.1016/j.future.2015.09.001",
                "title": "Impact of user patience on auto-scaling resource capacity for cloud services",
                "publication_year": 2016
            },
            "@cite_10": {
                "mid": "2034603054",
                "abstract": "Resource management in a cloud environment is a hard problem, due to: the scale of modern data centers; the heterogeneity of resource types and their interdependencies; the variability and unpredictability of the load; as well as the range of objectives of the different actors in a cloud ecosystem. Consequently, both academia and industry began significant research efforts in this area. In this paper, we survey the recent literature, covering 250+ publications, and highlighting key results. We outline a conceptual framework for cloud resource management and use it to structure the state-of-the-art review. Based on our analysis, we identify five challenges for future investigation. These relate to: providing predictable performance for cloud-hosted applications; achieving global manageability for cloud systems; engineering scalable resource management systems; understanding economic behavior and cloud pricing; and developing solutions for the mobile cloud paradigm .",
                "doi": "https://doi.org/10.1007/s10922-014-9307-7",
                "title": "Resource Management in Clouds: Survey and Research Challenges",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1812.09922",
        "mid": "2906539509",
        "abstract": "High bandwidth requirements are an obstacle for accelerating the training and inference of deep neural networks. Most previous research focuses on reducing the size of kernel maps for inference. We analyze parameter sparsity of six popular convolutional neural networks - AlexNet, MobileNet, ResNet-50, SqueezeNet, TinyNet, and VGG16. Of the networks considered, those using ReLU (AlexNet, SqueezeNet, VGG16) contain a high percentage of 0-valued parameters and can be statically pruned. Networks with Non-ReLU activation functions in some cases may not contain any 0-valued parameters (ResNet-50, TinyNet). We also investigate runtime feature map usage and find that input feature maps comprise the majority of bandwidth requirements when depth-wise convolution and point-wise convolutions used. We introduce dynamic runtime pruning of feature maps and show that 10 of dynamic feature map execution can be removed without loss of accuracy. We then extend dynamic pruning to allow for values within an epsilon of zero and show a further 5 reduction of feature map loading with a 1 loss of accuracy in top-1.",
        "related_work": "Rather, we look at all the feature maps and remove the maps that are dynamically determined not to be participating in the classification. Rhu @cite_10 recently described a compressing DMA engine (cDMA) that improved virtualized DNNs (vDNN) performance by 32 technique prunes by channel rather than elements. This benefits instruction set processors, particularly signal processors, because data can be easily loaded into the processor using sliding windows.",
        "ref_abstract": {
            "@cite_10": {
                "mid": "2962821792",
                "abstract": "Popular deep learning frameworks require users to fine-tune their memory usage so that the training data of a deep neural network (DNN) fits within the GPU physical memory. Prior work tries to address this restriction by virtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be utilized for memory allocations. Despite its merits, virtualizing memory can incur significant performance overheads when the time needed to copy data back and forth from CPU memory is higher than the latency to perform DNN computations. We introduce a high-performance virtualization strategy based on a \"compressing DMA engine\" (cDMA) that drastically reduces the size of the data structures that are targeted for CPU-side allocations. The cDMA engine offers an average 2.6x (maximum 13.8x) compression ratio by exploiting the sparsity inherent in offloaded data, improving the performance of virtualized DNNs by an average 53 (maximum 79 ) when evaluated on an NVIDIA Titan Xp.",
                "doi": "https://doi.org/10.1109/hpca.2018.00017",
                "title": "Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep Neural Networks",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1812.07894",
        "mid": "2883454930",
        "abstract": "Smartphone apps usually have access to sensitive user data such as contacts, geo-location, and account credentials and they might share such data to external entities through the Internet or with other apps. Confidentiality of user data could be breached if there are anomalies in the way sensitive data is handled by an app which is vulnerable or malicious. Existing approaches that detect anomalous sensitive data flows have limitations in terms of accuracy because the definition of anomalous flows may differ for different apps with different functionalities; it is normal for \"Health\" apps to share heart rate information through the Internet but is anomalous for \"Travel\" apps. In this paper, we propose a novel approach to detect anomalous sensitive data flows in Android apps, with improved accuracy. To achieve this objective, we first group trusted apps according to the topics inferred from their functional descriptions. We then learn sensitive information flows with respect to each group of trusted apps. For a given app under analysis, anomalies are identified by comparing sensitive information flows in the app against those flows learned from trusted apps grouped under the same topic. In the evaluation, information flow is learned from 11,796 trusted apps. We then checked for anomalies in 596 new (benign) apps and identified 2 previously-unknown vulnerable apps related to anomalous flows. We also analyzed 18 malware apps and found anomalies in 6 of them.",
        "related_work": "The approaches proposed in Mudflow @cite_13 and Chabada @cite_19 are closely related to ours. Mudflow @cite_13 is a tool for malware detection based on sensitive information flow. Similar to our approach, they rely on static taint analysis to detect flows of sensitive data towards potential leaks. Then, these flows are used to train a @math SVM one-class classifier and later classify new apps. While we also use static analysis, the main difference is that we consider the dominant topic inferred from app description as an important feature for the classification. Our empirical evaluation shows that dominant topics are fundamental to achieve a higher in anomaly detection. Moreover, our approach not only focuses on detecting malware, but also focuses on vulnerable and defective apps. Lastly, while Mudflow applies intra-component static analysis, we use inter-component analysis for covering flows across components.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "2168649891",
                "abstract": "How do we know a program does what it claims to do? After clustering Android apps by their description topics, we identify outliers in each cluster with respect to their API usage. A \"weather\" app that sends messages thus becomes an anomaly; likewise, a \"messaging\" app would typically not be expected to access the current location. Applied on a set of 22,500+ Android applications, our CHABADA prototype identified several anomalies; additionally, it flagged 56 of novel malware as such, without requiring any known malware patterns.",
                "doi": "https://doi.org/10.1145/2568225.2568276",
                "title": "Checking app behavior against app descriptions",
                "publication_year": 2014
            },
            "@cite_13": {
                "mid": "2071536101",
                "abstract": "What is it that makes an app malicious? One important factor is that malicious apps treat sensitive data differently from benign apps. To capture such differences, we mined 2,866 benign Android applications for their data flow from sensitive sources, and compare these flows against those found in malicious apps. We find that (a) for every sensitive source, the data ends up in a small number of typical sinks; (b) these sinks differ considerably between benign and malicious apps; (c) these differences can be used to flag malicious apps due to their abnormal data flow; and (d) malicious apps can be identified by their abnormal data flow alone, without requiring known malware samples. In our evaluation, our mudflow prototype correctly identified 86.4 of all novel malware, and 90.1 of novel malware leaking sensitive data.",
                "doi": "https://doi.org/10.5555/2818754.2818808",
                "title": "Mining apps for abnormal usage of sensitive data",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1812.06744",
        "mid": "2904146064",
        "abstract": "[Context.] The success of deep learning makes its usage more and more tempting in safety-critical applications. However such applications have historical standards (e.g., DO178, ISO26262) which typically do not envision the usage of machine learning. We focus in particular on of software artifacts, i.e., code modules, functions, or statements (depending on the desired granularity). [Problem.] Both code and requirements are a problem when dealing with deep neural networks: code constituting the network is not comparable to classical code; furthermore, requirements for applications where neural networks are required are typically very hard to specify: even though high-level requirements can be defined, it is very hard to make such requirements concrete enough, that one can qualify them of low-level requirements. An additional problem is that deep learning is in practice very much based on trial-and-error, which makes the final result hard to explain without the previous iterations. [Proposed solution.] We investigate which artifacts could play a similar role to code or low-level requirements in neural network development and propose various traces which one could possibly consider as a replacement for classical notions. We also propose a form of traceability (and new artifacts) in order to deal with the particular trial-and-error development process for deep learning.",
        "related_work": "In general, the safety of DNNs is commonly recognized as a huge challenge @cite_1 . There are more and more attempts towards the certification, verification, or explainability of DNNs, of which we provide now a short overview. None of them however (and, as far as we know, no other work either) addresses the traceability of DNNs.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2753457114",
                "abstract": "We propose a methodology for designing dependable Artificial Neural Networks (ANN) by extending the concepts of understandability, correctness, and validity that are crucial ingredients in existing certification standards. We apply the concept in a concrete case study in designing a high-way ANN-based motion predictor to guarantee safety properties such as impossibility for the ego vehicle to suggest moving to the right lane if there exists another vehicle on its right.",
                "doi": "https://doi.org/10.48550/arxiv.1709.00911",
                "title": "Neural Networks for Safety-Critical Applications - Challenges,\n  Experiments and Perspectives",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1907.01957",
        "mid": "2955363999",
        "abstract": "State-of-the-art end-to-end automatic speech recognition (ASR) extracts acoustic features from input speech signal every 10 ms which corresponds to a frame rate of 100 frames second. In this report, we investigate the use of high-frame-rate features extraction in end-to-end ASR. High frame rates of 200 and 400 frames second are used in the features extraction and provide additional information for end-to-end ASR. The effectiveness of high-frame-rate features extraction is evaluated independently and in combination with speed perturbation based data augmentation. Experiments performed on two speech corpora, Wall Street Journal (WSJ) and CHiME-5, show that using high-frame-rate features extraction yields improved performance for end-to-end ASR, both independently and in combination with speed perturbation. On WSJ corpus, the relative reduction of word error rate (WER) yielded by high-frame-rate features extraction independently and in combination with speed perturbation are up to 21.3 and 24.1 , respectively. On CHiME-5 corpus, the corresponding relative WER reductions are up to 2.8 and 7.9 , respectively, on the test data recorded by microphone arrays and up to 11.8 and 21.2 , respectively, on the test data recorded by binaural microphones.",
        "related_work": "Speed perturbation @cite_5 is a data augmentation technique which creates warped time signals in addition to the original speech signals. Given an audio signal of length @math and a warping factor @math , speed perturbation creates a new signal with duration @math by resampling the original signal with a sampling rate of @math , where @math is the sampling rate of the original signal. Speed perturbation shifts the speech spectrum and also results in change in number of frames as the duration of the resulting signal is different @cite_5 .",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2407080277",
                "abstract": "Data augmentation is a common strategy adopted to increase the quantity of training data, avoid overfitting and improve robustness of the models. In this paper, we investigate audio-level speech augmentation methods which directly process the raw signal. The method we particularly recommend is to change the speed of the audio signal, producing 3 versions of the original signal with speed factors of 0.9, 1.0 and 1.1. The proposed technique has a low implementation cost, making it easy to adopt. We present results on 4 different LVCSR tasks with training data ranging from 100 hours to 1000 hours, to examine the effectiveness of audio augmentation in a variety of data scenarios. An average relative improvement of 4.3 was observed across the 4 tasks.",
                "doi": "https://doi.org/10.21437/interspeech.2015-711",
                "title": "Audio augmentation for speech recognition",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1812.05418",
        "mid": "2905153446",
        "abstract": "In this work, we propose a domain flow generation(DLOW) approach to model the domain shift between two domains by generating a continuous sequence of intermediate domains flowing from one domain to the other. The benefits of our DLOW model are two-fold. First, it is able to transfer source images into different styles in the intermediate domains. The transferred images smoothly bridge the gap between source and target domains, thus easing the domain adaptation task. Second, when multiple target domains are provided in the training phase, our DLOW model can be learnt to generate new styles of images that are unseen in the training data. We implement our DLOW model based on the state-of-the-art CycleGAN. A domainness variable is introduced to guide the model to generate the desired intermediate domain images. In the inference phase, a flow of various styles of images can be obtained by varying the domainness variable. We demonstrate the effectiveness of our approach for both cross-domain semantic segmentation and the style generalization tasks on benchmark datasets.",
        "related_work": "Our work is partially inspired by SGF @cite_29 and GFK @cite_55 , which have shown that the intermediate domains between source and target domains are useful for addressing the domain adaptation problem. They represented each domain as a subspace, and then connected them on Grassmannian manifold to model intermediate domains. Different from them, we model the intermediate domains by directly translate images on pixel level. This allows us to easily improve the existing deep domain adaptation models by using the translated images as training data. Moreover, our model can also be applied to image-level domain generalization by generating mixed-style images.",
        "ref_abstract": {
            "@cite_55": {
                "mid": "2149466042",
                "abstract": "In real-world applications of visual recognition, many factors \u2014 such as pose, illumination, or image quality \u2014 can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied. As such, the classifiers often perform poorly on the target domain. Domain adaptation techniques aim to correct the mismatch. Existing approaches have concentrated on learning feature representations that are invariant across domains, and they often do not directly exploit low-dimensional structures that are intrinsic to many vision datasets. In this paper, we propose a new kernel-based method that takes advantage of such structures. Our geodesic flow kernel models domain shift by integrating an infinite number of subspaces that characterize changes in geometric and statistical properties from the source to the target domain. Our approach is computationally advantageous, automatically inferring important algorithmic parameters without requiring extensive cross-validation or labeled data from either domain. We also introduce a metric that reliably measures the adaptability between a pair of source and target domains. For a given target domain and several source domains, the metric can be used to automatically select the optimal source domain to adapt and avoid less desirable ones. Empirical studies on standard datasets demonstrate the advantages of our approach over competing methods.",
                "doi": "https://doi.org/10.1109/cvpr.2012.6247911",
                "title": "Geodesic flow kernel for unsupervised domain adaptation",
                "publication_year": 2012
            },
            "@cite_29": {
                "mid": "2128053425",
                "abstract": "Adapting the classifier trained on a source domain to recognize instances from a new target domain is an important problem that is receiving recent attention. In this paper, we present one of the first studies on unsupervised domain adaptation in the context of object recognition, where we have labeled data only from the source domain (and therefore do not have correspondences between object categories across domains). Motivated by incremental learning, we create intermediate representations of data between the two domains by viewing the generative subspaces (of same dimension) created from these domains as points on the Grassmann manifold, and sampling points along the geodesic between them to obtain subspaces that provide a meaningful description of the underlying domain shift. We then obtain the projections of labeled source domain data onto these subspaces, from which a discriminative classifier is learnt to classify projected data from the target domain. We discuss extensions of our approach for semi-supervised adaptation, and for cases with multiple source and target domains, and report competitive results on standard datasets.",
                "doi": "https://doi.org/10.1109/iccv.2011.6126344",
                "title": "Domain adaptation for object recognition: An unsupervised approach",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1907.01046",
        "mid": "2955907793",
        "abstract": "Detailed knowledge about the electrical power consumption in industrial production environments is a prerequisite to reduce and optimize their power consumption. Today's industrial production sites are equipped with a variety of sensors that, inter alia, monitor electrical power consumption in detail. However, these environments often lack an automated data collation and analysis. We present a system architecture that integrates different sensors and analyzes and visualizes the power consumption of devices, machines, and production plants. It is designed with a focus on scalability to support production environments of various sizes and to handle varying loads. We argue that a scalable architecture in this context must meet requirements for fault tolerance, extensibility, real-time data processing, and resource efficiency. As a solution, we propose a microservice-based architecture augmented by big data and stream processing techniques. Applying the fog computing paradigm, parts of it are deployed in an elastic, central cloud while other parts run directly, decentralized in the production environment. A prototype implementation of this architecture presents solutions how different kinds of sensors can be integrated and their measurements can be continuously aggregated. In order to make analyzed data comprehensible, it features a single-page web application that provides different forms of data visualization. We deploy this pilot implementation in the data center of a medium-sized enterprise, where we successfully monitor the power consumption of 16 servers. Furthermore, we show the scalability of our architecture with 20,000 simulated sensors.",
        "related_work": "Shrouf and Miragliotta @cite_2 report on different approaches for energy management enabled by Internet of Things (IoT) technologies. Based on literature, expert interviews, and reports of manufactures, they summarize different IoT architectures for power monitoring and present a general abstraction of them. The resulting architecture primarily focuses on network interconnections and integration of other systems. As in our approach, it respects real-time data processing and the challenge of integrating data of different sensors and data formats. However, data is only processed in a cloud or local server infrastructure and does not follow fog computing paradigms. The architecture represents a general approach and is therefore too abstract to offer a reference implementation.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "1968605868",
                "abstract": "Abstract In today's manufacturing scenario, rising energy prices, increasing ecological awareness, and changing consumer behaviors are driving decision-makers to prioritize green manufacturing. The Internet of Things paradigm promises to increase the visibility and awareness of energy consumption, thanks to smart sensors and smart meters at the machine and production line level. Consequently, real-time energy consumption data from manufacturing processes can be collected easily, and then analyzed, to improve energy-aware decision-making. Relying on a comprehensive literature review and on experts' insight, this paper contributes to the understanding of energy-efficient production management practices that are enhanced and enabled by the Internet of Things technology. In addition, it discusses the benefits that can be obtained thanks to adopting such management practices. Eventually, a framework is presented to support the integration of gathered energy data into a company's information technology tools and platforms. This is done with the ultimate goal of highlighting how operational and tactical decision-making processes could leverage on such data in order to improve energy efficiency, and therefore competitiveness, of manufacturing companies. With the outcomes of this paper, energy managers can approach the Internet of Things adoption in a benefit-driven manner, addressing those energy management practices that are more aligned with company maturity, measurable data and available information systems and tools.",
                "doi": "https://doi.org/10.1016/j.jclepro.2015.03.055",
                "title": "Energy management based on Internet of Things: practices and framework for adoption in production management",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1907.00462",
        "mid": "2946521116",
        "abstract": "We take interest in the early assessment of risk for depression in social media users. We focus on the eRisk 2018 dataset, which represents users as a sequence of their written online contributions. We implement four RNN-based systems to classify the users. We explore several aggregations methods to combine predictions on individual posts. Our best model reads through all writings of a user in parallel but uses an attention mechanism to prioritize the most important ones at each timestep.",
        "related_work": "@cite_1 used a more classical approach to classify Twitter users as being at risk of depression or not. They first manually crafted features that describe users' online behavior and characterize their speech. The measures were computed daily, so a user is represented as the time series of the features. Then, the training and predictions were done by a svm with PCA for dimensionality reduction.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2402700",
                "abstract": "Major depression constitutes a serious challenge in personal and public health. Tens of millions of people each year suffer from depression and only a fraction receives adequate treatment. We explore the potential to use social media to detect and diagnose major depressive disorder in individuals. We first employ crowdsourcing to compile a set of Twitter users who report being diagnosed with clinical depression, based on a standard psychometric instrument. Through their social media postings over a year preceding the onset of depression, we measure behavioral attributes relating to social engagement, emotion, language and linguistic styles, ego network, and mentions of antidepressant medications. We leverage these behavioral cues, to build a statistical classifier that provides estimates of the risk of depression, before the reported onset. We find that social media contains useful signals for characterizing the onset of depression in individuals, as measured through decrease in social activity, raised negative affect, highly clustered egonetworks, heightened relational and medicinal concerns, and greater expression of religious involvement. We believe our findings and methods may be useful in developing tools for identifying the onset of major depression, for use by healthcare agencies; or on behalf of individuals, enabling those suffering from depression to be more proactive about their mental health.",
                "doi": "https://doi.org/10.1609/icwsm.v7i1.14432",
                "title": "Predicting Depression via Social Media",
                "publication_year": 2021
            }
        }
    },
    {
        "aid": "1812.00804",
        "mid": "2950959563",
        "abstract": "Given a set of observations generated by an optimization process, the goal of inverse optimization is to determine likely parameters of that process. We cast inverse optimization as a form of deep learning. Our method, called deep inverse optimization, is to unroll an iterative optimization process and then use backpropagation to learn parameters that generate the observations. We demonstrate that by backpropagating through the interior point algorithm we can learn the coefficients determining the cost vector and the constraints, independently or jointly, for both non-parametric and parametric linear programs, starting from one or multiple observations. With this approach, inverse optimization can leverage concepts and algorithms from deep learning.",
        "related_work": "In the parametric optimization setting, @cite_16 develop an optimization model that encodes KKT optimality conditions for imputing objective function coefficients of a convex optimization problem. @cite_28 focus on the same problem under the assumption of noisy measurements, developing a bilevel problem and two algorithms which are shown to maintain statistical consistency. Saez-Gallego and Morales @cite_20 address the case of learning @math and @math jointly in a parametric setting where the @math vector is assumed to be an affine function of a regressor. The general case of learning the weights of a parametric linear optimization problem where @math , @math and @math are functions of @math (Figure (iii)) has not been addressed in the literature.",
        "ref_abstract": {
            "@cite_28": {
                "mid": "2963474773",
                "abstract": "Inverse optimization refers to the inference of unknown parameters of an optimization problem based on knowledge of its optimal solutions. This paper considers inverse optimization in the setting where measurements of the optimal solutions of a convex optimization problem are corrupted by noise. We first provide a formulation for inverse optimization and prove it to be NP-hard. In contrast to existing methods, we show that the parameter estimates produced by our formulation are statistically consistent. Our approach involves combining a new duality-based reformulation for bilevel programs with a regularization scheme that smooths discontinuities in the formulation. Using epi-convergence theory, we show the regularization parameter can be adjusted to approximate the original inverse optimization problem to arbitrary accuracy, which we use to prove our consistency results. Next, we propose two solution algorithms based on our duality-based formulation. The first is an enumeration algorithm that is applicabl...",
                "doi": "https://doi.org/10.1287/opre.2017.1705",
                "title": "Inverse Optimization with Noisy Data",
                "publication_year": 2018
            },
            "@cite_16": {
                "mid": "2107308405",
                "abstract": "We consider an optimizing process (or parametric optimization problem), i.e., an optimization problem that depends on some parameters. We present a method for imputing or estimating the objective function, based on observations of optimal or nearly optimal choices of the variable for several values of the parameter, and prior knowledge (or assumptions) about the objective. Applications include estimation of consumer utility functions from purchasing choices, estimation of value functions in control problems, given observations of an optimal (or just good) controller, and estimation of cost functions in a flow network.",
                "doi": "https://doi.org/10.1109/isic.2011.6045410",
                "title": "Imputing a convex objective function",
                "publication_year": 2011
            },
            "@cite_20": {
                "mid": "2964052424",
                "abstract": "We consider the problem of forecasting the aggregate demand of a pool of price-responsive consumers of electricity. The response of the aggregate load to price is modeled by an optimization problem that is characterized by a set of marginal utility curves and minimum and maximum power consumption limits. The task of estimating these parameters is addressed using a generalized inverse optimization scheme that, in turn, requires solving a nonconvex mathematical program. We introduce a solution method that overcomes the nonconvexities by solving instead two linear problems with a penalty term, which is statistically adjusted by using a cross-validation algorithm. The proposed methodology is data-driven and leverages information from regressors, such as time and weather variables, to account for changes in the parameter estimates. The power load of a group of heating, ventilation, and air conditioning systems in buildings is simulated, and the results show that the aggregate demand of the group can be successfully captured by the proposed model, making it suitable for short-term forecasting purposes.",
                "doi": "https://doi.org/10.1109/tsg.2017.2671743",
                "title": "Short-Term Forecasting of Price-Responsive Loads Using Inverse Optimization",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1811.12108",
        "mid": "2913534916",
        "abstract": "Complex image processing and computer vision systems often consist of a processing pipeline of functional modules. We intend to replace parts or all of a target pipeline with deep neural networks to achieve benefits such as increased accuracy or reduced computational requirement. To acquire a large amount of labeled data necessary to train the deep neural network, we propose a workflow that leverages the target pipeline to create a significantly larger labeled training set automatically, without prior domain knowledge of the target pipeline. We show experimentally that despite the noise introduced by automated labeling and only using a very small initially labeled data set, the trained deep neural networks can achieve similar or even better performance than the components they replace, while in some cases also reducing computational requirements.",
        "related_work": "Our work can be considered an approach to @cite_16 @cite_8 , in which a target function is approximated by a surrogate that is cheaper to compute but introduces inaccuracy. In computer vision and image processing, some level of inaccuracy is often tolerable, due to the limits of human perception and the lack of a clearly delineated correct'' answer @cite_20 . Approximation can be introduced at the hardware level, such as by using approximate adder circuits (e.g. @cite_17 ), or at the software level by restructuring the algorithm.",
        "ref_abstract": {
            "@cite_16": {
                "mid": "2020217519",
                "abstract": "Approximate computing has recently emerged as a promising approach to energy-efficient design of digital systems. Approximate computing relies on the ability of many systems and applications to tolerate some loss of quality or optimality in the computed result. By relaxing the need for fully precise or completely deterministic operations, approximate computing techniques allow substantially improved energy efficiency. This paper reviews recent progress in the area, including design of approximate arithmetic blocks, pertinent error and quality measures, and algorithm-level techniques for approximate computing.",
                "doi": "https://doi.org/10.1109/ets.2013.6569370",
                "title": "Approximate computing: An emerging paradigm for energy-efficient design",
                "publication_year": 2013
            },
            "@cite_17": {
                "mid": "1998824039",
                "abstract": "Addition is a fundamental function in arithmetic operation; several adder designs have been proposed for implementations in inexact computing. These adders show different operational profiles; some of them are approximate in nature while others rely on probabilistic features of nanoscale circuits. However, there has been a lack of appropriate metrics to evaluate the efficacy of various inexact designs. In this paper, new metrics are proposed for evaluating the reliability as well as the power efficiency of approximate and probabilistic adders. Reliability is analyzed using the so-called sequential probability transition matrices (SPTMs). Error distance (ED) is initially defined as the arithmetic distance between an erroneous output and the correct output for a given input. The mean error distance (MED) and normalized error distance (NED) are then proposed as unified figures that consider the averaging effect of multiple inputs and the normalization of multiple-bit adders. It is shown that the MED is an effective metric for measuring the implementation accuracy of a multiple-bit adder and that the NED is a nearly invariant metric independent of the size of an adder. The MED is, therefore, useful in assessing the effectiveness of an approximate or probabilistic adder implementation, while the NED is useful in characterizing the reliability of a specific design. Since inexact adders are often used for saving power, the product of power and NED is further utilized for evaluating the tradeoffs between power consumption and precision. Although illustrated using adders, the proposed metrics are potentially useful in assessing other arithmetic circuit designs for applications of inexact computing.",
                "doi": "https://doi.org/10.1109/tc.2012.146",
                "title": "New Metrics for the Reliability of Approximate and Probabilistic Adders",
                "publication_year": 2013
            },
            "@cite_20": {
                "mid": "1991735330",
                "abstract": "Approximate computing, which refers to a class of techniques that relax the requirement of exact equivalence between the specification and implementation of a computing system, has attracted significant interest in recent years. We propose a systematic methodology, called MACACO, for the M odeling and A nalysis of C ircuits for A pproximate C omputing. The proposed methodology can be utilized to analyze how an approximate circuit behaves with reference to a conventional correct implementation, by computing metrics such as worst-case error, average-case error, error probability, and error distribution. The methodology applies to both timing-induced approximations such as voltage over-scaling or over-clocking, and functional approximations based on logic complexity reduction. The first step in MACACO is the construction of an equivalent untimed circuit that represents the behavior of the approximate circuit at a given voltage and clock period. Next, we construct a virtual error circuit that represents the error in the approximate circuit's output for any given input or input sequence. Finally, we apply conventional Boolean analysis techniques (SAT solvers, BDDs) and statistical techniques (Monte-Carlo simulation) in order to compute the various metrics of interest. We have applied the proposed methodology to analyze a range of approximate designs for datapath building blocks. Our results show that MACACO can help a designer to systematically evaluate the impact of approximate circuits, and to choose between different approximate implementations, thereby facilitating the adoption of such circuits for approximate computing.",
                "doi": "https://doi.org/10.5555/2132325.2132474",
                "title": "MACACO: modeling and analysis of circuits for approximate computing",
                "publication_year": 2011
            },
            "@cite_8": {
                "mid": "2265166184",
                "abstract": "Approximate computing trades off computation quality with effort expended, and as rising performance demands confront plateauing resource budgets, approximate computing has become not merely attractive, but even imperative. In this article, we present a survey of techniques for approximate computing (AC). We discuss strategies for finding approximable program portions and monitoring output quality, techniques for using AC in different processing units (e.g., CPU, GPU, and FPGA), processor components, memory technologies, and so forth, as well as programming frameworks for AC. We classify these techniques based on several key characteristics to emphasize their similarities and differences. The aim of this article is to provide insights to researchers into working of AC techniques and inspire more efforts in this area to make AC the mainstream computing approach in future systems.",
                "doi": "https://doi.org/10.1145/2893356",
                "title": "A Survey of Techniques for Approximate Computing",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1811.10003",
        "mid": "2896049206",
        "abstract": "Abstract Automatic reading texts in scenes has attracted increasing interest in recent years as texts often carry rich semantic information that is useful for scene understanding. In this paper, we propose a novel scene text proposal technique aiming for accurate reading texts in scenes. Inspired by the pooling layer in the deep neural network architecture, a pooling based scene text proposal technique is developed. A novel score function is designed which exploits the histogram of oriented gradients and is capable of ranking the proposals according to their probabilities of being text. An end-to-end scene text reading system has also been developed by incorporating the proposed scene text proposal technique where false alarms elimination and words recognition are performed simultaneously. Extensive experiments over several public datasets show that the proposed technique can handle multi-orientation and multi-language scene texts and obtains outstanding proposal performance. The developed end-to-end systems also achieve very competitive scene text spotting and reading performance.",
        "related_work": "The scene text proposal idea is mainly inspired by the success of object proposal in many object detection systems. It has advantage in locating more possible text regions to offer higher detection recall. It's often evaluated according to the recall rate as well as the number of needed proposals - typically the smaller the better at a similar recall level @cite_62 . False-positive scene text proposals are usually eliminated by either a text nontext classifier @cite_39 @cite_3 or a scene text recognition model @cite_34 @cite_11 in end-to-end scene text reading systems.",
        "ref_abstract": {
            "@cite_62": {
                "mid": "2104446196",
                "abstract": "Current top performing Pascal VOC object detectors employ detection proposals to guide the search for objects thereby avoiding exhaustive sliding window search across images. Despite the popularity of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in depth analysis of ten object proposal methods along with four baselines regarding ground truth annotation recall (on Pascal VOC 2007 and ImageNet 2013), repeatability, and impact on DPM detector performance. Our findings show common weaknesses of existing methods, and provide insights to choose the most adequate method for different settings.",
                "doi": "https://doi.org/10.5244/c.28.24",
                "title": "How good are detection proposals, really?",
                "publication_year": 2014
            },
            "@cite_3": {
                "mid": "2704256938",
                "abstract": "In this paper, we develop a new approach called DeepText for text region proposal generation and text detection in natural images via a fully convolutional neural network (CNN). First, we propose the novel inception region proposal network (Inception-RPN), which slides an inception network with multi-scale windows over the top of convolutional feature maps and associates a set of text characteristic prior bounding boxes with each sliding position to generate high recall word region proposals. Next, we present a powerful text detection network that embeds ambiguous text category (ATC) information and multi-level region-of-interest pooling (MLRP) for text and non-text classification and accurate localization refinement. Our approach achieves an F-measure of 0.83 and 0.85 on the ICDAR 2011 and 2013 robust text detection benchmarks, outperforming previous state-of-the-art results.",
                "doi": "https://doi.org/10.1109/icassp.2017.7952348",
                "title": "DeepText: A new approach for text proposal generation and text detection in natural images",
                "publication_year": 2017
            },
            "@cite_39": {
                "mid": "1935817682",
                "abstract": "Recently, a variety of real-world applications have triggered huge demand for techniques that can extract textual information from natural scenes. Therefore, scene text detection and recognition have become active research topics in computer vision. In this work, we investigate the problem of scene text detection from an alternative perspective and propose a novel algorithm for it. Different from traditional methods, which mainly make use of the properties of single characters or strokes, the proposed algorithm exploits the symmetry property of character groups and allows for direct extraction of text lines from natural images. The experiments on the latest ICDAR benchmarks demonstrate that the proposed algorithm achieves state-of-the-art performance. Moreover, compared to conventional approaches, the proposed algorithm shows stronger adaptability to texts in challenging scenarios.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298871",
                "title": "Symmetry-based text line detection in natural scenes",
                "publication_year": 2015
            },
            "@cite_34": {
                "mid": "2962984063",
                "abstract": "Abstract Motivated by the success of powerful while expensive techniques to recognize words in a holistic way (, 2013; , 2014; , 2016) object proposals techniques emerge as an alternative to the traditional text detectors. In this paper we introduce a novel object proposals method that is specifically designed for text. We rely on a similarity based region grouping algorithm that generates a hierarchy of word hypotheses. Over the nodes of this hierarchy it is possible to apply a holistic word recognition method in an efficient way. Our experiments demonstrate that the presented method is superior in its ability of producing good quality word proposals when compared with class-independent algorithms. We show impressive recall rates with a few thousand proposals in different standard benchmarks, including focused or incidental text datasets, and multi-language scenarios. Moreover, the combination of our object proposals with existing whole-word recognizers (, 2014; , 2016) shows competitive performance in end-to-end word spotting, and, in some benchmarks, outperforms previously published results. Concretely, in the challenging ICDAR2015 Incidental Text dataset, we overcome in more than 10 F -score the best-performing method in the last ICDAR Robust Reading Competition (Karatzas, 2015). Source code of the complete end-to-end system is available at https: github.com lluisgomez TextProposals .",
                "doi": "https://doi.org/10.1016/j.patcog.2017.04.027",
                "title": "TextProposals: A text-specific selective search algorithm for word spotting in the wild",
                "publication_year": 2017
            },
            "@cite_11": {
                "mid": "2607175958",
                "abstract": "Text proposal has been gaining interest in recent years due to the great success of object proposal in categoriesindependent object localization. In this paper, we present a novel text-specific proposal technique that provides superior bounding boxes for accurate text localization in scenes. The proposed technique, which we call Text Edge Box (TEB), uses a binary edge map, a gradient map and an orientation map of an image as inputs. Connected components are first found within the binary edge map, which are scored by two proposed low-cue text features that are extracted in the gradient map and the orientation map, respectively. These scores present text probability of connected components and are aggregated in a text edge image. Scene texts proposals are finally generated by grouping the connected components and estimating their likelihood of being words. The proposed TEB has been evaluated on the two public scene text datasets: the Robust Reading Competition 2013 dataset (ICDAR 2013) dataset and the Street View Text (SVT) dataset. Experiments show that the proposed TEB outperforms the state-of-the-art techniques greatly.",
                "doi": "https://doi.org/10.1109/wacv.2017.149",
                "title": "Text-Edge-Box: An Object Proposal Approach for Scene Texts Localization",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1811.08051",
        "mid": "2901678097",
        "abstract": "Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss ( @math ), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding @math to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.",
        "related_work": "In object classification, Incremental learning (IL) is the process of increasing the breadth of an object classifier, by training it to recognize new classes, while retaining its knowledge of the classes on which it has been trained originally. In the past couple of years, there has been considerable research efforts in this field @cite_0 @cite_3 . Moreover, there exist several subsets of this research problem which impose different constraints in terms of data storage and evaluation. We can divide existing methods based on their constraints:",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2560647685",
                "abstract": "Abstract The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.",
                "doi": "https://doi.org/10.1073/pnas.1611835114",
                "title": "Overcoming catastrophic forgetting in neural networks",
                "publication_year": 2017
            },
            "@cite_3": {
                "mid": "2473930607",
                "abstract": "When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.",
                "doi": "https://doi.org/10.1109/tpami.2017.2773081",
                "title": "Learning without Forgetting",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1906.07900",
        "mid": "2949728259",
        "abstract": "Comprehensive quality-aware automated semantic web service composition is an NP-hard problem, where service composition workflows are unknown, and comprehensive quality, i.e., Quality of services (QoS) and Quality of semantic matchmaking (QoSM) are simultaneously optimized. The objective of this problem is to find a solution with optimized or near-optimized overall QoS and QoSM within polynomial time over a service request. In this paper, we proposed novel memetic EDA-based approaches to tackle this problem. The proposed method investigates the effectiveness of several neighborhood structures of composite services by proposing domain-dependent local search operators. Apart from that, a joint strategy of the local search procedure is proposed to integrate with a modified EDA to reduce the overall computation time of our memetic approach. To better demonstrate the effectiveness and scalability of our approach, we create a more challenging, augmented version of the service composition benchmark based on WSC-08 bansal2008wsc and WSC-09 kona2009wsc . Experimental results on this benchmark show that one of our proposed memetic EDA-based approach (i.e., MEEDA-LOP) significantly outperforms existing state-of-the-art algorithms.",
        "related_work": "Automated web service composition aims to loosely couple web services to fulfill a service request, without strictly obeying a pre-given abstract workflow. Instead, composition workflows are gradually built up while its component services are selected. Existing works in fully automated web service composition can be categorized into two approaches --- direct approaches and indirect approaches @cite_37 . The direct approaches represent composition solutions explicitly in the representation that displays actual execution flows of composite services, while the indirect approaches often represent composite services implicitly as permutations, which require a decoding process to build up actual execution workflows.",
        "ref_abstract": {
            "@cite_37": {
                "mid": "2605603844",
                "abstract": "Web services have become increasingly popular in recent years, and they are especially suitable to the process of Web service composition, which is when several services are combined to create an application that accomplishes a more complex task. In recent years, significant research efforts have been made on developing approaches for performing Quality of Service -aware Web service composition. Evolutionary computing (EC) techniques have been widely used for solving this problem, since they allow for the quality of compositions to be optimised, meanwhile also ensuring that the solutions produced have the required functionality. Existing EC-based composition approaches perform constrained optimisation to produce solutions that meet those requirements, however these constraints may hinder the effectiveness of the search. To address this issue, a novel framework based on an indirect representation is proposed in this work. The core idea is to first generate candidate service compositions encoded as sequences of services. Then, a decoding scheme is developed to transform any sequence of services into a corresponding feasible service composition. Given a service sequence, the decoding scheme builds the workflow from scratch by iteratively adding the services to proper positions of the workflow in the order of the sequence. This is beneficial because it allows the optimisation to be carried out in an unconstrained way, later enforcing functionality constraints during the decoding process. A number of encoding methods and corresponding search operators, including the PSO, GA, and GP-based methods, are proposed and tested, with results showing that the quality of the solutions produced by the proposed indirect approach is higher than that of a baseline direct representation-based approach for twelve out of the thirteen datasets considered. In particular, the method using the variable-length sequence representation has the most efficient execution time, while the fixed-length sequence produces the highest quality solutions.",
                "doi": "https://doi.org/10.1007/s10732-017-9330-4",
                "title": "Evolutionary computation for automatic Web service composition: an indirect representation approach",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1906.07258",
        "mid": "2949855468",
        "abstract": "Precise knowledge about the size of a crowd, its density and flow can provide valuable information for safety and security applications, event planning, architectural design and to analyze consumer behavior. Creating a powerful machine learning model, to employ for such applications requires a large and highly accurate and reliable dataset. Unfortunately the existing crowd counting and density estimation benchmark datasets are not only limited in terms of their size, but also lack annotation, in general too time consuming to implement. This paper attempts to address this very issue through a content aware technique, uses combinations of Chan-Vese segmentation algorithm, two-dimensional Gaussian filter and brute-force nearest neighbor search. The results shows that by simply replacing the commonly used density map generators with the proposed method, higher level of accuracy can be achieved using the existing state of the art models.",
        "related_work": "Liu @cite_17 proposed a universal network for counting people in a crowd with varying density and scale. in this study the proposed network is composed of two components: a detection network (DNet) and an encoder-decoder estimation network (ENet). The input first run through DNet to detect and count individuals who can be segmented clearly. Then, ENet is utilized to estimate the density maps of the remaining areas, where the numbers of individuals cannot be detected. Modified version of Xception used as an encoder for feature extraction and a combination of dilated convolution and transposed convolution used as decoder. Authors attempted to address the variations in crowd density with two literally isolated deep networks which significantly slows down the process lacks novelty.",
        "ref_abstract": {
            "@cite_17": {
                "mid": "2939776820",
                "abstract": "Counting people or objects with significantly varying scales and densities has attracted much interest from the research community and yet it remains an open problem. In this paper, we propose a simple but an efficient and effective network, named DENet, which is composed of two components, i.e., a detection network (DNet) and an encoder-decoder estimation network (ENet). We first run DNet on an input image to detect and count individuals who can be segmented clearly. Then, ENet is utilized to estimate the density maps of the remaining areas, where the numbers of individuals cannot be detected. We propose a modified Xception as an encoder for feature extraction and a combination of dilated convolution and transposed convolution as a decoder. In the ShanghaiTech Part A, UCF and WorldExpo'10 datasets, our DENet achieves lower Mean Absolute Error (MAE) than those of the state-of-the-art methods.",
                "doi": "https://doi.org/10.48550/arxiv.1904.08056",
                "title": "DENet: A Universal Network for Counting Crowd with Varying Densities and\n  Scales",
                "publication_year": 2019
            }
        }
    },
    {
        "aid": "1906.07138",
        "mid": "2900464026",
        "abstract": "Mapping road networks today is labor-intensive. As a result, road maps have poor coverage outside urban centers in many countries. Systems to automatically infer road network graphs from aerial imagery and GPS trajectories have been proposed to improve coverage of road maps. However, because of high error rates, these systems have not been adopted by mapping communities. We propose machine-assisted map editing, where automatic map inference is integrated into existing, human-centric map editing workflows. To realize this, we build Machine-Assisted iD (MAiD), where we extend the web-based OpenStreetMap editor, iD, with machine-assistance functionality. We complement MAiD with a novel approach for inferring road topology from aerial imagery that combines the speed of prior segmentation approaches with the accuracy of prior iterative graph construction methods. We design MAiD to tackle the addition of major, arterial roads in regions where existing maps have poor coverage, and the incremental improvement of coverage in regions where major roads are already mapped. We conduct two user studies and find that, when participants are given a fixed time to map roads, they are able to add as much as 3.5x more roads with MAiD.",
        "related_work": "Most state-of-the-art approaches for inferring road maps from aerial imagery apply convolutional neural networks (CNNs) to segment the imagery for road'' and non-road'' pixels, and then post-process the segmentation output to extract a road network graph. develop a cascaded CNN architecture with two jointly trained components, where the first component detects pixels on the road, and the second focuses on pixels close to the road centerline @cite_17 . They then threshold and thin the centerline segmentation output to extract a graph.",
        "ref_abstract": {
            "@cite_17": {
                "mid": "2593886839",
                "abstract": "Accurate road detection and centerline extraction from very high resolution (VHR) remote sensing imagery are of central importance in a wide range of applications. Due to the complex backgrounds and occlusions of trees and cars, most road detection methods bring in the heterogeneous segments; besides for the centerline extraction task, most current approaches fail to extract a wonderful centerline network that appears smooth, complete, as well as single-pixel width. To address the above-mentioned complex issues, we propose a novel deep model, i.e., a cascaded end-to-end convolutional neural network (CasNet), to simultaneously cope with the road detection and centerline extraction tasks. Specifically, CasNet consists of two networks. One aims at the road detection task, whose strong representation ability is well able to tackle the complex backgrounds and occlusions of trees and cars. The other is cascaded to the former one, making full use of the feature maps produced formerly, to obtain the good centerline extraction. Finally, a thinning algorithm is proposed to obtain smooth, complete, and single-pixel width road centerline network. Extensive experiments demonstrate that CasNet outperforms the state-of-the-art methods greatly in learning quality and learning speed. That is, CasNet exceeds the comparing methods by a large margin in quantitative performance, and it is nearly 25 times faster than the comparing methods. Moreover, as another contribution, a large and challenging road centerline data set for the VHR remote sensing image will be publicly available for further studies.",
                "doi": "https://doi.org/10.1109/tgrs.2017.2669341",
                "title": "Automatic Road Detection and Centerline Extraction via Cascaded End-to-End Convolutional Neural Network",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1810.06325",
        "mid": "2897282582",
        "abstract": "Artificial sound event detection (SED) aims to mimic the human ability to perceive and understand what is happening in the surroundings. Nowadays, deep learning offers valuable techniques for this goal, such as convolutional neural networks (CNNs). The capsule neural network (CapsNet) architecture has been recently introduced in the image processing field with the intent to overcome some of the known limitations of CNNs, specifically regarding the scarce robustness to affine transformations (i.e., perspective, size, and orientation) and the detection of overlapped images. This motivated the authors to employ CapsNets to deal with the polyphonic SED task, in which multiple sound events occur simultaneously. Specifically, we propose to exploit the capsule units to represent a set of distinctive properties for each individual sound event. Capsule units are connected through a so-called dynamic routing that encourages learning part-whole relationships and improves the detection performance in a polyphonic context. This paper reports extensive evaluations carried out on three publicly available datasets, showing how the CapsNet-based algorithm not only outperforms standard CNNs but also achieves the best results with respect to the state-of-the-art algorithms.",
        "related_work": "The use of deep learning models has been motivated by the increased availability of datasets and computational resources and resulted in significant performance improvements. The methods based on CNNs and RNNs have established the new state-of-the-art performance on the SED task, thanks to the capabilities to learn the non-linear relationship between time-frequency features of the audio signal and a target vector representing sound events. In @cite_3 , the authors show how local'' patterns can be learned by a CNN and can be exploited to improve the performance of detection and classification of non-speech acoustic events occurring in conversation scenes, in particular compared to a FNN-based system which processes multiple resolution spectrograms in parallel.",
        "ref_abstract": {
            "@cite_3": {
                "mid": "1846473900",
                "abstract": "In recent years, deep learning has not only permeated the computer vision and speech recognition research fields but also fields such as acoustic event detection (AED). One of the aims of AED is to detect and classify non-speech acoustic events occurring in conversation scenes including those produced by both humans and the objects that surround us. In AED, deep learning has enabled modeling of detail-rich features, and among these, high resolution spectrograms have shown a significant advantage over existing predefined features (e.g., Mel-filter bank) that compress and reduce detail. In this paper, we further asses the importance of feature extraction for deep learning-based acoustic event detection. AED, based on spectrogram-input deep neural networks, exploits the fact that sounds have \u201cglobal\u201d spectral patterns, but sounds also have \u201clocal\u201d properties such as being more transient or smoother in the time-frequency domain. These can be exposed by adjusting the time-frequency resolution used to compute the spectrogram, or by using a model that exploits locality leading us to explore two different feature extraction strategies in the context of deep learning: (1) using multiple resolution spectrograms simultaneously and analyzing the overall and event-wise influence to combine the results, and (2) introducing the use of convolutional neural networks (CNN), a state of the art 2D feature extraction model that exploits local structures, with log power spectrogram input for AED. An experimental evaluation shows that the approaches we describe outperform our state-of-the-art deep learning baseline with a noticeable gain in the CNN case and provides insights regarding CNN-based spectrogram characterization for AED.",
                "doi": "https://doi.org/10.1186/s13636-015-0069-2",
                "title": "Exploiting spectro-temporal locality in deep learning based acoustic event detection",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1810.03979",
        "mid": "2966206672",
        "abstract": "After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I O bandwidth and the energy consumption is dominated by I O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4\u00d7 relative to uncompressed data and a gain of 60 over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.",
        "related_work": "There are several methods out there describing hardware accelerators which exploit feature map sparsity to reduce computation: Cnvlutin @cite_3 , SCNN @cite_29 , Cambricon-X @cite_28 , NullHop @cite_13 , Eyeriss @cite_8 , EIE @cite_26 . Their focus is on power gating or skipping some of the operations and memory accesses. While this automatically entails defining a scheme to feed the data into the system, minimizing the bandwidth was not the primary objective of any of them. They all use one of three methods: Zero-RLE (used in SCNN): A simple run-length encoding for the zero values, i.e. a single prefix bit followed by the number of zero-values or the non-zero value. Zero-free neuron array format (ZFNAf) (used in Cnvlutin): Similarly to the widely-used compressed sparse row (CSR) format, non-zero elements are encoded with an offset and their value. Compressed column storage (CCS) format (e.g. used in EIE): Similar to ZFNAf, but the offsets are stored in relative form, thus requiring less bits to store them. Few bits are sufficient, and in case they are all exhausted, a zero-value can be encoded as if it was non-zero.",
        "ref_abstract": {
            "@cite_26": {
                "mid": "2285660444",
                "abstract": "State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120\u00d7 energy saving; Exploiting sparsity saves 10\u00d7; Weight sharing gives 8\u00d7; Skipping zero activations from ReLU saves another 3\u00d7. Evaluated on nine DNN benchmarks, EIE is 189\u00d7 and 13\u00d7 faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88\u00d7104 frames sec with a power dissipation of only 600mW. It is 24,000\u00d7 and 3,400\u00d7 more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9\u00d7, 19\u00d7 and 3\u00d7 better throughput, energy efficiency and area efficiency.",
                "doi": "https://doi.org/10.1145/3007787.3001163",
                "title": "EIE",
                "publication_year": 2016
            },
            "@cite_8": {
                "mid": "2442974303",
                "abstract": "Deep convolutional neural networks (CNNs) are widely used in modern AI systems for their superior accuracy but at the cost of high computational complexity. The complexity comes from the need to simultaneously process hundreds of filters and channels in the high-dimensional convolutions, which involve a significant amount of data movement. Although highly-parallel compute paradigms, such as SIMD SIMT, effectively address the computation requirement to achieve high throughput, energy consumption still remains high as data movement can be more expensive than computation. Accordingly, finding a dataflow that supports parallel processing with minimal data movement cost is crucial to achieving energy-efficient CNN processing without compromising accuracy. In this paper, we present a novel dataflow, called row-stationary (RS), that minimizes data movement energy consumption on a spatial architecture. This is realized by exploiting local data reuse of filter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations. Unlike dataflows used in existing designs, which only reduce certain types of data movement, the proposed RS dataflow can adapt to different CNN shape configurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism. To evaluate the energy efficiency of the different dataflows, we propose an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints. Experiments using the CNN configurations of AlexNet show that the proposed RS dataflow is more energy efficient than existing dataflows in both convolutional (1.4\u00d7 to 2.5\u00d7) and fully-connected layers (at least 1.3\u00d7 for batch size larger than 16). The RS dataflow has also been demonstrated on a fabricated chip, which verifies our energy analysis.",
                "doi": "https://doi.org/10.1145/3007787.3001177",
                "title": "Eyeriss",
                "publication_year": 2016
            },
            "@cite_28": {
                "mid": "2565851976",
                "abstract": "Neural networks (NNs) have been demonstrated to be useful in a broad range of applications such as image recognition, automatic translation and advertisement recommendation. State-of-the-art NNs are known to be both computationally and memory intensive, due to the ever-increasing deep structure, i.e., multiple layers with massive neurons and connections (i.e., synapses). Sparse neural networks have emerged as an effective solution to reduce the amount of computation and memory required. Though existing NN accelerators are able to efficiently process dense and regular networks, they cannot benefit from the reduction of synaptic weights. In this paper, we propose a novel accelerator, Cambricon-X, to exploit the sparsity and irregularity of NN models for increased efficiency. The proposed accelerator features a PE-based architecture consisting of multiple Processing Elements (PE). An Indexing Module (IM) efficiently selects and transfers needed neurons to connected PEs with reduced bandwidth requirement, while each PE stores irregular and compressed synapses for local computation in an asynchronous fashion. With 16 PEs, our accelerator is able to achieve at most 544 GOP s in a small form factor (6.38 mm2 and 954 mW at 65 nm). Experimental results over a number of representative sparse networks show that our accelerator achieves, on average, 7.23x speedup and 6.43x energy saving against the state-of-the-art NN accelerator.",
                "doi": "https://doi.org/10.5555/3195638.3195662",
                "title": "Cambricon-x: an accelerator for sparse neural networks",
                "publication_year": 2016
            },
            "@cite_29": {
                "mid": "2625457103",
                "abstract": "Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to a multiplier array, where they are extensively reused; product accumulation is performed in a novel accumulator array. On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7x and 2.3x, respectively, over a comparably provisioned dense CNN accelerator.",
                "doi": "https://doi.org/10.1145/3079856.3080254",
                "title": "SCNN",
                "publication_year": 2017
            },
            "@cite_3": {
                "mid": "2516141709",
                "abstract": "This work observes that a large fraction of the computations performed by Deep Neural Networks (DNNs) are intrinsically ineffectual as they involve a multiplication where one of the inputs is zero. This observation motivates Cnvlutin (CNV), a value-based approach to hardware acceleration that eliminates most of these ineffectual operations, improving performance and energy over a state-of-the-art accelerator with no accuracy loss. CNV uses hierarchical data-parallel units, allowing groups of lanes to proceed mostly independently enabling them to skip over the ineffectual computations. A co-designed data storage format encodes the computation elimination decisions taking them off the critical path while avoiding control divergence in the data parallel units. Combined, the units and the data storage format result in a data-parallel architecture that maintains wide, aligned accesses to its memory hierarchy and that keeps its data lanes busy. By loosening the ineffectual computation identification criterion, CNV enables further performance and energy efficiency improvements, and more so if a loss in accuracy is acceptable. Experimental measurements over a set of state-of-the-art DNNs for image classification show that CNV improves performance over a state-of-the-art accelerator from 1.24\u00d7 to 1.55\u00d7 and by 1.37\u00d7 on average without any loss in accuracy by removing zero-valued operand multiplications alone. While CNV incurs an area overhead of 4.49 , it improves overall EDP (Energy Delay Product) and ED2P (Energy Delay Squared Product) on average by 1.47\u00d7 and 2.01\u00d7, respectively. The average performance improvements increase to 1.52\u00d7 without any loss in accuracy with a broader ineffectual identification policy. Further improvements are demonstrated with a loss in accuracy.",
                "doi": "https://doi.org/10.1145/3007787.3001138",
                "title": "Cnvlutin",
                "publication_year": 2016
            },
            "@cite_13": {
                "mid": "2623629680",
                "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving many state-of-the-art (SOA) visual processing tasks. Even though graphical processing units are most often used in training and deploying CNNs, their power efficiency is less than 10 GOp s W for single-frame runtime inference. We propose a flexible and efficient CNN accelerator architecture called NullHop that implements SOA CNNs useful for low-power and low-latency application scenarios. NullHop exploits the sparsity of neuron activations in CNNs to accelerate the computation and reduce memory requirements. The flexible architecture allows high utilization of available computing resources across kernel sizes ranging from @math to @math . NullHop can process up to 128 input and 128 output feature maps per layer in a single pass. We implemented the proposed architecture on a Xilinx Zynq field-programmable gate array (FPGA) platform and presented the results showing how our implementation reduces external memory transfers and compute time in five different CNNs ranging from small ones up to the widely known large VGG16 and VGG19 CNNs. Postsynthesis simulations using Mentor Modelsim in a 28-nm process with a clock frequency of 500 MHz show that the VGG19 network achieves over 450 GOp s. By exploiting sparsity, NullHop achieves an efficiency of 368 , maintains over 98 utilization of the multiply\u2013accumulate units, and achieves a power efficiency of over 3 TOp s W in a core area of 6.3 mm2. As further proof of NullHop\u2019s usability, we interfaced its FPGA implementation with a neuromorphic event camera for real-time interactive demonstrations.",
                "doi": "https://doi.org/10.1109/tnnls.2018.2852335",
                "title": "NullHop: A Flexible Convolutional Neural Network Accelerator Based on Sparse Representations of Feature Maps",
                "publication_year": 2019
            }
        }
    },
    {
        "aid": "1809.07058",
        "mid": "2950546634",
        "abstract": "Navigating in search and rescue environments is challenging, since a variety of terrains has to be considered. Hybrid driving-stepping locomotion, as provided by our robot Momaro, is a promising approach. Similar to other locomotion methods, it incorporates many degrees of freedom---offering high flexibility but making planning computationally expensive for larger environments. We propose a navigation planning method, which unifies different levels of representation in a single planner. In the vicinity of the robot, it provides plans with a fine resolution and a high robot state dimensionality. With increasing distance from the robot, plans become coarser and the robot state dimensionality decreases. We compensate this loss of information by enriching coarser representations with additional semantics. Experiments show that the proposed planner provides plans for large, challenging scenarios in feasible time.",
        "related_work": "Planning for systems with high-dimensional motion flexibility quickly reaches its limits for larger environments since the search space grows exponentially. Similar to multiresolution planning, several approaches utilize multiple representations with different planning dimensionalities to decrease planning complexity. @cite_5 generate an initial plan in a low-dimensional search space and replan in the high-dimensional search space by only considering those states that are part of the low-dimensional plan. @cite_4 plan a path in a low-dimensional search space and only switch to high-dimensional planning in those areas where low-dimensional planning cannot find a solution. Similarly, @cite_9 plan in 2D and switch to high-dimensional planning in the robot vicinity and at key points. As described for multiresolution planning, planning with multiple robot configuration dimensionalities might lead to wrong or bad plans, since a low-dimensional robot representation might assess challenging situations wrongly.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2052718016",
                "abstract": "The manufacturing industry today is still focused on the maximization of production. A possible development able to support the global achievement of this goal is the implementation of a new support system for trajectory-planning, specific for industrial robots. This paper describes the trajectory-planning algorithm, able to generate trajectories manageable by human operators, consisting of linear and circular movement primitives. First, the world model and a topology preserving roadmap are stored in a probabilistic occupancy octree by applying a cell extension based algorithm. Successively, the roadmap is constructed within the free reachable joint space maximizing the clearance to the obstacles. A search algorithm is applied on robot configuration positions within the roadmap to identify a path avoiding static obstacles. Finally, the resulting path is converted through an elastic net algorithm into a robot trajectory, which consists of canonical ordered linear and circular movement primitives. The algorithm is demonstrated in a real industrial manipulator context.",
                "doi": "https://doi.org/10.1109/robio.2012.6491187",
                "title": "A cell based voronoi roadmap for motion planning of articulated robots using movement primitives",
                "publication_year": 2012
            },
            "@cite_9": {
                "mid": "2005784077",
                "abstract": "Planning with kinodynamic constraints is often required for mobile robots operating in cluttered, complex environments. A common approach is to use a two-dimensional (2-D) global planner for long range planning, and a short range higher dimensional planner or controller capable of satisfying all of the constraints on motion. However, this approach is incomplete and can result in oscillations and the inability to find a path to the goal. In this paper we present an approach to solving this problem by combining the global and local path planning problem into a single search using a combined 2-D and higher dimensional state-space.",
                "doi": "https://doi.org/10.1109/icra.2012.6225382",
                "title": "Combining global and local planning with guarantees on completeness",
                "publication_year": 2012
            },
            "@cite_4": {
                "mid": "2115970849",
                "abstract": "Path planning quickly becomes computationally hard as the dimensionality of the state-space increases. In this paper, we present a planning algorithm intended to speed up path planning for high-dimensional state-spaces such as robotic arms. The idea behind this work is that while planning in a high-dimensional state-space is often necessary to ensure the feasibilityof the resulting path, large portions of the path have a lower-dimensional structure. Based on this observation, our algorithm iteratively constructs a state-space of an adaptive dimensionality--a state-space that is high-dimensional only where the higher dimensionality is absolutely necessary for finding a feasible path. This often reduces drastically the size of the state-space, and as a result, the planning time and memory requirements. Analytically, we show that our method is complete and is guaranteed to find a solution if one exists, within a specified suboptimality bound. Experimentally, we apply the approach to 3D vehicle navigation (x, y, heading), and to a 7 DOF robotic arm on the Willow Garage\u2019s PR2 robot. The results from our experiments suggest that ourmethod can be substantially faster than some of the state-of-the-art planning algorithms optimized for those tasks.",
                "doi": "https://doi.org/10.1609/socs.v2i1.18204",
                "title": "Path Planning with Adaptive Dimensionality",
                "publication_year": 2021
            }
        }
    },
    {
        "aid": "1809.03200",
        "mid": "2949556825",
        "abstract": "Urban traffic scenarios often require a high degree of cooperation between traffic participants to ensure safety and efficiency. Observing the behavior of others, humans infer whether or not others are cooperating. This work aims to extend the capabilities of automated vehicles, enabling them to cooperate implicitly in heterogeneous environments. Continuous actions allow for arbitrary trajectories and hence are applicable to a much wider class of problems than existing cooperative approaches with discrete action spaces. Based on cooperative modeling of other agents, Monte Carlo Tree Search (MCTS) in conjunction with Decoupled-UCT evaluates the action-values of each agent in a cooperative and decentralized way, respecting the interdependence of actions among traffic participants. The extension to continuous action spaces is addressed by incorporating novel MCTS-specific enhancements for efficient search space exploration. The proposed algorithm is evaluated under different scenarios, showing that the algorithm is able to achieve effective cooperative planning and generate solutions egocentric planning fails to identify.",
        "related_work": "Other approaches are not explicitly cooperative, however they do capture the interdependencies of actions as they evaluate the threat resulting from different maneuver combinations, and hence predict the future motions of vehicles @cite_4 and are able to generate proactive cooperative driving actions @cite_26 .",
        "ref_abstract": {
            "@cite_26": {
                "mid": "2344985987",
                "abstract": "This paper presents a novel cooperative-driving prediction and planning framework for dynamic environments based on the methods of game theory. The proposed algorithm can be used for highly automated driving on highways or as a sophisticated prediction module for advanced driver-assistance systems with no need for intervehicle communication. The main contribution of this paper is a model-based interaction-aware motion prediction of all vehicles in a scene. In contrast to other state-of-the-art approaches, the system also models the replanning capabilities of all drivers. With that, the driving strategy is able to capture complex interactions between vehicles, thus planning maneuver sequences over longer time horizons. It also enables an accurate prediction of traffic for the next immediate time step. The prediction model is supported by an interpretation of what other drivers intend to do, how they interact with traffic, and the ongoing observation. As part of the prediction loop, the proposed planning strategy incorporates the expected reactions of all traffic participants, offering cooperative and robust driving decisions. By means of experimental results under simulated highway scenarios, the validity of the proposed concept and its real-time capability is demonstrated.",
                "doi": "https://doi.org/10.1109/tvt.2015.2508009",
                "title": "A Game-Theoretic Approach to Replanning-Aware Interactive Scene Prediction and Planning",
                "publication_year": 2016
            },
            "@cite_4": {
                "mid": "2134239466",
                "abstract": "In this work, a framework for motion prediction of vehicles and safety assessment of traffic scenes is presented. The developed framework can be used for driver assistant systems as well as for autonomous driving applications. In order to assess the safety of the future trajectories of the vehicle, these systems require a prediction of the future motion of all traffic participants. As the traffic participants have a mutual influence on each other, the interaction of them is explicitly considered in this framework, which is inspired by an optimization problem. Taking the mutual influence of traffic participants into account, this framework differs from the existing approaches which consider the interaction only insufficiently, suffering reliability in real traffic scenes. For motion prediction, the collision probability of a vehicle performing a certain maneuver, is computed. Based on the safety evaluation and the assumption that drivers avoid collisions, the prediction is realized. Simulation scenarios and real-world results show the functionality.",
                "doi": "https://doi.org/10.1109/ivs.2013.6629601",
                "title": "Interactive scene prediction for automotive applications",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1808.09679",
        "mid": "2888844929",
        "abstract": "Traditional survival models such as the Cox proportional hazards model are typically based on scalar or categorical clinical features. With the advent of increasingly large image datasets, it has become feasible to incorporate quantitative image features into survival prediction. So far, this kind of analysis is mostly based on radiomics features, i.e. a fixed set of features that is mathematically defined a priori. To capture highly abstract information, it is desirable to learn the feature extraction using convolutional neural networks. However, for tomographic medical images, model training is difficult because on the one hand, only few samples of 3D image data fit into one batch at once and on the other hand, survival loss functions are essentially ordering measures that require large batch sizes. In this work, we show that by simplifying survival analysis to median survival classification, convolutional neural networks can be trained with small batch sizes and learn features that predict survival equally well as end-to-end hazard prediction networks. Our approach outperforms the previous state of the art in a publicly available lung cancer dataset.",
        "related_work": "We aim to address this issue by transferring features learned by a classification problem to survival analysis without losing performance. Moreover, we propose a method to combine radiomics and learned CNN features that enforce the CNN to learn features that are both discriminative and not covered by the radiomics feature set. All methods are evaluated on a publicly available dataset of computed tomography (CT) images of non-small-cell lung cancer (NSCLC) patients and corresponding survival labels. We show that our method can outperform the previous state-of-the-art presented in @cite_3 .",
        "ref_abstract": {
            "@cite_3": {
                "mid": "2103004421",
                "abstract": "Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost.",
                "doi": "https://doi.org/10.1038/ncomms5006",
                "title": "Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1808.09123",
        "mid": "2888833318",
        "abstract": "When might human input help (or not) when assessing risk in fairness domains? Dressel and Farid (2018) asked Mechanical Turk workers to evaluate a subset of defendants in the ProPublica COMPAS data for risk of recidivism, and concluded that COMPAS predictions were no more accurate or fair than predictions made by humans. We delve deeper into this claim to explore differences in human and algorithmic decision making. We construct a Human Risk Score based on the predictions made by multiple Turk workers, characterize the features that determine agreement and disagreement between COMPAS and Human Scores, and construct hybrid Human+Machine models to predict recidivism. Our key finding is that on this data set, Human and COMPAS decision making differed, but not in ways that could be leveraged to significantly improve ground-truth prediction. We present the results of our analyses and suggestions for data collection best practices to leverage complementary strengths of human and machines in the fairness domain.",
        "related_work": "In addition to the work mentioned in the Introduction, Lakkaraju et. al., ( lakkaraju2017selective ) showed that analyses of recidivism based on human decisions are further complicated by the selective labels'' problem, where observability of outcomes are affected by judges' release decisions. Other work studied how humans perceive different features as fair or not @cite_8 .",
        "ref_abstract": {
            "@cite_8": {
                "mid": "2745133928",
                "abstract": "Evaluating whether machines improve on human performance is one of the central questions of machine learning. However, there are many domains where the data is selectively labeled, in the sense that the observed outcomes are themselves a consequence of the existing choices of the human decision-makers. For instance, in the context of judicial bail decisions, we observe the outcome of whether a defendant fails to return for their court appearance only if the human judge decides to release the defendant on bail. This selective labeling makes it harder to evaluate predictive models as the instances for which outcomes are observed do not represent a random sample of the population. Here we propose a novel framework for evaluating the performance of predictive models on selectively labeled data. We develop an approach called contraction which allows us to compare the performance of predictive models and human decision-makers without resorting to counterfactual inference. Our methodology harnesses the heterogeneity of human decision-makers and facilitates effective evaluation of predictive models even in the presence of unmeasured confounders (unobservables) which influence both human decisions and the resulting outcomes. Experimental results on real world datasets spanning diverse domains such as health care, insurance, and criminal justice demonstrate the utility of our evaluation metric in comparing human decisions and machine predictions.",
                "doi": "https://doi.org/10.1145/3097983.3098066",
                "title": "The Selective Labels Problem",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1808.07191",
        "mid": "2888396831",
        "abstract": "With the development of information technology, there is an explosive growth in the number of online comment concerning news, blogs and so on. The massive comments are overloaded, and often contain some misleading and unwelcome information. Therefore, it is necessary to identify high-quality comments and filter out low-quality comments. In this work, we introduce a novel task: high-quality comment identification (HQCI), which aims to automatically assess the quality of online comments. First, we construct a news comment corpus, which consists of news, comments, and the corresponding quality label. Second, we analyze the dataset, and find the quality of comments can be measured in three aspects: informativeness, consistency, and novelty. Finally, we propose a novel multi-target text matching model, which can measure three aspects by referring to the news and surrounding comments. Experimental results show that our method can outperform various baselines by a large margin on the news dataset.",
        "related_work": "There is previous work regarding rating the academic paper @cite_7 , while this work is about rating the news comments, which is different from them.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "2799174190",
                "abstract": "As more and more academic papers are being submitted to conferences and journals, evaluating all these papers by professionals is time-consuming and can cause inequality due to the personal factors of the reviewers. In this paper, in order to assist professionals in evaluating academic papers, we propose a novel task: automatic academic paper rating (AAPR), which automatically determine whether to accept academic papers. We build a new dataset for this task and propose a novel modularized hierarchical convolutional neural network to achieve automatic academic paper rating. Evaluation results show that the proposed model outperforms the baselines by a large margin. The dataset and code are available at this https URL",
                "doi": "https://doi.org/10.48550/arxiv.1805.03977",
                "title": "Automatic Academic Paper Rating Based on Modularized Hierarchical\n  Convolutional Neural Network",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1808.05492",
        "mid": "2887894061",
        "abstract": "When neural networks process images which do not resemble the distribution seen during training, so called out-of-distribution images, they often make wrong predictions, and do so too confidently. The capability to detect out-of-distribution images is therefore crucial for many real-world applications. We divide out-of-distribution detection between novelty detection ---images of classes which are not in the training set but are related to those---, and anomaly detection ---images with classes which are unrelated to the training set. By related we mean they contain the same type of objects, like digits in MNIST and SVHN. Most existing work has focused on anomaly detection, and has addressed this problem considering networks trained with the cross-entropy loss. Differently from them, we propose to use metric learning which does not have the drawback of the softmax layer (inherent to cross-entropy methods), which forces the network to divide its prediction power over the learned classes. We perform extensive experiments and evaluate both novelty and anomaly detection, even in a relevant application such as traffic sign recognition, obtaining comparable or better results than previous works.",
        "related_work": "Anomaly and novelty detection. Also known as out-of-distribution detection, it aims at identifying inputs that are completely different from or unknown to the original data distribution used for training @cite_14 . @cite_18 , they perform novelty detection by learning a distance in an embedding. It proposes a Kernel Null Foley-Sammon transform that aims at projecting all the samples of each in-distribution class into a single point in a certain space. Consequently, novelty detection can be performed by thresholding the distance of a test sample to the nearest of the collapsed class representations. However, they employ handcrafted features, thus optimizing only the transform parameters and not the representation, like in the presently dominating paradigm of deep learning.",
        "ref_abstract": {
            "@cite_18": {
                "mid": "2081604642",
                "abstract": "Detecting samples from previously unknown classes is a crucial task in object recognition, especially when dealing with real-world applications where the closed-world assumption does not hold. We present how to apply a null space method for novelty detection, which maps all training samples of one class to a single point. Beside the possibility of modeling a single class, we are able to treat multiple known classes jointly and to detect novelties for a set of classes with a single model. In contrast to modeling the support of each known class individually, our approach makes use of a projection in a joint subspace where training samples of all known classes have zero intra-class variance. This subspace is called the null space of the training data. To decide about novelty of a test sample, our null space approach allows for solely relying on a distance measure instead of performing density estimation directly. Therefore, we derive a simple yet powerful method for multi-class novelty detection, an important problem not studied sufficiently so far. Our novelty detection approach is assessed in comprehensive multi-class experiments using the publicly available datasets Caltech-256 and Image Net. The analysis reveals that our null space approach is perfectly suited for multi-class novelty detection since it outperforms all other methods.",
                "doi": "https://doi.org/10.1109/cvpr.2013.433",
                "title": "Kernel Null Space Methods for Novelty Detection",
                "publication_year": 2013
            },
            "@cite_14": {
                "mid": "2115627867",
                "abstract": "Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as ''one-class classification'', in which a model is constructed to describe ''normal'' training data. The novelty detection approach is typically used when the quantity of available ''abnormal'' data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that ''normality'' may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.",
                "doi": "https://doi.org/10.1016/j.sigpro.2013.12.026",
                "title": "A review of novelty detection",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1808.04581",
        "mid": "2949220746",
        "abstract": "The Authentication and Authorization for Constrained Environments (ACE) framework provides fine-grained access control in the Internet of Things, where devices are resource-constrained and with limited connectivity. The ACE framework defines separate profiles to specify how exactly entities interact and what security and communication protocols to use. This paper presents the novel ACE IPsec profile, which specifies how a client establishes a secure IPsec channel with a resource server, contextually using the ACE framework to enforce authorized access to remote resources. The profile makes it possible to establish IPsec Security Associations, either through their direct provisioning or through the standard IKEv2 protocol. We provide the first Open Source implementation of the ACE IPsec profile for the Contiki OS and test it on the resource-constrained Zolertia Firefly platform. Our experimental performance evaluation confirms that the IPsec profile and its operating modes are affordable and deployable also on constrained IoT platforms.",
        "related_work": "Finally, Sciancalepore propose a different authorization framework for the IoT @cite_24 , also based on OAuth 2.0 and other standard protocols. In particular, it provides access control through an intermediary gateway acting as mediator between IoT networks and non-constrained Internet segments. However, unlike the ACE framework, @cite_24 displays a considerably higher level of complexity and requires the intermediary gateway to be fully trusted.",
        "ref_abstract": {
            "@cite_24": {
                "mid": "2751141094",
                "abstract": "While the Internet of Things is breaking into the market, the controlled access to constrained resources still remains a blocking concern. Unfortunately, conventional solutions already accepted for both web and cloud applications cannot be directly used in this context. In fact, they generally require high computational and bandwidth capabilities (that are impossible to reach with constrained devices) and offer poor interoperability against standardized communication protocols for the Internet of Things. To solve this issue, this contribution presents a flexible authentication and authorization framework for the Internet of Things, namely OAuth-IoT. It leverages and properly harmonizes existing open-standards (including the OAuth 2.0 authorization framework, different token formats, and the protocol suite for the Internet of Things tailored by the Internet Engineering Task Force), while carefully taking into account the limited capabilities of constrained devices. Functionalities and benefits offered by OAuth-IoT are pragmatically shown by means of an experimental testbed, and further demonstrated with a very preliminary performance assessment.",
                "doi": "https://doi.org/10.1109/iscc.2017.8024606",
                "title": "OAuth-IoT: An access control framework for the Internet of Things based on open standards",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1808.04446",
        "mid": "2950895666",
        "abstract": "Recent breakthroughs in computer vision and natural language processing have spurred interest in challenging multi-modal tasks such as visual question-answering and visual dialogue. For such tasks, one successful approach is to condition image-based convolutional network computation on language via Feature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and shifting. We propose to generate the parameters of FiLM layers going up the hierarchy of a convolutional network in a multi-hop fashion rather than all at once, as in prior work. By alternating between attending to the language input and generating FiLM layer parameters, this approach is better able to scale to settings with longer input sequences such as dialogue. We demonstrate that multi-hop FiLM generation achieves state-of-the-art for the short input sequence task ReferIt --- on-par with single-hop FiLM generation --- while also significantly outperforming prior state-of-the-art and single-hop FiLM generation on the GuessWhat?! visual dialogue task.",
        "related_work": "The game @cite_23 has been a testbed for various vision-and-language tasks over the past years, including object retrieval @cite_19 @cite_15 @cite_32 @cite_34 @cite_11 @cite_44 , semantic image segmentation @cite_28 @cite_1 , and generating referring descriptions @cite_15 @cite_11 @cite_32 . To tackle object retrieval, @cite_19 @cite_15 @cite_44 extract additional visual features such as relative object locations and @cite_32 @cite_11 use reinforcement learning to iteratively train the object retrieval and description generation models. Closer to our work, @cite_25 @cite_34 use the full image and the object crop to locate the correct object. While some previous work relies on task-specific modules @cite_15 @cite_44 , our approach is general and can be easily extended to other vision-and-language tasks.",
        "ref_abstract": {
            "@cite_28": {
                "mid": "2302548814",
                "abstract": "In this paper we approach the novel problem of segmenting an image based on a natural language expression. This is different from traditional semantic segmentation over a predefined set of semantic classes, as e.g., the phrase \u201ctwo men sitting on the right bench\u201d requires segmenting only the two people on the right bench and no one standing or sitting on another bench. Previous approaches suitable for this task were limited to a fixed set of categories and or rectangular regions. To produce pixelwise segmentation for the language expression, we propose an end-to-end trainable recurrent and convolutional network model that jointly learns to process visual and linguistic information. In our model, a recurrent neural network is used to encode the referential expression into a vector representation, and a fully convolutional network is used to a extract a spatial feature map from the image and output a spatial response map for the target object. We demonstrate on a benchmark dataset that our model can produce quality segmentation output from the natural language expression, and outperforms baseline methods by a large margin.",
                "doi": "https://doi.org/10.1007/978-3-319-46448-0_7",
                "title": "Segmentation from Natural Language Expressions",
                "publication_year": 2016
            },
            "@cite_1": {
                "mid": "2247513039",
                "abstract": "Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr30k Entities and ReferItGame datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets.",
                "doi": "https://doi.org/10.1007/978-3-319-46448-0_49",
                "title": "Grounding of Textual Phrases in Images by Reconstruction",
                "publication_year": 2016
            },
            "@cite_32": {
                "mid": "2571175805",
                "abstract": "Referring expressions are natural language constructions used to identify particular objects within a scene. In this paper, we propose a unified framework for the tasks of referring expression comprehension and generation. Our model is composed of three modules: speaker, listener, and reinforcer. The speaker generates referring expressions, the listener comprehends referring expressions, and the reinforcer introduces a reward function to guide sampling of more discriminative expressions. The listener-speaker modules are trained jointly in an end-to-end learning framework, allowing the modules to be aware of one another during learning while also benefiting from the discriminative reinforcer&#x2019;s feedback. We demonstrate that this unified framework and training achieves state-of-the-art results for both comprehension and generation on three referring expression datasets.",
                "doi": "https://doi.org/10.1109/cvpr.2017.375",
                "title": "A Joint Speaker-Listener-Reinforcer Model for Referring Expressions",
                "publication_year": 2017
            },
            "@cite_44": {
                "mid": "2784458614",
                "abstract": "In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks.",
                "doi": "https://doi.org/10.48550/arxiv.1801.08186",
                "title": "MAttNet: Modular Attention Network for Referring Expression\n  Comprehension",
                "publication_year": 2018
            },
            "@cite_19": {
                "mid": "2964284374",
                "abstract": "Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.",
                "doi": "https://doi.org/10.1007/978-3-319-46493-0_48",
                "title": "Modeling Context Between Objects for Referring Expression Understanding",
                "publication_year": 2016
            },
            "@cite_23": {
                "mid": "2251512949",
                "abstract": "In this paper we introduce a new game to crowd-source natural language referring expressions. By designing a two player game, we can both collect and verify referring expressions directly within the game. To date, the game has produced a dataset containing 130,525 expressions, referring to 96,654 distinct objects, in 19,894 photographs of natural scenes. This dataset is larger and more varied than previous REG datasets and allows us to study referring expressions in real-world scenes. We provide an in depth analysis of the resulting dataset. Based on our findings, we design a new optimization based model for generating referring expressions and perform experimental evaluations on 3 test sets.",
                "doi": "https://doi.org/10.3115/v1/d14-1086",
                "title": "ReferItGame: Referring to Objects in Photographs of Natural Scenes",
                "publication_year": 2014
            },
            "@cite_15": {
                "mid": "2489434015",
                "abstract": "Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from https: github.com lichengunc refer), shows the advantages of our methods for both referring expression generation and comprehension.",
                "doi": "https://doi.org/10.1007/978-3-319-46475-6_5",
                "title": "Modeling Context in Referring Expressions",
                "publication_year": 2016
            },
            "@cite_34": {
                "mid": "2770129969",
                "abstract": "Recognising objects according to a pre-defined fixed set of class labels has been well studied in the Computer Vision. There are a great many practical applications where the subjects that may be of interest are not known beforehand, or so easily delineated, however. In many of these cases natural language dialog is a natural way to specify the subject of interest, and the task achieving this capability (a.k.a, Referring Expression Comprehension) has recently attracted attention. To this end we propose a unified framework, the ParalleL AttentioN (PLAN) network, to discover the object in an image that is being referred to in variable length natural expression descriptions, from short phrases query to long multi-round dialogs. The PLAN network has two attention mechanisms that relate parts of the expressions to both the global visual content and also directly to object candidates. Furthermore, the attention mechanisms are recurrent, making the referring process visualizable and explainable. The attended information from these dual sources are combined to reason about the referred object. These two attention mechanisms can be trained in parallel and we find the combined system outperforms the state-of-art on several benchmarked datasets with different length language input, such as RefCOCO, RefCOCO+ and GuessWhat?!.",
                "doi": "https://doi.org/10.1109/cvpr.2018.00447",
                "title": "Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries",
                "publication_year": 2018
            },
            "@cite_25": {
                "mid": "2963735856",
                "abstract": "In this paper, we address the task of natural language object retrieval, to localize a target object within a given image based on a natural language query of the object. Natural language object retrieval differs from text-based image retrieval task as it involves spatial information about objects within the scene and global scene context. To address this issue, we propose a novel Spatial Context Recurrent ConvNet (SCRC) model as scoring function on candidate boxes for object retrieval, integrating spatial configurations and global scene-level contextual information into the network. Our model processes query text, local image descriptors, spatial configurations and global context features through a recurrent network, outputs the probability of the query text conditioned on each candidate box as a score for the box, and can transfer visual-linguistic knowledge from image captioning domain to our task. Experimental results demonstrate that our method effectively utilizes both local and global information, outperforming previous baseline methods significantly on different datasets and scenarios, and can exploit large scale vision and language datasets for knowledge transfer.",
                "doi": "https://doi.org/10.1109/cvpr.2016.493",
                "title": "Natural Language Object Retrieval",
                "publication_year": 2016
            },
            "@cite_11": {
                "mid": "2583360688",
                "abstract": "We consider generation and comprehension of natural language referring expression for objects in an image. Unlike generic image captioning which lacks natural standard evaluation criteria, quality of a referring expression may be measured by the receivers ability to correctly infer which object is being described. Following this intuition, we propose two approaches to utilize models trained for comprehension task to generate better expressions. First, we use a comprehension module trained on human-generated expressions, as a critic of referring expression generator. The comprehension module serves as a differentiable proxy of human evaluation, providing training signal to the generation module. Second, we use the comprehension model in a generate-and-rerank pipeline, which chooses from candidate expressions generated by a model according to their performance on the comprehension task. We show that both approaches lead to improved referring expression generation on multiple benchmark datasets.",
                "doi": "https://doi.org/10.1109/cvpr.2017.333",
                "title": "Comprehension-Guided Referring Expressions",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1808.04091",
        "mid": "2887561040",
        "abstract": "In this paper, we propose the task of live comment generation. Live comments are a new form of comments on videos, which can be regarded as a mixture of comments and chats. A high-quality live comment should be not only relevant to the video, but also interactive with other users. In this work, we first construct a new dataset for live comment generation. Then, we propose a novel end-to-end model to generate the human-like live comments by referring to the video and the other users' comments. Finally, we evaluate our model on the constructed dataset. Experimental results show that our method can significantly outperform the baselines.",
        "related_work": "One task that is similar to live comment generation is image caption generation, which is an area that has been studied for a long time. tried to generate descriptions of an image by retrieving from a big sentence pool. proposed to generate descriptions based on the parsing result of the image with a simple language model. These systems are often applied in a pipeline fashion, and the generated description is not creative. More recent work is to use stepwise merging network to improve the performance @cite_2 .",
        "ref_abstract": {
            "@cite_2": {
                "mid": "2952978943",
                "abstract": "The encode-decoder framework has shown recent success in image captioning. Visual attention, which is good at detailedness, and semantic attention, which is good at comprehensiveness, have been separately proposed to ground the caption on the image. In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time. At each time step when generating the caption, the decoder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined. The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances.(The code is available at this https URL)",
                "doi": "https://doi.org/10.48550/arxiv.1808.08732",
                "title": "simNet: Stepwise Image-Topic Merging Network for Generating Detailed and\n  Comprehensive Image Captions",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1807.10002",
        "mid": "2884915206",
        "abstract": "Estimating human gaze from natural eye images only is a challenging task. Gaze direction can be defined by the pupil- and the eyeball center where the latter is unobservable in 2D images. Hence, achieving highly accurate gaze estimates is an ill-posed problem. In this paper, we introduce a novel deep neural network architecture specifically designed for the task of gaze estimation from single eye input. Instead of directly regressing two angles for the pitch and yaw of the eyeball, we regress to an intermediate pictorial representation which in turn simplifies the task of 3D gaze direction estimation. Our quantitative and qualitative results show that our approach achieves higher accuracies than the state-of-the-art and is robust to variation in gaze, head pose and image quality.",
        "related_work": "Traditional approaches to image-based gaze estimation are typically categorized as or . Feature-based approaches reduce an eye image down to a set of features based on hand-crafted rules @cite_39 @cite_1 @cite_26 @cite_0 and then feed these features into simple, often linear machine learning models to regress the final gaze estimate. Model-based methods instead attempt to fit a known 3D model to the eye image @cite_6 @cite_7 @cite_10 @cite_21 by minimizing a suitable energy.",
        "ref_abstract": {
            "@cite_26": {
                "mid": "2087862817",
                "abstract": "Despite the widespread use of mobile phones and tablets, hand-held portable devices have only recently been identified as a promising platform for gaze-aware applications. Estimating gaze on portable devices is challenging given their limited computational resources, low quality integrated front-facing RGB cameras, and small screens to which gaze is mapped. In this paper we present EyeTab, a model-based approach for binocular gaze estimation that runs entirely on an unmodified tablet. EyeTab builds on set of established image processing and computer vision algorithms and adapts them for robust and near-realtime gaze estimation. A technical prototype evaluation with eight participants in a normal indoors office setting shows that EyeTab achieves an average gaze estimation accuracy of 6.88\u00b0 of visual angle at 12 frames per second.",
                "doi": "https://doi.org/10.1145/2578153.2578185",
                "title": "EyeTab",
                "publication_year": 2014
            },
            "@cite_7": {
                "mid": "1971652879",
                "abstract": "Simple-setup real-time gaze estimation system using only a consumer depth camera.3D model based gaze estimation method allowing free head movement.Iris center localization method able to handle relatively low-quality eye images.Simple system calibration easy to be carried out for nonprofessional users. Existing eye-gaze-tracking systems typically require multiple infrared (IR) lights and high-quality cameras to achieve good performance and robustness against head movement. This requirement limits the systems' potential for broader applications. In this paper, we present a low-cost, non-intrusive, simple-setup gaze estimation system that can estimate the gaze direction under free head movement. In particular, the proposed system only uses a consumer depth camera (Kinect sensor) positioned at a distance from the subject. We develop a simple procedure to calibrate the geometric relationship between the screen and the camera, and subject-specific parameters. A parameterized iris model is then used to locate the center of the iris for gaze feature extraction, which can handle low-quality eye images. Finally, the gaze direction is determined based on a 3D geometric eye model, where the head movement and deviation of the visual axis from the optical axis are taken into consideration. Experimental results indicate that the system can estimate gaze with an accuracy of 1.4-2.7? and is robust against large head movements. Two real-time human-computer interaction (HCI) applications are presented to demonstrate the potential of the proposed system for wide applications.",
                "doi": "https://doi.org/10.1016/j.ins.2015.02.004",
                "title": "Real time gaze estimation with a consumer depth camera",
                "publication_year": 2015
            },
            "@cite_21": {
                "mid": "2778474385",
                "abstract": "3D model-based gaze estimation methods are widely explored because of their good accuracy and ability to handle free head movement. Traditional methods with complex hardware systems (Eg. infrared lights, 3D sensors, etc.) are restricted to controlled environments, which significantly limit their practical utilities. In this paper, we propose a 3D model-based gaze estimation method with a single web-camera, which enables instant and portable eye gaze tracking. The key idea is to leverage on the proposed 3D eye-face model, from which we can estimate 3D eye gaze from observed 2D facial landmarks. The proposed system includes a 3D deformable eye-face model that is learned offline from multiple training subjects. Given the deformable model, individual 3D eye-face models and personal eye parameters can be recovered through the unified calibration algorithm. Experimental results show that the proposed method outperforms state-of-the-art methods while allowing convenient system setup and free head movement. A real time eye tracking system running at 30 FPS also validates the effectiveness and efficiency of the proposed method.",
                "doi": "https://doi.org/10.1109/iccv.2017.114",
                "title": "Real Time Eye Gaze Tracking with 3D Deformable Eye-Face Model",
                "publication_year": 2017
            },
            "@cite_1": {
                "mid": "2004128235",
                "abstract": "Most eye gaze estimation systems rely on explicit calibration, which is inconvenient to the user, limits the amount of possible training data and consequently the performance. Since there is likely a strong correlation between gaze and interaction cues, such as cursor and caret locations, a supervised learning algorithm can learn the complex mapping between gaze features and the gaze point by training on incremental data collected implicitly from normal computer interactions. We develop a set of robust geometric gaze features and a corresponding data validation mechanism that identifies good training data from noisy interaction-informed data collected in real-use scenarios. Based on a study of gaze movement patterns, we apply behavior-informed validation to extract gaze features that correspond with the interaction cue, and data-driven validation provides another level of crosschecking using previous good data. Experimental evaluation shows that the proposed method achieves an average error of 4.06o, and demonstrates the effectiveness of the proposed gaze estimation method and corresponding validation mechanism.",
                "doi": "https://doi.org/10.1145/2647868.2655031",
                "title": "Building a Self-Learning Eye Gaze Model from User Interaction Data",
                "publication_year": 2014
            },
            "@cite_6": {
                "mid": "2097087373",
                "abstract": "Most commercial eye gaze tracking systems are based on the use of infrared lights. However, such systems may not work outdoor or may have a very limited head box for them to work. This paper proposes a non-infrared based approach to track one's eye gaze with an RGBD camera (in our case, Kinect). The proposed method adopts a personalized 3D face model constructed off-line. To detect the eye gaze, our system tracks the iris center and a set of 2D facial landmarks whose 3D locations are provided by the RGBD camera. A simple onetime calibration procedure is used to obtain the parameters of the personalized eye gaze model. We compare the performance of the proposed method against the 2D approach using only RGB input on the same images, and find that the use of depth information directly from Kinect achieves more accurate tracking. As expected, the results from the proposed method are not as accurate as the ones from infrared-based approaches. However, this method has the potential for practical use with upcoming better and cheaper depth cameras.",
                "doi": "https://doi.org/10.1145/2638728.2641694",
                "title": "Eye gaze tracking using an RGBD camera",
                "publication_year": 2014
            },
            "@cite_39": {
                "mid": "2048968870",
                "abstract": "Low cost eye tracking is an actual challenging research topic for the eye tracking community. Gaze tracking based on a web cam and without infrared light is a searched goal to broaden the applications of eye tracking systems. Web cam based eye tracking results in new challenges to solve such as a wider field of view and a lower image quality. In addition, no infrared light implies that glints cannot be used anymore as a tracking feature. In this paper, a thorough study has been carried out to evaluate pupil (iris) center-eye corner (PC-EC) vector as feature for gaze estimation based on interpolation methods in low cost eye tracking, as it is considered to be partially equivalent to the pupil center-corneal reflection (PC-CR) vector. The analysis is carried out both based on simulated and real data. The experiments show that eye corner positions in the image move slightly when the user is looking at different points of the screen, even with a static head position. This lowers the possible accuracy of the gaze estimation, significantly reducing the accuracy of the system under standard working conditions to 2--3 degrees.",
                "doi": "https://doi.org/10.1145/2168556.2168598",
                "title": "Evaluation of pupil center-eye corner vector for gaze estimation using a web cam",
                "publication_year": 2012
            },
            "@cite_0": {
                "mid": "2598992495",
                "abstract": "We study gaze estimation on tablets; our key design goal is uncalibrated gaze estimation using the front-facing camera during natural use of tablets, where the posture and method of holding the tablet are not constrained. We collected a large unconstrained gaze dataset of tablet users, labeled Rice TabletGaze dataset. The dataset consists of 51 subjects, each with 4 different postures and 35 gaze locations. Subjects vary in race, gender and in their need for prescription glasses, all of which might impact gaze estimation accuracy. We made three major observations on the collected data and employed a baseline algorithm for analyzing the impact of several factors on gaze estimation accuracy. The baseline algorithm is based on multilevel HoG feature and Random Forests regressor, which achieves a mean error of 3.17 cm. We perform extensive evaluation on the impact of various practical factors such as person dependency, dataset size, race, wearing glasses and user posture on the gaze estimation accuracy.",
                "doi": "https://doi.org/10.1007/s00138-017-0852-4",
                "title": "TabletGaze: dataset and analysis for unconstrained appearance-based gaze estimation in mobile tablets",
                "publication_year": 2017
            },
            "@cite_10": {
                "mid": "2519247488",
                "abstract": "Morphable face models are a powerful tool, but have previously failed to model the eye accurately due to complexities in its material and motion. We present a new multi-part model of the eye that includes a morphable model of the facial eye region, as well as an anatomy-based eyeball model. It is the first morphable model that accurately captures eye region shape, since it was built from high-quality head scans. It is also the first to allow independent eyeball movement, since we treat it as a separate part. To showcase our model we present a new method for illumination- and head-pose\u2013invariant gaze estimation from a single RGB image. We fit our model to an image through analysis-by-synthesis, solving for eye region shape, texture, eyeball pose, and illumination simultaneously. The fitted eyeball pose parameters are then used to estimate gaze direction. Through evaluation on two standard datasets we show that our method generalizes to both webcam and high-quality camera images, and outperforms a state-of-the-art CNN method achieving a gaze estimation accuracy of (9.44^ ) in a challenging user-independent scenario.",
                "doi": "https://doi.org/10.1007/978-3-319-46448-0_18",
                "title": "A 3D Morphable Eye Region Model for Gaze Estimation",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1807.05151",
        "mid": "2951039794",
        "abstract": "Investigative journalism in recent years is confronted with two major challenges: 1) vast amounts of unstructured data originating from large text collections such as leaks or answers to Freedom of Information requests, and 2) multi-lingual data due to intensified global cooperation and communication in politics, business and civil society. Faced with these challenges, journalists are increasingly cooperating in international networks. To support such collaborations, we present the new version of new s leak 2.0, our open-source software for content-based searching of leaks. It includes three novel main features: 1) automatic language detection and language-dependent information extraction for 40 languages, 2) entity and keyword visualization for efficient exploration, and 3) decentral deployment for analysis of confidential data from various formats. We illustrate the new analysis capabilities with an exemplary case study.",
        "related_work": "@cite_10 is a more advanced open-source application developed by computer scientists in collaboration with journalists to support investigative journalism. The application supports import of PDF, MS Office and HTML documents, document clustering based on topic similarity, a simplistic location entity detection, full-text search, and document tagging. Since this tool is already mature and has successfully been used in a number of published news stories, we adapted some of its most useful features such as document tagging and a keyword-in-context (KWIC) view for search hits. Furthermore, in we concentrate on intuitive and visually pleasing approaches to display extracted contents for a fast exploration of collections.",
        "ref_abstract": {
            "@cite_10": {
                "mid": "2027855569",
                "abstract": "For an investigative journalist, a large collection of documents obtained from a Freedom of Information Act request or a leak is both a blessing and a curse: such material may contain multiple newsworthy stories, but it can be difficult and time consuming to find relevant documents. Standard text search is useful, but even if the search target is known it may not be possible to formulate an effective query. In addition, summarization is an important non-search task. We present Overview , an application for the systematic analysis of large document collections based on document clustering, visualization, and tagging. This work contributes to the small set of design studies which evaluate a visualization system \u201cin the wild\u201d, and we report on six case studies where Overview was voluntarily used by self-initiated journalists to produce published stories. We find that the frequently-used language of \u201cexploring\u201d a document collection is both too vague and too narrow to capture how journalists actually used our application. Our iterative process, including multiple rounds of deployment and observations of real world usage, led to a much more specific characterization of tasks. We analyze and justify the visual encoding and interaction techniques used in Overview 's design with respect to our final task abstractions, and propose generalizable lessons for visualization design methodology.",
                "doi": "https://doi.org/10.1109/tvcg.2014.2346431",
                "title": "Overview: The Design, Adoption, and Analysis of a Visual Document Mining Tool for Investigative Journalists",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1807.00324",
        "mid": "2964012674",
        "abstract": "Abstract Middleboxes have become a vital part of modern networks by providing services such as load balancing, optimization of network traffic, and content filtering. A sequence of middleboxes comprising a logical service is called a Service Function Chain (SFC) . In this context, the main issues are to maintain an acceptable level of network path survivability and a fair allocation of the resource between different demands in the event of faults or failures. In this paper, we focus on the problems of traffic engineering, failure recovery, fault prevention, and SFC with reliability and energy consumption constraints in Software Defined Networks (SDN). These types of deployments use Fog computing as an emerging paradigm to manage the distributed small-size traffic flows passing through the SDN-enabled switches (possibly Fog Nodes). The main aim of this integration is to support service delivery in real-time, failure recovery, and fault-awareness in an SFC context. Firstly, we present an architecture for Failure Recovery and Fault Prevention called FRFP; this is a multi-tier structure in which the real-time traffic flows pass through SDN-enabled switches to jointly decrease the network side-effects of flow rerouting and energy consumption of the Fog Nodes. We then mathematically formulate an optimization problem called the Optimal Fog-Supported Energy-Aware SFC rerouting algorithm (OFES) and propose a near-optimal heuristic called Heuristic OFES (HFES) to solve the corresponding problem in polynomial time. In this way, the energy consumption and the reliability of the selected paths are optimized, while the Quality of Service (QoS) constraints are met and the network congestion is minimized. In a reliability context, the focus of this work is on fault prevention; however, since we use a reallocation technique, the proposed scheme can be used as a failure recovery scheme. We compare the performance of HFES and OFES in terms of energy consumption, average path length, fault probability, network side-effects, link utilization, and Fog Node utilization. Additionally, we analyze the computational complexity of HFES. We use a real-world network topology to evaluate our algorithm. The simulation results show that the heuristic algorithm is applicable to large-scale networks.",
        "related_work": "Consequently, numerous works focus on providing SFC in SDNs. An SFC taxonomy that considers architecture and performance dimensions as the basis for the subsequent state-of-the-art analysis is introduced in @cite_1 .",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2538737679",
                "abstract": "Service function chaining is a network capability that provides support for application-driven-networking through the ordered interconnection of service functions. The lifecycle management of service function chains is enabled by two recently emerged technologies, software defined networking and network function virtualization, that promise a number of efficiency, effectiveness, and flexibility gains. This article introduces a service function chaining taxonomy that considers architecture and performance dimensions as the basis for the subsequent stateof- the-art analysis. The article concludes with a gap analysis of existing solutions and the identification of future research challenges.",
                "doi": "https://doi.org/10.1109/mcom.2016.1600219rp",
                "title": "Service Function Chaining in Next Generation Networks: State of the Art and Research Challenges",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1806.07574",
        "mid": "2809204870",
        "abstract": "In this work, we address a challenging problem of fine-grained and coarse-grained recognition of object manipulation actions. Due to the variations in geometrical and motion constraints, there are different manipulations actions possible to perform different sets of actions with an object. Also, there are subtle movements involved to complete most of object manipulation actions. This makes the task of object manipulation action recognition difficult with only just the motion information. We propose to use grasp and motion-constraints information to recognise and understand action intention with different objects. We also provide an extensive experimental evaluation on the recent Yale Human Grasping dataset consisting of large set of 455 manipulation actions. The evaluation involves a) Different contemporary multi-class classifiers, and binary classifiers with one-vs-one multi- class voting scheme, b) Differential comparisons results based on subsets of attributes involving information of grasp and motion-constraints, c) Fine-grained and Coarse-grained object manipulation action recognition based on fine-grained as well as coarse-grained grasp type information, and d) Comparison between Instance level and Sequence level modeling of object manipulation actions. Our results justifies the efficacy of grasp attributes for the task of fine-grained and coarse-grained object manipulation action recognition.",
        "related_work": "Most of the action recognition methodologies models the action using full-body motion based features, which only works well for the specific class of action recognition problem where action set is relatively small such as in @cite_19 , @cite_17 , @cite_0 . These approach do not look useful when it comes to their application on real everyday actions. Research in the area of human action recognition has been mainly focused on full-body motions that can be characterized by movement and change of posture like walking, waving, etc.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2031765333",
                "abstract": "In this paper we introduce a real-time system for action detection. The system uses a small set of robust features extracted from 3D skeleton data. Features are effectively described based on the probability distribution of skeleton data. The descriptor computes a pyramid of sample covariance matrices and mean vectors to encode the relationship between the features. For handling the intra-class variations of actions, such as action temporal scale variations, the descriptor is computed using different window scales for each action. Discriminative elements of the descriptor are mined using feature selection. The system achieves accurate detection results on difficult unsegmented sequences. Experiments on MSRC-12 and G3D datasets show that the proposed system outperforms the state-of-the-art in detection accuracy with very low latency. To the best of our knowledge, we are the first to propose using multi-scale description in action detection from 3D skeleton data.",
                "doi": "https://doi.org/10.1109/wacv.2015.138",
                "title": "Real-Time Multi-scale Action Detection from 3D Skeleton Data",
                "publication_year": 2015
            },
            "@cite_19": {
                "mid": "2486913577",
                "abstract": "We consider the problem of detecting and localizing a human action from continuous action video from depth cameras. We believe that this problem is more challenging than the problem of traditional action recognition as we do not have the information about the starting and ending frames of an action class. Another challenge which makes the problem difficult, is the latency in detection of actions. In this paper, we introduce a greedy approach to detect the action class, invariant of their temporal scale in the testing sequences using class templates and basic skeleton based feature representation from the depth stream data generated using Microsoft Kinect. We evaluate the proposed method on the standard G3D and UTKinect-Action datasets consisting of five and ten actions, respectively. Our results demonstrate that the proposed approach performs well for action detection and recognition under different temporal scales, and is able to outperform the state of the art methods at low latency.",
                "doi": "https://doi.org/10.1109/cvprw.2016.45",
                "title": "Scale Invariant Human Action Detection from Depth Cameras Using Class Templates",
                "publication_year": 2016
            },
            "@cite_17": {
                "mid": "2048821851",
                "abstract": "Recently introduced cost-effective depth sensors coupled with the real-time skeleton estimation algorithm of [16] have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent a human skeleton. In this paper, we propose a new skelet al representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space. Since 3D rigid body motions are members of the special Euclidean group SE(3), the proposed skelet al representation lies in the Lie group SE(3)\u00d7\u2026\u00d7SE(3), which is a curved manifold. Using the proposed representation, human actions can be modeled as curves in this Lie group. Since classification of curves in this Lie group is not an easy task, we map the action curves from the Lie group to its Lie algebra, which is a vector space. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experimental results on three action datasets show that the proposed representation performs better than many existing skelet al representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches.",
                "doi": "https://doi.org/10.1109/cvpr.2014.82",
                "title": "Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1806.07226",
        "mid": "2968805489",
        "abstract": "For the domain of self-driving and automatic parking, perception is a basic and critical technique, moreover, the detection of lane markings and parking slots is an important part of visual perception. Compared with front sight images, panoramic images(PI) can capture more comprehensive pavement information. However, the imbalance of different classes in PI is even more serious. Additionally, the judgment of boundary information between areas is a hard problem in deep models. Therefore, we propose a new model named DFNet to solve these problems. The proposed model has two main contributions, one is dynamic loss weights, and the other is residual fusion block(RFB). DFNet use dynamic loss weights to overcome the negative effect of imbalance dataset, which are calculated according to the pixel number of each class in a batch. RFB is composed of several convolutional layers, a pooling layer, and a fusion layer to combine the feature maps by pixel multiplication, which can reduce boundary information loss. We evaluate our method on PSV dataset, and the achieved advanced results demonstrate the effectiveness of the proposed model.",
        "related_work": "Deep learning has achieved far more accurate results than traditional methods on image processing. There are several works on lane makings and parking slots detection using deep learning on front sight images. J Kim and M Lee @cite_2 presented a robust lane detection method based on the combined convolutional neural network with random sample consensus algorithm; S @cite_25 proposed a unified end-to-end trainable multi-task network that jointly handles lane and road marking detection and recognition guided by a vanishing point; G @cite_7 proposed a decentralized and efficient solution for visual parking slots occupancy detection based on a deep convolutional neural network.",
        "ref_abstract": {
            "@cite_25": {
                "mid": "2964332990",
                "abstract": "In this paper, we propose a unified end-to-end trainable multi-task network that jointly handles lane and road marking detection and recognition that is guided by a vanishing point under adverse weather conditions. We tackle rainy and low illumination conditions, which have not been extensively studied until now due to clear challenges. For example, images taken under rainy days are subject to low illumination, while wet roads cause light reflection and distort the appearance of lane and road markings. At night, color distortion occurs under limited illumination. As a result, no benchmark dataset exists and only a few developed algorithms work under poor weather conditions. To address this shortcoming, we build up a lane and road marking benchmark which consists of about 20,000 images with 17 lane and road marking classes under four different scenarios: no rain, rain, heavy rain, and night. We train and evaluate several versions of the proposed multi-task network and validate the importance of each task. The resulting approach, VPGNet, can detect and classify lanes and road markings, and predict a vanishing point with a single forward pass. Experimental results show that our approach achieves high accuracy and robustness under various conditions in realtime (20 fps). The benchmark and the VPGNet model will be publicly available",
                "doi": "https://doi.org/10.1109/iccv.2017.215",
                "title": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition",
                "publication_year": 2017
            },
            "@cite_7": {
                "mid": "2545300838",
                "abstract": "We propose an effective CNN architecture for visual parking occupancy detection.The CNN architecture is small enough to run on smart cameras.The proposed solution performs and generalizes better than other SotA approaches.We provide a new training validation dataset for parking occupancy detection. A smart camera is a vision system capable of extracting application-specific information from the captured images. The paper proposes a decentralized and efficient solution for visual parking lot occupancy detection based on a deep Convolutional Neural Network (CNN) specifically designed for smart cameras. This solution is compared with state-of-the-art approaches using two visual datasets: PKLot, already existing in literature, and CNRPark-EXT. The former is an existing dataset, that allowed us to exhaustively compare with previous works. The latter dataset has been created in the context of this research, accumulating data across various seasons of the year, to test our approach in particularly challenging situations, exhibiting occlusions, and diverse and difficult viewpoints. This dataset is public available to the scientific community and is another contribution of our research. Our experiments show that our solution outperforms and generalizes the best performing approaches on both datasets. The performance of our proposed CNN architecture on the parking lot occupancy detection task, is comparable to the well-known AlexNet, which is three orders of magnitude larger.",
                "doi": "https://doi.org/10.1016/j.eswa.2016.10.055",
                "title": "Deep learning for decentralized parking lot occupancy detection",
                "publication_year": 2017
            },
            "@cite_2": {
                "mid": "1829670322",
                "abstract": "In this paper, we introduce a robust lane detection method based on the combined convolutional neural network (CNN) with random sample consensus (RANSAC) algorithm. At first, we calculate edges in an image using a hat shape kernel and then detect lanes using the CNN combined with the RANSAC. If the road scene is simple, we can easily detect the lane by using the RANSAC algorithm only. But if the road scene is complex and includes roadside trees, fence, or intersection etc., then it is hard to detect lanes robustly because of noisy edges. To alleviate that problem, we use CNN in the lane detection before and after applying the RANSAC algorithm. In training process of CNN, input data consist of edge images in a region of interest (ROI) and target data become the images that have only drawn real white color lane in black background. The CNN structure consists of 8 layers with 3 convolutional layers, 2 subsampling layers and multi-layer perceptron (MLP) including 3 fully-connected layers. Convolutional and subsampling layers are hierarchically arranged and their arrangement represents a deep structure in deep learning. As a result, proposed lane detection algorithm successfully eliminates noise lines and the performance is found to be better than other formal line detection algorithms such as RANSAC and hough transform.",
                "doi": "https://doi.org/10.1007/978-3-319-12637-1_57",
                "title": "Robust Lane Detection Based On Convolutional Neural Network and Random Sample Consensus",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1806.07116",
        "mid": "2964174544",
        "abstract": "We analyze a millimeter wave network, deployed along the streets of a city, in terms of positioning and downlink data-rate performance, respectively. First, we present a transmission scheme where the base stations provide jointly positioning and data-communication functionalities. Accordingly, we study the trade- off between the localization and the data rate performance based on theoretical bounds. Then, we obtain an upper bound on the probability of beam misalignment based on the derived localization error bound. Finally, we prescribe the network operator a scheme to select the beamwidth and the power splitting factor between the localization and communication functions to address different quality of service requirements, while limiting cellular outage.",
        "related_work": "* -0.2cm In the context of sub-6GHz systems, @cite_4 have studied a distributed antenna system providing both data communication and positioning functionalities. The authors assumed that the UEs know the positions of the BSs and attempt to estimate their own positions based on the received signals. @cite_5 have shown that localization using mm-wave frequencies is efficient in terms of accuracy, even in the presence of a limited number of anchor nodes. In fact, mm-wave beam-forming allows for accurate localization and orientation of UEs with respect to the BSs @cite_11 . @cite_12 have studied a location-aided initial access strategy for mm-wave networks, in which the information of UE locations enables to speed up the channel estimation and beam-forming procedures. @cite_11 have studied the trade-off between communication rate and positioning quality in a single user mm-wave link. Similarly @cite_16 have studied the beamforming optimization and spectral power allocation based on theoretical localization bounds.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2569316870",
                "abstract": "A distributed antenna system whose goal is to provide data communication and positioning functionalities to mobile stations (MSs) is studied. Each MS receives data from a number of base stations (BSs) and uses the received signal not only to extract the information but to determine its location as well. This is done based on time-of-arrival or time-difference-of-arrival measurements, depending on the assumed synchronization conditions. The problem of minimizing the overall power expenditure of the BSs under data throughput and localization accuracy requirements is formulated with respect to the beamforming vectors used at the BSs. The analysis covers both frequency-flat and frequency-selective channels and accounts for robustness constraints in the presence of parameter uncertainty as well. The proposed algorithmic solutions are based on rank-relaxation and difference-of-convex programming.",
                "doi": "https://doi.org/10.1109/tvt.2014.2317831",
                "title": "Beamforming Design for Joint Localization and Data Transmission in Distributed Antenna System",
                "publication_year": 2015
            },
            "@cite_5": {
                "mid": "2527530399",
                "abstract": "mmWave (millimeter-Wave) is a very promising technology for the future wireless communication. To mitigate its high attenuation characteristics, mmWave communication frequently employs directional beamforming for both transmission and reception. Localization commonly takes advantage of directionality in RF frequencies in urban and indoor environments. In this paper, we use lessons learned from classical RF-based localization for discussing a set of feasible localization approaches in the context of mmWave bands. We further map the requirements of each discussed localization approach to design requirements for future mmWave devices and assess the expected accuracy of such approaches for a set of realistic scenarios. Our results show that mmWave-based localization is promising in both its availability and accuracy, even in the presence of a limited number of localization anchor nodes.",
                "doi": "https://doi.org/10.1109/iwcmc.2016.7577201",
                "title": "Localization as a feature of mmWave communication",
                "publication_year": 2016
            },
            "@cite_16": {
                "mid": "2783354023",
                "abstract": "In this paper we study optimal beamforming policy to minimize the Cramer Rao Lower Bound (CRLB) of joint angle of arrival (AoA) and time delay estimation for a multicarrier millimeter wave (mmWave) system. Considering one single base station (BS) with rough a priori knowledge of channel coefficients and location information of the mobile station (MS), we show that it is possible to improve the accuracy of AoA and time delay estimation, and hence of mobile station (MS) positioning, by means of optimized beamforming. Mathematical formulation of the CRLB is first introduced and the related optimization problem is solved for a each single subcarrier independently. Then we propose a solution optimizing the beamformer jointly for the multicarrier system. A few examples are provided to illustrate the benefits from bound-based beamforming optimization.",
                "doi": "https://doi.org/10.1109/wpnc.2017.8250057",
                "title": "Localization bound based beamforming optimization for multicarrier mmWave MIMO",
                "publication_year": 2017
            },
            "@cite_12": {
                "mid": "2509489695",
                "abstract": "Millimeter-wave (mm-wave) communication is a promising technology for next-generation wireless systems. One challenging application lies in the vehicular domain, where mm-wave should support ultra-fast and high-rate data exchanges among vehicles and between vehicles and infrastructure. To achieve ultra-fast initial access between nodes, we propose a location-aided beamforming strategy and analyze the resulting performance in terms of antenna gain and latency. We find that location information can significantly speed up initial access.",
                "doi": "https://doi.org/10.1109/spawc.2016.7536855",
                "title": "Location-aided mm-wave channel estimation for vehicular communication",
                "publication_year": 2016
            },
            "@cite_11": {
                "mid": "2725848324",
                "abstract": "millimeter wave (mmW) communication systems have the potential to increase data rates with low-latency, highly directional communication links. Due to the geometric nature of the propagation, mmW signals can also be used for accurate positioning. This paper explores the trade-off between communication rate and positioning quality in mmW systems. We show how rate and positioning quality interact as a function of bandwidth, number of antennas, and receiver location.",
                "doi": "https://doi.org/10.1109/iccw.2017.7962756",
                "title": "On the trade-off between positioning and data rate for mm-wave communication",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1806.05325",
        "mid": "2807817052",
        "abstract": "Millimeter wave (mmWave) signals are much more sensitive to blockage, which results in a significant increase of the outage probability, especially for the users at the edge of the cells. In this paper, we exploit the technique of base station (BS) cooperation to improve the performance of the cell-edge users in the downlink transmission of mmWave cellular networks. We design two cooperative schemes, which are referred to as fixed-number BS cooperation (FNC) scheme and fixed-region BS cooperation (FRC) scheme, respectively. In FNC scheme, the cooperative BSs consist of the M nearest BSs around the served cell-edge users, and in FRC scheme, the cooperative BSs include all the BSs located within a given region. We derive the expressions for the average rate and outage probability of a typical cell-edge user located at the origin based on the stochastic geometry framework. To reduce the computational complexity of our analytical results for the outage probability, we further propose a Gamma approximation based method to provide approximations with satisfying accuracy. Our analytical results incorporate the critical characteristics of mmWave channels, i.e., the blockage effects, the different path loss of LOS and NLOS links and the highly directional antenna arrays. Simulation results show that the performance of the cell-edge users is greatly improved when mmWave networks are combined with the technique of BS cooperation.",
        "related_work": "To analyze the performance of the mmWave networks, theoretical channel models have been proposed to describe the new propagation characteristics in mmWave bands @cite_24 @cite_28 @cite_14 @cite_7 @cite_39 @cite_6 . In @cite_24 @cite_28 , the authors proposed a LOS ball model to approximate the irregular LOS region, which was shown to be flexible yet accurate enough to capture the features of the blockage effects in mmWave bands by the field measurements @cite_7 . The LOS ball model was further extended to the two-ball-based blockage model in @cite_14 and the multiple-ball-based blockage model in @cite_6 to account for the three different states of each link, i.e., LOS, NLOS and outage. Based on the established theoretical channel models, the network-wide system performance of the mmWave networks was investigated, e.g., in @cite_7 @cite_39 @cite_0 .",
        "ref_abstract": {
            "@cite_14": {
                "mid": "1567643386",
                "abstract": "In this paper, a new mathematical framework to the analysis of millimeter wave cellular networks is introduced. Its peculiarity lies in considering realistic path-loss and blockage models, which are derived from recently reported experimental data. The path-loss model accounts for different distributions of line-of-sight and non-line-of-sight propagation conditions and the blockage model includes an outage state that provides a better representation of the outage possibilities of millimeter wave communications. By modeling the locations of the base stations as points of a Poisson point process and by relying on a noise-limited approximation for typical millimeter wave network deployments, simple and exact integral as well as approximated and closed-form formulas for computing the coverage probability and the average rate are obtained. With the aid of Monte Carlo simulations, the noise-limited approximation is shown to be sufficiently accurate for typical network densities. The noise-limited approximation, however, may not be sufficiently accurate for ultra-dense network deployments and for sub-gigahertz transmission bandwidths. For these case studies, the analytical approach is generalized to take the other-cell interference into account at the cost of increasing its computational complexity. The proposed mathematical framework is applicable to cell association criteria based on the smallest path-loss and on the highest received power. It accounts for beamforming alignment errors and for multi-tier cellular network deployments. Numerical results confirm that sufficiently dense millimeter wave cellular networks are capable of outperforming micro wave cellular networks, in terms of coverage probability and average rate.",
                "doi": "https://doi.org/10.1109/twc.2015.2431689",
                "title": "Stochastic Geometry Modeling and Analysis of Multi-Tier Millimeter Wave Cellular Networks",
                "publication_year": 2015
            },
            "@cite_7": {
                "mid": "1953553238",
                "abstract": "Millimeter wave (mmWave) cellular systems will require high-gain directional antennas and dense base station (BS) deployments to overcome a high near-field path loss and poor diffraction. As a desirable side effect, high-gain antennas offer interference isolation, providing an opportunity to incorporate self-backhauling , i.e., BSs backhauling among themselves in a mesh architecture without significant loss in the throughput, to enable the requisite large BS densities. The use of directional antennas and resource sharing between access and backhaul links leads to coverage and rate trends that significantly differ from conventional UHF cellular systems. In this paper, we propose a general and tractable mmWave cellular model capturing these key trends and characterize the associated rate distribution. The developed model and analysis are validated using actual building locations from dense urban settings and empirically derived path loss models. The analysis shows that, in sharp contrast to the interference-limited nature of UHF cellular networks, the spectral efficiency of mmWave networks (besides the total rate) also increases with the BS density, particularly at the cell edge. Increasing the system bandwidth does not significantly influence the cell edge rate, although it boosts the median and peak rates. With self-backhauling, different combinations of the wired backhaul fraction (i.e., the fraction of BSs with a wired connection) and the BS density are shown to guarantee the same median rate (QoS).",
                "doi": "https://doi.org/10.1109/jsac.2015.2435357",
                "title": "Tractable Model for Rate in Self-Backhauled Millimeter Wave Cellular Networks",
                "publication_year": 2015
            },
            "@cite_28": {
                "mid": "2031858701",
                "abstract": "Millimeter wave (mmWave) holds promise as a carrier frequency for fifth generation cellular networks. Because mmWave signals are sensitive to blockage, prior models for cellular networks operated in the ultra high frequency (UHF) band do not apply to analyze mmWave cellular networks directly. Leveraging concepts from stochastic geometry, this paper proposes a general framework to evaluate the coverage and rate performance in mmWave cellular networks. Using a distance-dependent line-of-site (LOS) probability function, the locations of the LOS and non-LOS base stations are modeled as two independent non-homogeneous Poisson point processes, to which different path loss laws are applied. Based on the proposed framework, expressions for the signal-to-noise-and-interference ratio (SINR) and rate coverage probability are derived. The mmWave coverage and rate performance are examined as a function of the antenna geometry and base station density. The case of dense networks is further analyzed by applying a simplified system model, in which the LOS region of a user is approximated as a fixed LOS ball. The results show that dense mmWave networks can achieve comparable coverage and much higher data rates than conventional UHF cellular systems, despite the presence of blockages. The results suggest that the cell size to achieve the optimal SINR scales with the average size of the area that is LOS to a user.",
                "doi": "https://doi.org/10.1109/twc.2014.2364267",
                "title": "Coverage and Rate Analysis for Millimeter-Wave Cellular Networks",
                "publication_year": 2015
            },
            "@cite_6": {
                "mid": "2614764375",
                "abstract": "In this paper, we provide an analytical framework to analyze heterogeneous downlink millimeter-wave (mm-wave) cellular networks consisting of @math tiers of randomly located base stations (BSs), where each tier operates in an mm-wave frequency band. Signal-to-interference-plus-noise ratio (SINR) coverage probability is derived for the entire network using tools from stochastic geometry. The distinguishing features of mm-wave communications, such as directional beamforming, and having different path loss laws for line-of-sight and non-line-of-sight links are incorporated into the coverage analysis by assuming averaged biased-received power association and Nakagami fading. By using the noise-limited assumption for mm-wave networks, a simpler expression requiring the computation of only one numerical integral for coverage probability is obtained. Also, the effect of beamforming alignment errors on the coverage probability analysis is investigated to get insight on the performance in practical scenarios. Downlink rate coverage probability is derived as well to get more insights on the performance of the network. Moreover, the effect of deploying low-power smaller cells and the impact of biasing factor on energy efficiency is analyzed. Finally, a hybrid cellular network operating in both mm-wave and @math -wave frequency bands is addressed.",
                "doi": "https://doi.org/10.1109/tcomm.2017.2705692",
                "title": "Coverage in Heterogeneous Downlink Millimeter Wave Cellular Networks",
                "publication_year": 2017
            },
            "@cite_39": {
                "mid": "344766627",
                "abstract": "Ad hoc networks provide an on-demand, infrastructure-free means to communicate between soldiers in war zones, aid workers in disaster areas, or consumers in device-to-device (D2D) applications. Unfortunately, ad hoc networks are limited by interference due to nearby transmissions. Millimeter-wave (mmWave) devices offer several potential advantages for ad hoc networks, including reduced interference due to directional antennas and building blockages, not to mention huge bandwidth channels for large data rates. This paper uses a stochastic geometry approach to characterize the one-way and two-way signal-to-interference ratio distribution of a mmWave ad hoc network with directional antennas, random blockages, and ALOHA channel access. The interference-to-noise ratio shows that a fundamental limitation of an ad hoc network, interference, may still be an issue. The performance of mmWave ad hoc networks is bounded by the transmission capacity and area spectral efficiency. The results show that mmWave networks can support much higher densities and larger spectral efficiencies, even in the presence of blockage, compared with lower frequency communication for certain link distances. Due to the increased bandwidth, the rate coverage of mmWave can be much greater than lower frequency devices.",
                "doi": "https://doi.org/10.1109/tsp.2016.2551690",
                "title": "Performance Analysis of Outdoor mmWave Ad Hoc Networks",
                "publication_year": 2016
            },
            "@cite_24": {
                "mid": "1999153524",
                "abstract": "The millimeter-wave (mmWave) band offers the potential for high-bandwidth communication channels in cellular networks. It is not clear, however, whether both high data rates and coverage in terms of signal-to-noise-plus-interference ratio can be achieved in interference-limited mmWave cellular networks due to the differences in propagation conditions and antenna topologies. This article shows that dense mmWave networks can achieve both higher data rates and comparable coverage relative to conventional microwave networks. Sum rate gains can be achieved using more advanced beamforming techniques that allow multiuser transmission. The insights are derived using a new theoretical network model that incorporates key characteristics of mmWave networks.",
                "doi": "https://doi.org/10.1109/mcom.2014.6894455",
                "title": "Coverage and capacity of millimeter-wave cellular networks",
                "publication_year": 2014
            },
            "@cite_0": {
                "mid": "2344809130",
                "abstract": "Recent studies show that millimeter wave (mmWave) communications can offer orders of magnitude, which increases in the cellular capacity. However, the secrecy performance of an mmWave cellular network has not been investigated so far. Leveraging the new path-loss and blockage models for mmWave channels, which are significantly different from the conventional microwave channel, this paper comprehensively studies the network-wide physical layer security performance of the downlink transmission in an mmWave cellular network under a stochastic geometry framework. We first study the secure connectivity probability and the average number of perfect communication links per unit area in a noise-limited mmWave network for both non-colluding and colluding eavesdroppers scenarios, respectively. Then, we evaluate the effect of the artificial noise (AN) on the secrecy performance, and derive the analysis result of average number of perfect communication links per unit area in an interference-limited mmWave network. Numerical results demonstrate the network-wide secrecy performance, and provide interesting insights into how the secrecy performance is influenced by various network parameters: antenna array pattern, base station intensity, and AN power allocation.",
                "doi": "https://doi.org/10.1109/twc.2016.2562010",
                "title": "Physical Layer Security in Millimeter Wave Cellular Networks",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1806.04533",
        "mid": "2964070281",
        "abstract": "Person re-identification (Re-ID) aims to match the image frames which contain the same person in the surveillance videos. Most of the Re-ID algorithms conduct supervised training in some small labeled datasets, so directly deploying these trained models to the real-world large camera networks may lead to a poor performance due to underfitting. The significant difference between the source training dataset and the target testing dataset makes it challenging to incrementally optimize the model. To address this challenge, we propose a novel solution by transforming the unlabeled images in the target domain to fit the original classifier by using our proposed similarity preserved generative adversarial networks model, SimPGAN. Specifically, SimPGAN adopts the generative adversarial networks with the cycle consistency constraint to transform the unlabeled images in the target domain to the style of the source domain. Meanwhile, SimPGAN uses the similarity consistency loss, which is measured by a siamese deep convolutional neural network, to preserve the similarity of the transformed images of the same person. Comprehensive experiments based on multiple real surveillance datasets are conducted, and the results show that our algorithm is better than the state-of-the-art cross-dataset unsupervised person Re-ID algorithms.",
        "related_work": ": Most existing person Re-ID models are supervised, and based on either invariant feature learning @cite_3 , metric learning @cite_10 or deep learning @cite_4 . However, in the practical deployment of Re-ID algorithms in large-scale camera networks, it is usually costly and unpractical to label the massive online surveillance videos to support supervised learning as mentioned in @cite_14 .",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2441160157",
                "abstract": "Most existing person re-identification (Re-ID) approaches follow a supervised learning framework, in which a large number of labelled matching pairs are required for training. This severely limits their scalability in realworld applications. To overcome this limitation, we develop a novel cross-dataset transfer learning approach to learn a discriminative representation. It is unsupervised in the sense that the target dataset is completely unlabelled. Specifically, we present an multi-task dictionary learning method which is able to learn a dataset-shared but targetdata-biased representation. Experimental results on five benchmark datasets demonstrate that the method significantly outperforms the state-of-the-art.",
                "doi": "https://doi.org/10.1109/cvpr.2016.146",
                "title": "Unsupervised Cross-Dataset Transfer Learning for Person Re-identification",
                "publication_year": 2016
            },
            "@cite_4": {
                "mid": "2414767909",
                "abstract": "Person re-identification is to seek a correct match for a person of interest across different camera views among a large number of impostors. It typically involves two procedures of non-linear feature extractions against dramatic appearance changes, and subsequent discriminative analysis in order to reduce intra-personal variations while enlarging inter-personal differences. In this paper, we introduce a hybrid deep architecture which combines Fisher vectors and deep neural networks to learn non-linear transformations of pedestrian images to a deep space where data can be linearly separable. The proposed method starts from Fisher vector encoding which computes a sequence of local feature extraction, aggregation, and encoding. The resulting Fisher vector output are fed into stacked supervised layer to seek non-linear transformation into a deep space. On top of the deep neural network, Linear Discriminant Analysis (LDA) is reinforced such that linearly separable latent representations can be learned in an end-to-end fashion. By optimizing an objective function modified from LDA, the network is enforced to produce feature distributions which have a low variance within the same class and high variance between classes. The objective is essentially derived from the general LDA eigenvalue problem and allows to train the network with Stochastic Gradient Descent and back-propagate LDA gradients to compute Gaussian Mixture Model (GMM) gradients in Fisher vector encoding. For empirical evaluations, we test our approach on four benchmark data sets in person re-identification (VIPeR 1, CUHK03 2, CUHK01 3, and Market 1501 4). Extensive experiments on these benchmarks show that our method can achieve state-of-the-art results. HighlightsA hybrid architecture that combines Fisher vectors and deep neural networks.An end-to-end training with linear discriminant analysis as objective.Deep features are linearly separable and class separability is maximally preserved.",
                "doi": "https://doi.org/10.1016/j.patcog.2016.12.022",
                "title": "Deep linear discriminant analysis on fisher networks: A hybrid architecture for person re-identification",
                "publication_year": 2017
            },
            "@cite_10": {
                "mid": "2341680599",
                "abstract": "Person re-identification aims to match the images of pedestrians across different camera views from different locations. This is a challenging intelligent video surveillance problem that remains an active area of research due to the need for performance improvement. Person re-identification involves two main steps: feature representation and metric learning. Although the keep it simple and straightforward (KISS) metric learning method for discriminative distance metric learning has been shown to be effective for the person re-identification, the estimation of the inverse of a covariance matrix is unstable and indeed may not exist when the training set is small, resulting in poor performance. Here, we present dual-regularized KISS (DR-KISS) metric learning. By regularizing the two covariance matrices, DR-KISS improves on KISS by reducing overestimation of large eigenvalues of the two estimated covariance matrices and, in doing so, guarantees that the covariance matrix is irreversible. Furthermore, we provide theoretical analyses for supporting the motivations. Specifically, we first prove why the regularization is necessary. Then, we prove that the proposed method is robust for generalization. We conduct extensive experiments on three challenging person re-identification datasets, VIPeR, GRID, and CUHK 01, and show that DR-KISS achieves new state-of-the-art performance.",
                "doi": "https://doi.org/10.1109/tip.2016.2553446",
                "title": "Person Re-Identification by Dual-Regularized KISS Metric Learning",
                "publication_year": 2016
            },
            "@cite_3": {
                "mid": "1949591461",
                "abstract": "Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification, feature representation and metric learning. An effective feature representation should be robust to illumination and viewpoint changes, and a discriminant metric should be learned to match various person images. In this paper, we propose an effective feature representation called Local Maximal Occurrence (LOMO), and a subspace and metric learning method called Cross-view Quadratic Discriminant Analysis (XQDA). The LOMO feature analyzes the horizontal occurrence of local features, and maximizes the occurrence to make a stable representation against viewpoint changes. Besides, to handle illumination variations, we apply the Retinex transform and a scale invariant texture operator. To learn a discriminant metric, we propose to learn a discriminant low dimensional subspace by cross-view quadratic discriminant analysis, and simultaneously, a QDA metric is learned on the derived subspace. We also present a practical computation method for XQDA, as well as its regularization. Experiments on four challenging person re-identification databases, VIPeR, QMUL GRID, CUHK Campus, and CUHK03, show that the proposed method improves the state-of-the-art rank-1 identification rates by 2.2 , 4.88 , 28.91 , and 31.55 on the four databases, respectively.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298832",
                "title": "Person re-identification by Local Maximal Occurrence representation and metric learning",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1806.04604",
        "mid": "2963977175",
        "abstract": "This paper describes the development of finite abstractions of Max-Plus-Linear (MPL) systems using tropical operations. The idea of tropical abstraction is inspired by the fact that an MPL system is a discrete-event model updating its state with operations in the tropical algebra. The abstract model is a finite-state transition system: we show that the abstract states can be generated by operations on the tropical algebra, and that the generation of transitions can be established by tropical multiplications of matrices. The complexity of the algorithms based on tropical algebra is discussed and their performance is tested on a numerical benchmark against an existing alternative abstraction approach.",
        "related_work": "The notion of abstraction of an MPL system has been first discussed in @cite_4 . The procedure starts by transforming the MPL system characterised by @math into a PWA (piece-wise affine) model [Algorithm 2] Dieky1 , and then considering the partitions associated to the obtained PWA [Algorithm 6] Dieky1 . The abstract states associated to the partitions are represented by DBMs. The transitions are then generated using one-step forward-reachability analysis @cite_4 : first, the image of each abstract state w.r.t. the MPL system is computed; then, each image is intersected with partitions associated to other abstract states; finally, transition relations are defined for each non-empty intersection. This procedure is summarised in [Algorithm 7] Dieky1 .",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2027649310",
                "abstract": "This work puts forward a novel technique to generate finite abstractions of autonomous and nonautonomous Max-Plus-Linear (MPL) models, a class of discrete-event systems used to characterize the dynamics of the timing related to successive events that synchronize autonomously. Nonautonomous versions of MPL models embed within their dynamics nondeterminism, namely a signal choice that is usually regarded as an exogenous control or schedule. In this paper, abstractions of MPL models are characterized as finite-state Labeled Transition Systems (LTS). LTS are obtained first by partitioning the state space (and, for the nonautonomous model, by covering the input space) of the MPL model and by associating states of the LTS to the introduced partitions, then by defining relations among the states of the LTS based on dynamical transitions between the corresponding partitions of the MPL state space, and finally by labeling the LTS edges according to the one-step timing properties of the events of the original MPL model. In order to establish formal equivalences, the finite abstractions are proven to either simulate or to bisimulate the original MPL model. This approach enables the study of general properties of the original MPL model by verifying (via model checking) equivalent logical specifications over the finite LTS abstraction. The computational aspects related to the abstraction procedure are thoroughly discussed and its performance is tested on a numerical benchmark.",
                "doi": "https://doi.org/10.1109/tac.2013.2273299",
                "title": "Finite Abstractions of Max-Plus-Linear Systems",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1805.12032",
        "mid": "2949112265",
        "abstract": "In the age of social news, it is important to understand the types of reactions that are evoked from news sources with various levels of credibility. In the present work we seek to better understand how users react to trusted and deceptive news sources across two popular, and very different, social media platforms. To that end, (1) we develop a model to classify user reactions into one of nine types, such as answer, elaboration, and question, etc, and (2) we measure the speed and the type of reaction for trusted and deceptive news sources for 10.8M Twitter posts and 6.2M Reddit comments. We show that there are significant differences in the speed and the type of reactions between trusted and deceptive news sources on Twitter, but far smaller differences on Reddit.",
        "related_work": "As we noted above, most studies that examine misinformation spread focus on individual events such as natural disasters @cite_15 , political elections @cite_11 , or crises @cite_16 and examine the response to the event on social media. A recent study by Vosoughi al found that news stories that were fact-checked and found to be false spread faster and to more people than news items found to be true. In contrast, our methodology considers immediate reactions to news of varying credibility, so we can determine whether certain reactions or reactions to trusted or deceptive news sources evoke more or faster responses from social media users.",
        "ref_abstract": {
            "@cite_15": {
                "mid": "2295893477",
                "abstract": "Social media have become an established feature of the dynamic information space that emerges during crisis events. Both emergency responders and the public use these platforms to search for, disseminate, challenge, and make sense of information during crises. In these situations rumors also proliferate, but just how fast such information can spread is an open question. We address this gap, modeling the speed of information transmission to compare retransmission times across content and context features. We specifically contrast rumor-affirming messages with rumor-correcting messages on Twitter during a notable hostage crisis to reveal differences in transmission speed. Our work has important implications for the growing field of crisis informatics.",
                "doi": "https://doi.org/10.1109/hicss.2016.248",
                "title": "Rumors at the Speed of Light? Modeling the Rate of Rumor Transmission During Crisis",
                "publication_year": 2016
            },
            "@cite_16": {
                "mid": "2164082612",
                "abstract": "Characterizing information diffusion on social platforms like Twitter enables us to understand the properties of underlying media and model communication patterns. As Twitter gains in popularity, it has also become a venue to broadcast rumors and misinformation. We use epidemiological models to characterize information cascades in twitter resulting from both news and rumors. Specifically, we use the SEIZ enhanced epidemic model that explicitly recognizes skeptics to characterize eight events across the world and spanning a range of event types. We demonstrate that our approach is accurate at capturing diffusion in these events. Our approach can be fruitfully combined with other strategies that use content modeling and graph theoretic features to detect (and possibly disrupt) rumors.",
                "doi": "https://doi.org/10.1145/2501025.2501027",
                "title": "Epidemiological modeling of news and rumors on Twitter",
                "publication_year": 2013
            },
            "@cite_11": {
                "mid": "2724523750",
                "abstract": "Recent accounts from researchers, journalists, as well as federal investigators, reached a unanimous conclusion: social media are systematically exploited to manipulate and alter public opinion. Some disinformation campaigns have been coordinated by means of bots, social media accounts controlled by computer scripts that try to disguise themselves as legitimate human users. In this study, we describe one such operation that occurred in the run up to the 2017 French presidential election. We collected a massive Twitter dataset of nearly 17 million posts, posted between 27 April and 7 May 2017 (Election Day). We then set to study the MacronLeaks disinformation campaign: By leveraging a mix of machine learning and cognitive behavioral modeling techniques, we separated humans from bots, and then studied the activities of the two groups independently, as well as their interplay. We provide a characterization of both the bots and the users who engaged with them, and oppose it to those users who didn\u2019t. Prior interests of disinformation adopters pinpoint to the reasons of scarce success of this campaign: the users who engaged with MacronLeaks are mostly foreigners with pre-existing interest in alt-right topics and alternative news media, rather than French users with diverse political views. Concluding, anomalous account usage patterns suggest the possible existence of a black market for reusable political disinformation bots.",
                "doi": "https://doi.org/10.5210/fm.v22i8.8005",
                "title": "Disinformation and social bot operations in the run up to the 2017 French presidential election",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1805.10115",
        "mid": "2804680395",
        "abstract": "Motivated by the difficulty of effecting fundamental change in the architecture of the Internet, in this paper, we study from a theoretical perspective the question of how individuals can join forces toward collective ventures. To that end, we draw on an elementary concept in Internet systems engineering, namely, that of incremental deployability, which we study mathematically and computationally. For example, we show that incremental deployability is at least as general a concept as the Nash equilibrium (in that the latter can be derived from the former). We then draw on this foundation to design and analyze institutional mechanisms that are not only promising to bootstrap emerging Internet architectures but they also have broader applications in social organization beyond its predominant market (and finance)-based character.",
        "related_work": "For example, @cite_39 show that the deployment of Secure-BGP can be facilitated by a pair of mechanisms, namely, (i) routing policies that prefer Internet paths safeguarded by Secure-BGP in partial deployment and (ii) offloading cryptography from autonomous systems without a customer (known as stub autonomous systems) to their providers. These techniques improve the incremental deployability of Secure-BGP, however, the incentive structure these techniques induce remains akin to the stag hunt, and, in fact, Gill propose concentrating peer pressure and regulatory efforts to a small fraction of core autonomous systems for their techniques to be effective in driving the growth of Secure-BGP. Our point, therefore, that, although such line of effort is certainly helpful to the adoption of emerging technologies it is not in general conclusive, remains.",
        "ref_abstract": {
            "@cite_39": {
                "mid": "2087039608",
                "abstract": "With a cryptographic root-of-trust for Internet routing(RPKI [17]) on the horizon, we can finally start planning the deployment of one of the secure interdomain routing protocols proposed over a decade ago (Secure BGP [22], secure origin BGP [37]). However, if experience with IPv6 is any indicator, this will be no easy task. Security concerns alone seem unlikely to provide sufficient local incentive to drive the deployment process forward. Worse yet, the security benefits provided by the S*BGP protocols do not even kick in until a large number of ASes have deployed them. Instead, we appeal to ISPs' interest in increasing revenue-generating traffic. We propose a strategy that governments and industry groups can use to harness ISPs' local business objectives and drive global S*BGP deployment. We evaluate our deployment strategy using theoretical analysis and large-scale simulations on empirical data. Our results give evidence that the market dynamics created by our proposal can transition the majority of the Internet to S*BGP.",
                "doi": "https://doi.org/10.1145/2018436.2018439",
                "title": "Let the market drive deployment",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1805.07324",
        "mid": "2804494348",
        "abstract": "Network embedding has become a hot research topic recently which can provide low-dimensional feature representations for many machine learning applications. Current work focuses on either (1) whether the embedding is designed as an unsupervised learning task by explicitly preserving the structural connectivity in the network, or (2) whether the embedding is a by-product during the supervised learning of a specific discriminative task in a deep neural network. In this paper, we focus on bridging the gap of the two lines of the research. We propose to adapt the Generative Adversarial model to perform network embedding, in which the generator is trying to generate vertex pairs, while the discriminator tries to distinguish the generated vertex pairs from real connections (edges) in the network. Wasserstein-1 distance is adopted to train the generator to gain better stability. We develop three variations of models, including GANE which applies cosine similarity, GANE-O1 which preserves the first-order proximity, and GANE-O2 which tries to preserves the second-order proximity of the network in the low-dimensional embedded vector space. We later prove that GANE-O2 has the same objective function as GANE-O1 when negative sampling is applied to simplify the training process in GANE-O2. Experiments with real-world network datasets demonstrate that our models constantly outperform state-of-the-art solutions with significant improvements on precision in link prediction, as well as on visualizations and accuracy in clustering tasks.",
        "related_work": "Recent advances in Generative Adversarial Networks (GANs) @cite_15 have proven GANs as a powerful framework for learning complex data distributions. The core idea is to define the generator and the discriminator to be the minimax game players competing with each other to push the generator to produce high quality data to fool the discriminator.",
        "ref_abstract": {
            "@cite_15": {
                "mid": "2099471712",
                "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                "doi": "https://doi.org/10.3156/jsoft.29.5_177_2",
                "title": "GAN\uff08Generative Adversarial Nets\uff09",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1805.07043",
        "mid": "2798590591",
        "abstract": "Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.",
        "related_work": "There is abundant research work on aspect based sentiment analysis. Actually, the name ABSA is used to describe two different subtasks in the literature. We classify the existing work into two main categories based on the descriptions of sentiment analysis tasks in SemEval 2014 Task 4 @cite_31 : Aspect-Term Sentiment Analysis and Aspect-Category Sentiment Analysis.",
        "ref_abstract": {
            "@cite_31": {
                "mid": "2251648804",
                "abstract": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.",
                "doi": "https://doi.org/10.3115/v1/s14-2004",
                "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1805.05744",
        "mid": "2951227051",
        "abstract": "The introduction of the schema.org vocabulary was a big step towards making websites machine read- and understandable. Due to schema.org's RDF-like nature storing annotations in a graph database is easy and efficient. In this paper the authors show how they gather touristic data in the Austrian region of Tirol and provide this data publicly in a knowledge graph. The definition of subsets of the vocabulary is followed by providing means to map data sources efficiently to schema.org and then store the annotated content into the graph. To showcase the consumption of the touristic data four scenarios are described which use the knowledge graph for real life applications and data analysis.",
        "related_work": "The Tyrolean Tourism Knowledge Graph contains static (e.g. phone number, address) and dynamic (e.g. accommodation offers) data based on schema.org annotations collected from different sources such as Destination Management Organizations (DMO) and Geographical Information Systems (GIS). In our previous work @cite_5 , we explained how we annotated the relevant data in the region with schema.org from different sources. Our knowledge graph consolidates these annotations and enables intelligent applications like chatbots to contribute the digitalization of tourism in Tyrol. Additionally, since we store the historical data, the knowledge graph allows data analytics to provide insights from the region.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2754432582",
                "abstract": "The tourism industry has a significant impact on the world\u2019s economy, contributes 10.2 of the world\u2019s gross domestic product in 2016. It becomes a very competitive industry, where having a strong online presence is an essential aspect for business success. To achieve this goal, the proper usage of latest Web technologies, particularly schema.org annotations is crucial. In this paper, we present our effort to improve the online visibility of touristic service providers in the region of Tyrol, Austria, by creating and deploying a substantial amount of semantic annotations according to schema.org, a widely used vocabulary for structured data on the Web. We started our work from Tourismusverband (TVB) Mayrhofen-Hippach and all touristic service providers in the Mayrhofen-Hippach region and applied the same approach to other TVBs and regions, as well as other use cases. The rationale for doing this is straightforward. Having schema.org annotations enables search engines to understand the content better, and provide better results for end users, as well as enables various intelligent applications to utilize them. As a direct consequence, the region of Tyrol and its touristic service increase their online visibility and decrease the dependency on intermediaries, i.e. Online Travel Agency (OTA).",
                "doi": "https://doi.org/10.1007/978-3-319-69459-7_24",
                "title": "Complete Semantics to Empower Touristic Service Providers",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1805.04310",
        "mid": "2949275688",
        "abstract": "Human body part parsing, or human semantic part segmentation, is fundamental to many computer vision tasks. In conventional semantic segmentation methods, the ground truth segmentations are provided, and fully convolutional networks (FCN) are trained in an end-to-end scheme. Although these methods have demonstrated impressive results, their performance highly depends on the quantity and quality of training data. In this paper, we present a novel method to generate synthetic human part segmentation data using easily-obtained human keypoint annotations. Our key idea is to exploit the anatomical similarity among human to transfer the parsing results of a person to another person with similar pose. Using these estimated results as additional training data, our semi-supervised model outperforms its strong-supervised counterpart by 6 mIOU on the PASCAL-Person-Part dataset, and we achieve state-of-the-art human parsing results. Our approach is general and can be readily extended to other object animal parsing task assuming that their anatomical similarity can be annotated by keypoints. The proposed model and accompanying source code are available at this https URL",
        "related_work": "This paper is closely related to the following areas: semantic part segmentation, joint pose and body part estimation, and weakly supervised learning. In this subtask of semantic segmentation, fully convolutional network (FCN) @cite_25 and its variants @cite_4 @cite_0 @cite_24 have demonstrated promising results. In @cite_4 , Chen proposed atrous convolution to capture object features at different scales and they further combined the convolutional neural network (CNN) with a Conditional Random Field (CRF) to improve the accuracy. @cite_0 , the authors proposed an attention mechanism that softly combines the segmentation predictions at different scales according to the context. To tackle the problem of scale and location variance, Xia @cite_24 developed a model that adaptively zoom the input image into the proper scale to refine the parsing results.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2158865742",
                "abstract": "Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation. One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixelwise classification. In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location. We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.",
                "doi": "https://doi.org/10.48550/arxiv.1511.03339",
                "title": "Attention to Scale: Scale-aware Semantic Image Segmentation",
                "publication_year": 2015
            },
            "@cite_4": {
                "mid": "2952865063",
                "abstract": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or 'atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \"DeepLab\" system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.",
                "doi": "https://doi.org/10.48550/arxiv.1606.00915",
                "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,\n  Atrous Convolution, and Fully Connected CRFs",
                "publication_year": 2016
            },
            "@cite_25": {
                "mid": "2952632681",
                "abstract": "Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20 relative improvement to 62.2 mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
                "doi": "https://doi.org/10.48550/arxiv.1605.06211",
                "title": "Fully Convolutional Networks for Semantic Segmentation",
                "publication_year": 2016
            },
            "@cite_24": {
                "mid": "2346977708",
                "abstract": "Parsing articulated objects, e.g. humans and animals, into semantic parts (e.g. body, head and arms, etc.) from natural images is a challenging and fundamental problem for computer vision. A big difficulty is the large variability of scale and location for objects and their corresponding parts. Even limited mistakes in estimating scale and location will degrade the parsing output and cause errors in boundary details. To tackle these difficulties, we propose a \"Hierarchical Auto-Zoom Net\" (HAZN) for object part parsing which adapts to the local scales of objects and parts. HAZN is a sequence of two \"Auto-Zoom Net\" (AZNs), each employing fully convolutional networks that perform two tasks: (1) predict the locations and scales of object instances (the first AZN) or their parts (the second AZN); (2) estimate the part scores for predicted object instance or part regions. Our model can adaptively \"zoom\" (resize) predicted image regions into their proper scales to refine the parsing. We conduct extensive experiments over the PASCAL part datasets on humans, horses, and cows. For humans, our approach significantly outperforms the state-of-the-arts by 5 mIOU and is especially better at segmenting small instances and small parts. We obtain similar improvements for parsing cows and horses over alternative methods. In summary, our strategy of first zooming into objects and then zooming into parts is very effective. It also enables us to process different regions of the image at different scales adaptively so that, for example, we do not need to waste computational resources scaling the entire image.",
                "doi": "https://doi.org/10.48550/arxiv.1511.06881",
                "title": "Zoom Better to See Clearer: Human and Object Parsing with Hierarchical\n  Auto-Zoom Net",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1805.04354",
        "mid": "2799755616",
        "abstract": "We present a novel, reusable and task-agnostic primitive for assessing the outcome of a force-interaction robotic skill, useful e.g. for applications such as quality control in industrial manufacturing. The proposed method is easily programmed by kinesthetic teaching, and the desired adaptability and reusability are achieved by machine learning models. The primitive records sensory data during both demonstrations and reproductions of a movement. Recordings include the end-effector's Cartesian pose and exerted wrench at each time step. The collected data are then used to train Gaussian Processes which create models of the wrench as a function of the robot's pose. The similarity between the wrench models of the demonstration and the movement's reproduction is derived by measuring their Hellinger distance. This comparison creates features that are fed as inputs to a Naive Bayes classifier which estimates the movement's probability of success. The evaluation is performed on two diverse robotic assembly tasks -- snap-fitting and screwing -- with a total of 5 use cases, 11 demonstrations, and more than 200 movement executions. The performance metrics prove the proposed method's capability of generalization to different demonstrations and movements.",
        "related_work": "An alternative solution to assess the task outcome is statistically setting a threshold regarding the wrench signature and the pose trajectories. Costa al @cite_7 verify the success of a process by setting a threshold for eccentricity and typicality, distance metrics for time series. Haidu al @cite_10 define lower and upper bounds of the trajectory profile based on successful trails. Thus, trajectory profiles that exceed the threshold indicates a failure. Nevertheless, the movement reproduction varies for different task parameters (e.g. start and goal state) and demonstrations. In those cases, the bounds have to be changed accordingly which requires reprogramming.",
        "ref_abstract": {
            "@cite_10": {
                "mid": "2219364599",
                "abstract": "Predicting the outcome of an action can help a robot detect failures in advance, and schedule action replanning before an error occurs. We propose using an interactive physics based simulator with the aim of collecting realistic data to be used for learning. We then show how we save and query for specific information from the data more effectively. The data from the simulation is used to learn a failure detection model which is utilized by a real robot performing the same actions. We show that learning from simulation data is realistic enough to be applied on a real robot. The learning algorithm is more simple in design and outperforms the more complex one from our previous work.",
                "doi": "https://doi.org/10.1109/iros.2015.7354136",
                "title": "Learning action failure models from interactive physics-based simulations",
                "publication_year": 2015
            },
            "@cite_7": {
                "mid": "1654125355",
                "abstract": "Fault detection is a task of major importance in industry nowadays, since that it can considerably reduce the risk of accidents involving human lives, in addition to production and, consequently, financial losses. Therefore, fault detection systems have been largely studied in the past few years, resulting in many different methods and approaches to solve such problem. This paper presents a detailed study on fault detection on industrial processes based on the recently introduced eccentricity and typicality data analytics (TEDA) approach. TEDA is a recursive and non-parametric method, firstly proposed to the general problem of anomaly detection on data streams. It is based on the measures of data density and proximity from each read data point to the analyzed data set. TEDA is an online autonomous learning algorithm that does not require a priori knowledge about the process, is completely free of user- and problem-defined parameters, requires very low computational effort and, thus, is very suitable for real-time applications. The results further presented were generated by the application of TEDA to a pilot plant for industrial process.",
                "doi": "https://doi.org/10.1109/ijcnn.2015.7280712",
                "title": "Online fault detection based on Typicality and Eccentricity Data Analytics",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1805.04246",
        "mid": "2800411455",
        "abstract": "Clustering is a fundamental task in data analysis, and spectral clustering has been recognized as a promising approach to it. Given a graph describing the relationship between data, spectral clustering explores the underlying cluster structure in two stages. The first stage embeds the nodes of the graph into real space, and the second stage groups the embedded nodes into several clusters. The use of the @math -means method in the grouping stage is currently standard practice. We present a spectral clustering algorithm that uses convex programming in the grouping stage, and study how well it works. The concept behind the algorithm design lies in the following observation. The nodes with the largest degree in each cluster may be found by computing an enclosing ellipsoid for embedded nodes in real space, and the clusters may be identified by using those nodes. We show that the observations are valid, and the algorithm returns clusters to provide the conductance of graph, if the gap assumption, introduced by Peng el al. at COLT 2015, is satisfied. We also give an experimental assessment of the algorithm's performance.",
        "related_work": "We describe the results of in @cite_20 . Let @math be the clusters of an optimal @math -way partition for the conductance problem. We choose an algorithm for solving the minimization problem of @math shown in ) and suppose that the algorithm has an approximation ratio of @math . Let @math be the output of a spectral clustering algorithm that uses a @math -means method based on the @math -approximation algorithm. They showed the following statement in Theorem 1.2 of @cite_20 . Set @math as and suppose that @math is so large that @math . After suitable renumbering of the output of the algorithm, we have Accordingly, if @math , those can be written as Here, the notation @math denotes the symmetric difference of the sets @math and @math . The results tell us that, if @math is large, the difference between @math and @math is small and the conductance of @math is close to that of @math . also developed a nearly linear time algorithm and examined the performance.",
        "ref_abstract": {
            "@cite_20": {
                "mid": "2117909080",
                "abstract": "In this paper we study variants of the widely used spectral clustering that partitions a graph into @math clusters by (1) embedding the vertices of a graph into a low-dimensional space using the bottom eigenvectors of the Laplacian matrix and (2) grouping the embedded points into @math clusters via @math -means algorithms. We show that, for a wide class of graphs, spectral clustering gives a good approximation of the optimal clustering. While this approach was proposed in the early 1990s and has comprehensive applications, prior to our work similar results were known only for graphs generated from stochastic models. We also give a nearly linear time algorithm for partitioning well-clustered graphs based on computing a matrix exponential and approximate nearest neighbor data structures.",
                "doi": "https://doi.org/10.1137/15m1047209",
                "title": "Partitioning Well-Clustered Graphs: Spectral Clustering Works!",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1805.03812",
        "mid": "2893706439",
        "abstract": "With huge amounts of training data, deep learning has made great breakthroughs in many artificial intelligence (AI) applications. However, such large-scale data sets present computational challenges, requiring training to be distributed on a cluster equipped with accelerators like GPUs. With the fast increase of GPU computing power, the data communications among GPUs have become a potential bottleneck on the overall training performance. In this paper, we first propose a general directed acyclic graph (DAG) model to describe the distributed synchronous stochastic gradient descent (S-SGD) algorithm, which has been widely used in distributed deep learning frameworks. To understand the practical impact of data communications on training performance, we conduct extensive empirical studies on four state-of-the-art distributed deep learning frameworks (i.e., Caffe-MPI, CNTK, MXNet and TensorFlow) over multi-GPU and multi-node environments with different data communication techniques, including PCIe, NVLink, 10GbE, and InfiniBand. Through both analytical and experimental studies, we identify the potential bottlenecks and overheads that could be further optimized. At last, we make the data set of our experimental traces publicly available, which could be used to support simulation-based studies.",
        "related_work": "S-SGD requires the set of computing units (e.g., GPUs) to exchange data iteratively, which can be implemented by either parameter server (PS) based methods @cite_27 @cite_2 or decentralized methods. In PS-based methods, there is one or more PSes that store the global model. The PS aggregates parameters at each iteration, updates the model, and then pushes the updated model to each computing unit. Performance models have been built by S. @cite_30 to generalize the performance of the PS-based methods, which provides guidelines for better system scalability.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "2757938527",
                "abstract": "Scale of data and scale of computation infrastructures together enable the current deep learning renaissance. However, training large-scale deep architectures demands both algorithmic improvement and careful system configuration. In this paper, we focus on employing the system approach to speed up large-scale training. Via lessons learned from our routine benchmarking effort, we first identify bottlenecks and overheads that hinter data parallelism. We then devise guidelines that help practitioners to configure an effective system and fine-tune parameters to achieve desired speedup. Specifically, we develop a procedure for setting minibatch size and choosing computation algorithms. We also derive lemmas for determining the quantity of key components such as the number of GPUs and parameter servers. Experiments and examples show that these guidelines help effectively speed up large-scale deep learning training.",
                "doi": "https://doi.org/10.48550/arxiv.1709.06622",
                "title": "Distributed Training Large-Scale Deep Architectures",
                "publication_year": 2017
            },
            "@cite_27": {
                "mid": "2060393849",
                "abstract": "Big data may contain big values, but also brings lots of challenges to the computing theory, architecture, framework, knowledge discovery algorithms, and domain specific tools and applications. Beyond the 4-V or 5-V characters of big datasets, the data processing shows the features like inexact, incremental, and inductive manner. This brings new research opportunities to research community across theory, systems, algorithms, and applications. Is there some new \"theory\" for the big data? How to handle the data computing algorithms in an operatable manner? This report shares some view on new challenges identified, and covers some of the application scenarios such as micro-blog data analysis and data processing in building next generation search engines.",
                "doi": "https://doi.org/10.1145/2640087.2644155",
                "title": "Scaling Distributed Machine Learning with the Parameter Server",
                "publication_year": 2014
            },
            "@cite_2": {
                "mid": "2339765813",
                "abstract": "Large-scale deep learning requires huge computational resources to train a multi-layer neural network. Recent systems propose using 100s to 1000s of machines to train networks with tens of layers and billions of connections. While the computation involved can be done more efficiently on GPUs than on more traditional CPU cores, training such networks on a single GPU is too slow and training on distributed GPUs can be inefficient, due to data movement overheads, GPU stalls, and limited GPU memory. This paper describes a new parameter server, called GeePS, that supports scalable deep learning across GPUs distributed among multiple machines, overcoming these obstacles. We show that GeePS enables a state-of-the-art single-node GPU implementation to scale well, such as to 13 times the number of training images processed per second on 16 machines (relative to the original optimized single-node code). Moreover, GeePS achieves a higher training throughput with just four GPU machines than that a state-of-the-art CPU-only system achieves with 108 machines.",
                "doi": "https://doi.org/10.1145/2901318.2901323",
                "title": "GeePS",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1804.11001",
        "mid": "2964134356",
        "abstract": "Wireless access points on Unmanned Aerial Vehicles (UAVs) are being considered for mobile service provisioning in commercial networks. These UAV access points will carry radio infrastructure and will be temporarily deployed in areas of dense user traffic to deal with excess user data demand. In this paper, we analyze the coverage when UAVs act as access points for users on the ground in an urban area. We consider the impact of several UAV placement strategies, either independent of, or responsive to, the locations of the users on the ground. Our analysis allows us to demonstrate how the density of the UAVs in the network will determine whether the UAVs should position themselves closer to user hotspots to improve the received signal strength, or further away from one another to mitigate interference. In addition, we demonstrate how network design parameters such as the UAV height above ground or the antenna beamwidth impact the coverage probability. In addition to simulations, we provide mathematical expressions for the coverage probability, for the scenario where UAVs are positioned directly above the centers of user hotspots.",
        "related_work": "We report an emerging trend which can be observed in the works cited above. With the notable exception of @cite_0 , existing state of the art on UAV network optimisation tends to ignore the effects of interference, instead focusing on scenarios where individual UAVs are operating in isolation. Without interference the wireless links are limited by the geometry of the environment, and therefore the network performance is generally optimised through minimising the distance between the UAV and the receiver, as this minimises the pathloss, increases the LOS probability and enables the network to reduce transmit power. These optimisation strategies may not apply to a scenario where multiple UAVs are operating concurrently and creating interference for each other. In the presence of interference, decreasing the distances between transmitters and receivers may also have the result of decreasing the distances between interferers and receivers, potentially causing a net decrease in channel performance.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2518981698",
                "abstract": "UAVs (Unmanned Aerial Vehicles) are candidates for serving as wireless access points in disaster or mass event scenarios. Here we elaborate on the question where to optimally place the available UAVs in space for a Poisson user distribution on the ground. Since the number of possible placement combinations is too high for a systematic search, we approach the optimal solution by a simple heuristic bio-inspired procedure. It turns out that changes in altitude contribute almost as much to increasing the average spectral efficiency as changes in horizontal directions. However, increasing the altitude requires much more energy than an equal movement in a horizontal direction. We show that restricting altitude movements by an attenuation factor can result in similar results as a procedure without such altitude restrictions. Even more intriguingly, keeping all UAVs at one favorable fixed altitude clearly outperforms the procedure with altitude flexibility. This has implications for the time UAVs can stay aloft with their on-board energy.",
                "doi": "https://doi.org/10.1109/wcnc.2016.7565073",
                "title": "Role of altitude when exploring optimal placement of UAV access points",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1804.10255",
        "mid": "2798779428",
        "abstract": "Statistical analysis on object data presents many challenges. Basic summaries such as means and variances are difficult to compute. We apply ideas from topology to study object data. We present a framework for using death vectors and persistence landscapes to vectorize object data and perform statistical analysis. We apply this method to some common leaf images that were previously shown to be challenging to compare using a 3D shape techniques. Surprisingly, the most persistent features are shown to be \u201ctopological noise\u201d and the statistical analysis depends on the less persistent features which we refer to as the \u201cgeometric signal\u201d. We also describe the first steps to a new approach to using topology for object data analysis, which applies topology to distributions on object spaces. We introduce a new Frechet-Morse function technique for probability distribution on a compact object space, extending the Frechet means lo a larger number of location parameters, including Frechet antimeans. An example of 3D data analysis to distinguish two flowers using the new location parameters associated with a Veronese-Whitney (VW) embedding of random projective shapes of 3D configurations extracted from a set of pairs of their digital camera images is also given here.",
        "related_work": "Our approach and results are closely related to work by (2016) @cite_22 , who also applied TDA to object data. In their case they considered brain artery structures extracted from magnetic resonance images and applied persistence homology, which was encoded in vectors by the order statistic on the most persistent points in the persistence diagram. Also, closely related is work by Kovacev- (2016) @cite_28 who applied persistent homology and persistence landscapes to protein structure data.",
        "ref_abstract": {
            "@cite_28": {
                "mid": "2408772186",
                "abstract": "Persistent homology captures the evolution of topological features of a model as a parameter changes. The most commonly used summary statistics of persistent homology are the barcode and the persistence diagram. Another summary statistic, the persistence landscape, was recently introduced by Bubenik. It is a functional summary, so it is easy to calculate sample means and variances, and it is straightforward to construct various test statistics. Implementing a permutation test we detect conformational changes between closed and open forms of the maltose-binding protein, a large biomolecule consisting of 370 amino acid residues. Furthermore, persistence landscapes can be applied to machine learning methods. A hyperplane from a support vector machine shows the clear separation between the closed and open proteins conformations. Moreover, because our approach captures dynamical properties of the protein our results may help in identifying residues susceptible to ligand binding; we show that the majority of active site residues and allosteric pathway residues are located in the vicinity of the most persistent loop in the corresponding filtered Vietoris-Rips complex. This finding was not observed in the classical anisotropic network model.",
                "doi": "https://doi.org/10.1515/sagmb-2015-0057",
                "title": "Using persistent homology and dynamical distances to analyze protein binding",
                "publication_year": 2016
            },
            "@cite_22": {
                "mid": "1753705553",
                "abstract": "New representations of tree-structured data objects, using ideas from topological data analysis, enable improved statistical analyses of a population of brain artery trees. A number of representations of each data tree arise from persistence diagrams that quantify branching and looping of vessels at multiple scales. Novel approaches to the statistical analysis, through various summaries of the persistence diagrams, lead to heightened correlations with covariates such as age and sex, relative to earlier analyses of this data set. The correlation with age continues to be signicant even after controlling for correlations from earlier signicant summaries.",
                "doi": "https://doi.org/10.1214/15-aoas886",
                "title": "Persistent homology analysis of brain artery trees",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1804.08396",
        "mid": "2799024314",
        "abstract": "This paper describes and evaluates the use of Generative Adversarial Networks (GANs) for path planning in support of smart mobility applications such as indoor and outdoor navigation applications, individualized wayfinding for people with disabilities (e.g., vision impairments, physical disabilities, etc.), path planning for evacuations, robotic navigations, and path planning for autonomous vehicles. We propose an architecture based on GANs to recommend accurate and reliable paths for navigation applications. The proposed system can use crowd-sourced data to learn the trajectories and infer new ones. The system provides users with generated paths that help them navigate from their local environment to reach a desired location. As a use case, we experimented with the proposed method in support of a wayfinding application in an indoor environment. Our experiments assert that the generated paths are correct and reliable. The accuracy of the classification task for the generated paths is up to 99 and the quality of the generated paths has a mean opinion score of 89 .",
        "related_work": "Li @cite_25 proposed an indoor navigation system based on off-the-shelf smartphone sensors and magnetic features. They used several approaches to enhance the accuracy of the system including multi-dimensional dynamic time warping, weighted k-nearest neighbors, and exploiting magnetic gradient fingerprints. They also mitigated the impact of magnetic matching mismatches to reduce the position errors. They reported root mean square error between 4.3 m and 5.6 m.",
        "ref_abstract": {
            "@cite_25": {
                "mid": "2506376746",
                "abstract": "This paper presents an algorithm for navigating in challenging indoor environments that do not have WiFi. Dead-reckoning (DR) based on off-the-shelf smartphone sensors and magnetic matching (MM) based on indoor magnetic features are integrated. For DR, we utilize a two-filter algorithm structure and multi-level constraints to navigate under different human motion conditions. For MM, we use several approaches to enhance its performance. These approaches include multi-dimensional dynamic time warping, weighted k -nearest neighbor, and utilization of magnetic gradient fingerprints. Furthermore, realized that the key to enhance the DR MM performance is to mitigate the impact of MM mismatches, we introduced and evaluated two mismatch-detection approaches, including a threshold-based method that sets the measurement noises of MM positions based on their distances to the historical DR MM position solutions, and an adaptive Kalman filter-based method that introduces the estimation of the innovation sequence covariance into the calculation of the gain matrix instead of adjusting the measurement noises. The proposed mismatch-detection mechanism reduced the DR MM errors by 45.9 -67.9 in indoor tests with two smartphones, in two buildings, and under four motion conditions.",
                "doi": "https://doi.org/10.1109/jsen.2016.2591824",
                "title": "Self-Contained Indoor Pedestrian Navigation Using Smartphone Sensors and Magnetic Features",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1804.05468",
        "mid": "2797672404",
        "abstract": "The modularization of Service Function Chains (SFCs) in Network Function Virtualization (NFV) could introduce significant performance overhead and resource efficiency degradation due to introducing frequent packet transfer and consuming much more hardware resources. In response, we exploit the lightweight and individually scalable features of elements in Modularized SFCs (MSFCs) and propose CoCo, a compact and optimized consolidation framework for MSFC in NFV. CoCo addresses the above problems in two ways. First, CoCo Optimized Placer pays attention to the problem of which elements to consolidate and provides a performance-aware placement algorithm to place MSFCs compactly and optimize the global packet transfer cost. Second, CoCo Individual Scaler innovatively introduces a push-aside scaling up strategy to avoid degrading performance and taking up new CPU cores. To support MSFC consolidation, CoCo also provides an automatic runtime scheduler to ensure fairness when elements are consolidated on CPU core. Our evaluation results show that CoCo achieves significant performance improvement and efficient resource utilization.",
        "related_work": "Click @cite_17 proposed the idea of modularization and applies it to routers. Recently, Slick @cite_7 and OpenBox @cite_1 were proposed to detailedly discuss modularized NFs and decouple control plane and data plane of modularized NFs for easy management. Besides, OpenBox focused on merging elements to shorten the processing path length. However, above works mainly focus on orchestration-level module management and are orthogonal to our optimizations on performance-aware placement and dynamically scaling.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2498885363",
                "abstract": "We present OpenBox \u2014 a software-defined framework for network-wide development, deployment, and management of network functions (NFs). OpenBox effectively decouples the control plane of NFs from their data plane, similarly to SDN solutions that only address the network\u2019s forwarding plane. OpenBox consists of three logic components. First, user-defined OpenBox applications provide NF specifications through the OpenBox north-bound API. Second, a logically-centralized OpenBox controller is able to merge logic of multiple NFs, possibly from multiple tenants, and to use a network-wide view to efficiently deploy and scale NFs across the network data plane. Finally, OpenBox instances constitute OpenBox\u2019s data plane and are implemented either purely in software or contain specific hardware accelerators (e.g., a TCAM). In practice, different NFs carry out similar processing steps on the same packet, and our experiments indeed show a significant improvement of the network performance when using OpenBox. Moreover, OpenBox readily supports smart NF placement, NF scaling, and multi-tenancy through its controller.",
                "doi": "https://doi.org/10.1145/2934872.2934875",
                "title": "OpenBox",
                "publication_year": 2016
            },
            "@cite_7": {
                "mid": "2057439722",
                "abstract": "Current approaches to in-network traffic processing involve the deployment of monolithic middleboxes in virtual machines. These approaches make it difficult to reuse functionality across different packet processing elements and also do not use available in-network processing resources efficiently. We present Slick, a framework for programming network functions that allows a programmer to write a single high-level control program that specifies custom packet processing on precise subsets of traffic. The Slick runtime coordinates the placement of fine-grained packet processing elements (e.g., firewalls, load balancers) and steers traffic through sequences of these element instances. A Slick program merely dictates what processing should be performed on specific traffic flows, without requiring the programmer to specify where in the network specific processing elements are instantiated or how traffic should be routed through them. In contrast to previous work, Slick handles both the placement of fine-grained elements and the steering of traffic through specific sequences of element instances, allowing for more efficient use of network resources than solutions that solve each problem in isolation.",
                "doi": "https://doi.org/10.1145/2774993.2774998",
                "title": "Programming slick network functions",
                "publication_year": 2015
            },
            "@cite_17": {
                "mid": "1988150362",
                "abstract": "Large cloud service providers have invested in increasingly larger datacenters to house the computing infrastructure required to support their services. Accordingly, researchers and industry practitioners alike have focused a great deal of effort designing network fabrics to efficiently interconnect and manage the traffic within these datacenters in performant yet efficient fashions. Unfortunately, datacenter operators are generally reticent to share the actual requirements of their applications, making it challenging to evaluate the practicality of any particular design. Moreover, the limited large-scale workload information available in the literature has, for better or worse, heretofore largely been provided by a single datacenter operator whose use cases may not be widespread. In this work, we report upon the network traffic observed in some of Facebook's datacenters. While Facebook operates a number of traditional datacenter services like Hadoop, its core Web service and supporting cache infrastructure exhibit a number of behaviors that contrast with those reported in the literature. We report on the contrasting locality, stability, and predictability of network traffic in Facebook's datacenters, and comment on their implications for network architecture, traffic engineering, and switch design.",
                "doi": "https://doi.org/10.1145/2785956.2787472",
                "title": "Inside the Social Network's (Datacenter) Network",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1804.02617",
        "mid": "2796428665",
        "abstract": "Generative Adversarial Networks (GANs) have been promising in the field of image generation, however, they have been hard to train for language generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them which causes high levels of instability in training GANs. Consequently, past work has resorted to pre-training with maximum-likelihood or training GANs without pre-training with a WGAN objective with a gradient penalty. In this study, we present a comparison of those approaches. Furthermore, we present the results of some experiments that indicate better training and convergence of Wasserstein GANs (WGANs) when a weaker regularization term is enforcing the Lipschitz constraint.",
        "related_work": "Sai et. al @cite_6 have introduced a simple baseline that addresses the discrete output space problem without relying on gradient estimators and shows that it is able to achieve state-of-the-art results on a Chinese poem generation dataset and presented quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.",
        "ref_abstract": {
            "@cite_6": {
                "mid": "2620623908",
                "abstract": "Generative Adversarial Networks (GANs) have gathered a lot of attention from the computer vision community, yielding impressive results for image generation. Advances in the adversarial generation of natural language from noise however are not commensurate with the progress made in generating images, and still lag far behind likelihood based methods. In this paper, we take a step towards generating natural language with a GAN objective alone. We introduce a simple baseline that addresses the discrete output space problem without relying on gradient estimators and show that it is able to achieve state-of-the-art results on a Chinese poem generation dataset. We present quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.",
                "doi": "https://doi.org/10.48550/arxiv.1705.10929",
                "title": "Adversarial Generation of Natural Language",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1804.02135",
        "mid": "2795485067",
        "abstract": "Recent advances in neural autoregressive models have improve the performance of speech synthesis (SS). However, as they lack the ability to model global characteristics of speech (such as speaker individualities or speaking styles), particularly when these characteristics have not been labeled, making neural autoregressive SS systems more expressive is still an open issue. In this paper, we propose to combine VoiceLoop, an autoregressive SS model, with Variational Autoencoder (VAE). This approach, unlike traditional autoregressive SS systems, uses VAE to model the global characteristics explicitly, enabling the expressiveness of the synthesized speech to be controlled in an unsupervised manner. Experiments using the VCTK and Blizzard2012 datasets show the VAE helps VoiceLoop to generate higher quality speech and to control the expressions in its synthesized speech by incorporating global characteristics into the speech generating process.",
        "related_work": "@cite_18 @cite_14 pointed out that UESS can be divided into two parts: predicting expressive information from text; and synthesizing the speech with a particular expression. In this paper only the latter stage is considered for simplicity.",
        "ref_abstract": {
            "@cite_18": {
                "mid": "2156146072",
                "abstract": "Current text-to-speech synthesis (TTS) systems are often perceived as lacking expressiveness, limiting the ability to fully convey information. This paper describes initial investigations into improving expressiveness for statistical speech synthesis systems. Rather than using hand-crafted definitions of expressive classes, an unsupervised clustering approach is described which is scalable to large quantities of training data. To incorporate this \u201cexpression cluster\u201d information into an HMM-TTS system two approaches are described: cluster questions in the decision tree construction; and average expression speech synthesis (AESS) using cluster-based linear transform adaptation. The performance of the approaches was evaluated on audiobook data in which the reader exhibits a wide range of expressiveness. A subjective listening test showed that synthesising with AESS results in speech that better reflects the expressiveness of human speech than a baseline expression-independent system.",
                "doi": "https://doi.org/10.1109/icassp.2012.6288797",
                "title": "Unsupervised clustering of emotion and voice styles for expressive TTS",
                "publication_year": 2012
            },
            "@cite_14": {
                "mid": "2059721297",
                "abstract": "Generating expressive, naturally sounding, speech from text using a speech synthesis (TTS) system is a highly challenging problem. However for tasks such as audiobooks it is essential if their use is to become widespread. Generating expressive speech from text can be divided into two parts: predicting expressive information from text; and synthesizing the speech with a particular expression. Traditionally these components have been studied separately. This paper proposes an integrated approach, where the training data and representation of expressive synthesis is shared across the two components. There are several advantages to this scheme including: robust handling of automatically generated expressive labels; support for a continuous representation of expressions; and joint training of the expression predictor and speech synthesizer. Synthesis experiments indicated that the proposed approach produced far more expressive speech than both a neutral TTS and one where the expression was randomly selected. The experimental results also show the advantage of a continuous expressive synthesis space over a discrete space.",
                "doi": "https://doi.org/10.1109/jstsp.2013.2294938",
                "title": "Integrated Expression Prediction and Speech Synthesis From Text",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1804.00586",
        "mid": "2963140844",
        "abstract": "This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for modeling volumetric shape patterns. The maximum likelihood training of the model follows an \"analysis by synthesis\" scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis.",
        "related_work": "Recently, the vision community has witnessed the success of deep learning, and researchers have used the models in the field of deep learning, such as convolutional deep belief network @cite_20 , deep convolutional neural network @cite_25 , and deep convolutional generative adversarial nets (GAN) @cite_15 , to model 3D objects for the sake of synthesis and analysis. Our proposed 3D model is also powered by the ConvNets. It incorporates a bottom-up 3D ConvNet structure for defining the probability density, and learns the parameters of the ConvNet by an analysis by synthesis'' scheme.",
        "ref_abstract": {
            "@cite_15": {
                "mid": "2546066744",
                "abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",
                "doi": "https://doi.org/10.48550/arxiv.1610.07584",
                "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D\n  Generative-Adversarial Modeling",
                "publication_year": 2016
            },
            "@cite_25": {
                "mid": "2211722331",
                "abstract": "Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.",
                "doi": "https://doi.org/10.1109/iros.2015.7353481",
                "title": "VoxNet: A 3D Convolutional Neural Network for real-time object recognition",
                "publication_year": 2015
            },
            "@cite_20": {
                "mid": "1920022804",
                "abstract": "3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298801",
                "title": "3D ShapeNets: A deep representation for volumetric shapes",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1803.10705",
        "mid": "2964051375",
        "abstract": "Conditional probabilistic graphical models provide a powerful framework for structured regression in spatio-temporal datasets with complex correlation patterns. However, in real-life applications a large fraction of observations is often missing, which can severely limit the representational power of these models. In this paper we propose a Marginalized Gaussian Conditional Random Fields (m-GCRF) structured regression model for dealing with missing labels in partially observed temporal attributed graphs. This method is aimed at learning with both labeled and unlabeled parts and effectively predicting future values in a graph. The method is even capable of learning from nodes for which the response variable is never observed in history, which poses problems for many state-of-the-art models that can handle missing data. The proposed model is characterized for various missingness mechanisms on 500 synthetic graphs. The benefits of the new method are also demonstrated on a challenging application for predicting precipitation based on partial observations of climate variables in a temporal graph that spans the entire continental US. We also show that the method can be useful for optimizing the costs of data collection in climate applications via active reduction of the number of weather stations to consider. In experiments on these real-world and synthetic datasets we show that the proposed model is consistently more accurate than alternative semi-supervised structured models, as well as models that either use imputation to deal with missing values or simply ignore them altogether.",
        "related_work": "One of the standard ways of handling missing values is imputing values based on some predictive model, and then applying the analysis on a fully observed dataset. To exploit the graph structure, previous studies have proposed imputation of missing values based on the exponential random graph model @cite_11 . The limitation of such an approach is that it is slow, as it requires Gibbs sampling, and so it cannot handle large graphs. Imputation of missing values can also be accomplished using matrix (or tensor) factorization methods. These methods can impute missing values with high accuracy even when large percentages (up to 95 , imputation-based methods use only point estimates of the missing values, effectively ignoring the prediction uncertainty when learning with imputed values. Techniques known as Multiple Imputation (MI) try to correct for this drawback, by sampling from the posterior distribution of missing values. On the other hand, these techniques can be less effective when a larger fraction of data is missing @cite_4 , and can be computationally very demanding.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2110275044",
                "abstract": "Background Multiple imputation is becoming increasingly popular for handling missing data. However, it is often implemented without adequate consideration of whether it offers any advantage over complete case analysis for the research question of interest, or whether potential gains may be offset by bias from a poorly fitting imputation model, particularly as the amount of missing data increases.",
                "doi": "https://doi.org/10.1186/1742-7622-9-3",
                "title": "Recovery of information from multiple imputation: a simulation study",
                "publication_year": 2012
            },
            "@cite_11": {
                "mid": "2039839421",
                "abstract": "The predictive analysis of longitudinal social surveys is highly sensitive to the effects of missing data in temporal observations. Such high sensitivity to missing values raises the need for accurate data imputation, because without it a large fraction of collected data could not be used properly. Previous studies focused on the treatment of missing data in longitudinal social networks due to non-respondents and dealt with the problem largely by imputing missing links in isolation or analyzing the imputation effects on network statistics. We propose to account for changing network topology and interdependence between actors' links and attributes to construct a unified approach for imputation of links and attributes in longitudinal social surveys. The new method, based on an exponential random graph model, is evaluated experimentally for five scenarios of missing data models utilizing synthetic and real life datasets with 20 ---60 of nodes missing. The obtained results outperformed all alternatives, four of which were link imputation methods and two node attribute imputation methods. We further discuss the applicability and scalability of our approach to real life problems and compare our model with the latest advancements in the field. Our findings suggest that the proposed method can be used as a viable imputation tool in longitudinal studies.",
                "doi": "https://doi.org/10.1007/s10994-013-5420-1",
                "title": "Imputation of missing links and attributes in longitudinal social surveys",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1803.10687",
        "mid": "2794616326",
        "abstract": "Inferring walls configuration of indoor environment could help robot \"understand\" the environment better. This allows the robot to execute a task that involves inter-room navigation, such as picking an object in the kitchen. In this paper, we present a method to inferring walls configuration from a moving RGB-D sensor. Our goal is to combine a simple wall configuration model and fast wall detection method in order to get a system that works online, is real-time, and does not need a Manhattan World assumption. We tested our preliminary work, i.e. wall detection and measurement from moving RGB-D sensor, with MIT Stata Center Dataset. The performance of our method is reported in terms of accuracy and speed of execution.",
        "related_work": "Within GTSAM framework, @cite_7 uses door signs and walls as landmarks in SLAM. Door-signs are detected by a SVM-based classifier upon Histogram of Oriented Gradient (HOG) features. Walls are extracted from laser data using RANSAC. It is important to note that the SLAM works offline, i.e., it works after all observations data available. This is understandable regarding the SLAM works only with small number of landmarks (i.e., walls and door-signs) which is insufficient to make GTSAM framework works accurately online.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "2102990061",
                "abstract": "Complex and structured landmarks like objects have many advantages over low-level image features for semantic mapping. Low level features such as image corners suffer from occlusion boundaries, ambiguous data association, imaging artifacts, and viewpoint dependance. Artificial landmarks are an unsatisfactory alternative because they must be placed in the environment solely for the robot's benefit. Human environments contain many objects which can serve as suitable landmarks for robot navigation such as signs, objects, and furniture. Maps based on high level features which are identified by a learned classifier could better inform tasks such as semantic mapping and mobile manipulation. In this paper we present a technique for recognizing door signs using a learned classifier as one example of this approach, and demonstrate their use in a graphical SLAM framework with data association provided by reasoning about the semantic meaning of the sign.",
                "doi": "https://doi.org/10.1109/iros.2011.6095152",
                "title": "Simultaneous localization and mapping with learned object recognition and semantic data association",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1803.09403",
        "mid": "2794515184",
        "abstract": "Computer-generated graphics (CGs) are images generated by computer software. The rapid development of computer graphics technologies has made it easier to generate photorealistic computer graphics, and these graphics are quite difficult to distinguish from natural images (NIs) with the naked eye. In this paper, we propose a method based on sensor pattern noise (SPN) and deep learning to distinguish CGs from NIs. Before being fed into our convolutional neural network (CNN)-based model, these images\u2014CGs and NIs\u2014are clipped into image patches. Furthermore, three high-pass filters (HPFs) are used to remove low-frequency signals, which represent the image content. These filters are also used to reveal the residual signal as well as SPN introduced by the digital camera device. Different from the traditional methods of distinguishing CGs from NIs, the proposed method utilizes a five-layer CNN to classify the input image patches. Based on the classification results of the image patches, we deploy a majority vote scheme to obtain the classification results for the full-size images. The experiments have demonstrated that (1) the proposed method with three HPFs can achieve better results than that with only one HPF or no HPF and that (2) the proposed method with three HPFs achieves 100 accuracy, although the NIs undergo a JPEG compression with a quality factor of 75.",
        "related_work": "Gando et al @cite_18 presented a deep learning method based on a fine-tuned deep convolutional neural network. This method can automatically distinguish illustrations from photographs and achieve 96.8 Rahmouni et al @cite_20 presented a custom pooling layer to extract statistical features and a CNN framework to distinguish computer-generated graphics from real photographic images. A weighted voting scheme was used to aggregate the local estimates of class probabilities and predict the label of the whole picture. The best accuracy in @cite_20 is 93.2",
        "ref_abstract": {
            "@cite_18": {
                "mid": "2509155366",
                "abstract": "Automatically detecting illustrations is needed for the target system.Deep Convolutional Neural Networks have been successful in computer vision tasks.DCNN with fine-tuning outperformed the other models including handcrafted features. Systems for aggregating illustrations require a function for automatically distinguishing illustrations from photographs as they crawl the network to collect images. A previous attempt to implement this functionality by designing basic features that were deemed useful for classification achieved an accuracy of only about 58 . On the other hand, deep neural networks had been successful in computer vision tasks, and convolutional neural networks (CNNs) had performed good at extracting such useful image features automatically. We evaluated alternative methods to implement this classification functionality with focus on deep neural networks. As the result of experiments, the method that fine-tuned deep convolutional neural network (DCNN) acquired 96.8 accuracy, outperforming the other models including the custom CNN models that were trained from scratch. We conclude that DCNN with fine-tuning is the best method for implementing a function for automatically distinguishing illustrations from photographs.",
                "doi": "https://doi.org/10.1016/j.eswa.2016.08.057",
                "title": "Fine-tuning deep convolutional neural networks for distinguishing illustrations from photographs",
                "publication_year": 2016
            },
            "@cite_20": {
                "mid": "2786289897",
                "abstract": "This paper presents a deep-learning method for distinguishing computer generated graphics from real photographic images. The proposed method uses a Convolutional Neural Network (CNN) with a custom pooling layer to optimize current best-performing algorithms feature extraction scheme. Local estimates of class probabilities are computed and aggregated to predict the label of the whole picture. We evaluate our work on recent photo-realistic computer graphics and show that it outperforms state of the art methods for both local and full image classification.",
                "doi": "https://doi.org/10.1109/wifs.2017.8267647",
                "title": "Distinguishing computer graphics from natural images using convolution neural networks",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1803.06924",
        "mid": "2793803704",
        "abstract": "Infrastructure as a service clouds hide the complexity of maintaining the physical infrastructure with a slight disadvantage: they also hide their internal working details. Should users need knowledge about these details e.g., to increase the reliability or performance of their applications, they would need solutions to detect behavioural changes in the underlying system. Existing runtime solutions for such purposes offer limited capabilities as they are mostly restricted to revealing weekly or yearly behavioural periodicity in the infrastructure. This article proposes a technique for predicting generic background workload by means of simulations that are capable of providing additional knowledge of the underlying private cloud systems in order to support activities like cloud orchestration or workflow enactment. Our technique uses long-running scientific workflows and their behaviour discrepancies and tries to replicate these in a simulated cloud with known (trace-based) workloads. We argue that the better we can mimic the current discrepancies the better we can tell expected workloads in the near future on the real life cloud. We evaluated the proposed prediction approach with a biochemical application on both real and simulated cloud infrastructures. The proposed algorithm has shown to produce significantly ( 20 ) better workload predictions for the future of simulated clouds than random workload selection.",
        "related_work": "Concerning workload modelling, @cite_0 used data traces obtained from a data centre to characterise and predict workload on VMs. Their goal was to explore cross-VM workload correlations, and predict workload changes due to dependencies among applications running in different VMs -- while we approach the load prediction from the workflow enactment point of view.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2143039774",
                "abstract": "Cloud computing promises high scalability, flexibility and cost-effectiveness to satisfy emerging computing requirements. To efficiently provision computing resources in the cloud, system administrators need the capabilities of characterizing and predicting workload on the Virtual Machines (VMs). In this paper, we use data traces obtained from a real data center to develop such capabilities. First, we search for repeatable workload patterns by exploring cross-VM workload correlations resulted from the dependencies among applications running on different VMs. Treating workload data samples as time series, we develop a co-clustering technique to identify groups of VMs that frequently exhibit correlated workload patterns, and also the time periods in which these VM groups are active. Then, we introduce a method based on Hidden Markov Modeling (HMM) to characterize the temporal correlations in the discovered VM clusters and to predict variations of workload patterns. The experimental results show that our method can not only help better understand group-level workload characteristics, but also make more accurate predictions on workload changes in a cloud.",
                "doi": "https://doi.org/10.1109/noms.2012.6212065",
                "title": "Workload characterization and prediction in the cloud: A multiple time series approach",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1803.06294",
        "mid": "2792862287",
        "abstract": "The advent of SDN has brought a plethora of new architectures and controller designs for many use-cases and scenarios. Existing SDN deployments focus on campus, datacenter and WAN networks. However, little research efforts have been devoted to the scenario of effectively controlling a full deployment of end-nodes (e.g. smartphones) that are transient and scattered across the Internet. In this paper, we present a rigorous analysis of the challenges associated with an SDN architecture for end-nodes, show that such challenges are not found in existing SDN scenarios, and provide practical design guidelines to address them. Then, and following these guidelines we present a reference architecture based on a decentralized, distributed and symmetric controller with a connectionless pull-oriented southbound and an intent-driven northbound. Finally, we measure a proof-of-concept deployment to assess the validity of the analysis as well as the architecture.",
        "related_work": "There have been other works that discussed the interactions of end-nodes and SDN. Nevertheless, and to the best of our knowledge, this paper is the first ever to carefully explore all implications of a full SDN deployment of end-nodes. For instance, the authors of @cite_14 enable SDN within a mobile device to aggregate all its available interfaces. However, and contrary to our work, they do not consider that end-nodes can be transient, scattered, with low traffic locality and very numerous. Similarly, the authors of @cite_7 extend OpenFlow to bring SDN to end-nodes and complement existing approaches in the field of Software-defined Radio. They -intentionally- kept out the scope the complete architecture to support SDN-aware end-nodes. Similarly, meSDN @cite_6 proposes a mobile extension for SDN to optimize wireless channel transmission on an existing SDN network. However, it does not consider devices connecting to legacy -non SDN- networks or transient devices that roam frequently.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2122711168",
                "abstract": "Poor connectivity is common when we use wireless networks on the go. A natural way to tackle the problem is to take advantage of the multiple network interfaces on our mobile devices, and use all the networks around us. Using multiple networks at a time makes makes possible faster connections, seamless connectivity and potentially lower usage charges. The goal of this paper is to explore how to make use of all the networks with today's technology. Specifically, we prototyped a solution on an Android phone. Using our prototype, we demonstrate the benefits (and difficulties) of using multiple networks at the same time.",
                "doi": "https://doi.org/10.1145/2342468.2342474",
                "title": "Making use of all the networks around us",
                "publication_year": 2012
            },
            "@cite_6": {
                "mid": "2096110307",
                "abstract": "Mobile devices interact wirelessly with a growing proliferation of cloud-based applications. Due to significant traffic growth and a wide variety of multimedia solutions, enterprise IT departments are demanding more fine-grained visibility and control of mobile traffic. They want to deliver optimal performance and a high quality of experience to a variety of users and applications. In the wired world, Software-Defined Networking (SDN) is a technology being embraced to deliver performance guarantees to end users by dynamically orchestrating quality of service (QoS) policies on edge switches and routers. Guaranteeing performance in a wired access network does not require any network control on clients, because the last hop between the network edge and wired device is a dedicated point-to-point link (e.g. Ethernet). However, this is not the case with wireless LANs (WLAN), since the last hop is a shared half-duplex medium and the WiFi MAC protocol does not allow access points to coordinate client uplink transmissions or 802.11 QoS settings. Hence, we argue that the SDN paradigm needs to be extended to mobile clients to provide optimal network performance between the cloud and wirelessly-connected clients. In this paper, we propose a framework called meSDN and demonstrate that it enables WLAN virtualization, application-aware QoS and improves power-efficiency from our prototype on Android phones.",
                "doi": "https://doi.org/10.1145/2609908.2609948",
                "title": "meSDN",
                "publication_year": 2014
            },
            "@cite_7": {
                "mid": "2292061281",
                "abstract": "The connectivity capabilities of mobile wireless devices have been forever changing how networks operate, increasingly demanding resources from the network. This places a need for novel mobile network architectures and mechanisms, targeting tomorrows challenges, as envisaged by 5G networks research efforts. This paper sheds a light on an important 5G aspect, namely the interaction of the mobile node with the network for the optimization of mobility processes, defining a concept framework where Software Defined Networking (SDN) reaches the end device. In this way, SDN protocols such as OpenFlow are extended with mobility management capabilities, where the mobile terminal can provide details about perceived connectivity targets and conditions, and the controller is able to enforce necessary changes to data flows all the way to the terminal node. This framework was experimentally evaluated in a physical wireless testbed, with results showcasing its feasibility in a mobile offloading scenario.",
                "doi": "https://doi.org/10.1109/glocomw.2015.7414073",
                "title": "Extending SDN to End Nodes Towards Heterogeneous Wireless Mobility",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1803.05846",
        "mid": "2794091644",
        "abstract": "Meaningful facial parts can convey key cues for both facial action unit detection and expression prediction. Textured 3D face scan can provide both detailed 3D geometric shape and 2D texture appearance cues of the face which are beneficial for Facial Expression Recognition (FER). However, accurate facial parts extraction as well as their fusion are challenging tasks. In this paper, a novel system for 3D FER is designed based on accurate facial parts extraction and deep feature fusion of facial parts. In particular, each textured 3D face scan is firstly represented as a 2D texture map and a depth map with one-to-one dense correspondence. Then, the facial parts of both texture map and depth map are extracted using a novel 4-stage process consists of facial landmark localization, facial rotation correction, facial resizing, facial parts bounding box extraction and post-processing procedures. Finally, deep fusion Convolutional Neural Networks (CNNs) features of all facial parts are learned from both texture maps and depth maps, respectively and nonlinear SVMs are used for expression prediction. Experiments are conducted on the BU-3DFE database, demonstrating the effectiveness of combing different facial parts, texture and depth cues and reporting the state-of-the-art results in comparison with all existing methods under the same setting.",
        "related_work": "Having small patches as proposed by @cite_15 lacks the ability to automatically localize and determine what part is in effect for an expression. It can also have trouble distinguishing patches when applied on faces of different gender and ethnicity. Having a learned detector for the face and parts as proposed by @cite_32 cannot guarantee the robustness as much as face detection, alignment and localization techniques do, especially when there are peculiar samples provided. Both proposals used the JAFFE and CK+ database, which are not very diverse when it comes to their subjects ethnicities. In their experiments, they adopt a cross-validation approach, in which they do not ensure a subject independent protocol. This can result in a high performance which can be inconsistent when a new subject is ever tested.",
        "ref_abstract": {
            "@cite_15": {
                "mid": "2065379720",
                "abstract": "In this paper, we present a new idea to analyze facial expression by exploring some common and specific information among different expressions. Inspired by the observation that only a few facial parts are active in expression disclosure (e.g., around mouth, eye), we try to discover the common and specific patches which are important to discriminate all the expressions and only a particular expression, respectively. A two-stage multitask sparse learning (MTSL) framework is proposed to efficiently locate those discriminative patches. In the first stage MTSL, expression recognition tasks are combined to located common patches. Each of the tasks aims to find dominant patches for each expression. Secondly, two related tasks, facial expression recognition and face verification tasks, are coupled to learn specific facial patches for individual expression. The two-stage patch learning is performed on patches sampled by multiscale strategy. Extensive experiments validate the existence and significance of common and specific patches. Utilizing these learned patches, we achieve superior performances on expression recognition compared to the state-of-the-arts.",
                "doi": "https://doi.org/10.1109/tcyb.2014.2354351",
                "title": "Learning Multiscale Active Facial Patches for Expression Analysis",
                "publication_year": 2015
            },
            "@cite_32": {
                "mid": "2971794874",
                "abstract": "This paper mainly studies facial expression recognition with the components by face parsing (FP). Considering the disadvantage that different parts of face contain different amount of information for facial expression and the weighted function are not the same for different faces, an idea is proposed to recognize facial expression using components which are active in expression disclosure. The face parsing detectors are trained via deep belief network and tuned by logistic regression. The detectors first detect face, and then detect nose, eyes and mouth hierarchically. A deep architecture pretrained with stacked autoencoder is applied to facial expression recognition with the concentrated features of detected components. The parsing components remove the redundant information in expression recognition, and images don't need to be aligned or any other artificial treatment. Experimental results on the Japanese Female Facial Expression database and extended Cohn-Kanade dataset outperform other methods and show the effectiveness and robustness of this algorithm.",
                "doi": "https://doi.org/10.1109/smartcomp.2014.7043872",
                "title": "Facial expression recognition via deep learning",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1802.10171",
        "mid": "2963606198",
        "abstract": "Weakly supervised learning with only coarse labels can obtain visual explanations of deep neural network such as attention maps by back-propagating gradients. These attention maps are then available as priors for tasks such as object localization and semantic segmentation. In one common framework we address three shortcomings of previous approaches in modeling such attention maps: We (1) make attention maps an explicit and natural component of the end-to-end training for the first time, (2) provide self-guidance directly on these maps by exploring supervision from the network itself to improve them, and (3) seamlessly bridge the gap between using weak and extra supervision if available. Despite its simplicity, experiments on the semantic segmentation task demonstrate the effectiveness of our methods. We clearly surpass the state-of-the-art on PASCAL VOC 2012 test and val. sets. Besides, the proposed framework provides a way not only explaining the focus of the learner but also feeding back with direct guidance towards specific tasks. Under mild assumptions our method can also be understood as a plug-in to existing weakly supervised learners to improve their generalization performance.",
        "related_work": "Identifying bias in datasets @cite_21 is another important usage of the network attention. @cite_5 analyses the location of attention maps of a trained model to find out the dataset bias, which helps them to build a better unbiased dataset. However, in practical applications, it is hard remove all the bias of the dataset and time-consuming to build a new dataset. How to garantee the generalization ability of the learned network is still challenging. Different from the existing methods, our model can fundamentally solve this problem by providing supervision directly on network's attention and guiding the network to focus on the areas critical to the task of interest, therefore is robust to dataset bias.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2962858109",
                "abstract": "We propose a technique for producing \u2018visual explanations\u2019 for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach \u2013 Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for \u2018dog\u2019 or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both make identical predictions. Our code is available at https: github.com ramprs grad-cam along with a demo on CloudCV [2] and video at youtu.be COjUB9Izk6E.",
                "doi": "https://doi.org/10.1109/iccv.2017.74",
                "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization",
                "publication_year": 2017
            },
            "@cite_21": {
                "mid": "2031342017",
                "abstract": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.",
                "doi": "https://doi.org/10.1109/cvpr.2011.5995347",
                "title": "Unbiased look at dataset bias",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1802.05629",
        "mid": "2743557796",
        "abstract": "This paper introduces a new family of models of intensional Martin-L \"of type theory. We use constructive ordered algebra in toposes. Identity types in the models are given by a notion of Moore path. By considering a particular gros topos, we show that there is such a model that is non-truncated, i.e. contains non-trivial structure at all dimensions. In other words, in this model a type in a nested sequence of identity types can contain more than one element, no matter how great the degree of nesting. Although inspired by existing non-truncated models of type theory based on simplicial and on cubical sets, the notion of model presented here is notable for avoiding any form of Kan filling condition in the semantics of types.",
        "related_work": "Although our use of constructive algebra within toposes to make models of intensional type theory appears to be new, we are not the only ones to consider using some form of path with strictly unitary and associative composition to model identity types with a judgemental computation rule. Van den Berg and Garner @cite_7 use topological Moore paths and a simplicial version of them to get instances of their notion of for modelling identity types. The results of Sections and show that any ordered abelian group in a topos induces a path object category structure on that topos; and since the notion of fibration we use (Definition ) is closely related to the one used in @cite_7 (see Proposition 6.1.5 of that paper), one can get alternative, more abstract categorical proofs of Theorems and from the work of Van den Berg and Garner. However, the concrete calculations in the internal language that we give are quite simple by comparison; and this approach proves its worth in , whose results on obtaining function extensionality from the ordered ring structure are new.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "1997582403",
                "abstract": "In this paper we construct new categorical models for the identity types of Martin-Lof type theory, in the categories Top of topological spaces and SSet of simplicial sets. We do so building on earlier work of Awodey and Warren [2009], which has suggested that a suitable environment for the interpretation of identity types should be a category equipped with a weak factorization system in the sense of Bousfield--Quillen. It turns out that this is not quite enough for a sound model, due to some subtle coherence issues concerned with stability under substitution; and so our first task is to introduce a slightly richer structure, which we call a homotopy-theoretic model of identity types, and to prove that this is sufficient for a sound interpretation. Now, although both Top and SSet are categories endowed with a weak factorization system---and indeed, an entire Quillen model structure---exhibiting the additional structure required for a homotopy-theoretic model is quite hard to do. However, the categories we are interested in share a number of common features, and abstracting these leads us to introduce the notion of a path object category. This is a relatively simple axiomatic framework, which is nonetheless sufficiently strong to allow the construction of homotopy-theoretic models. Now by exhibiting suitable path object structures on Top and SSet, we endow those categories with the structure of a homotopy-theoretic model and, in this way, obtain the desired topological and simplicial models of identity types.",
                "doi": "https://doi.org/10.1145/2071368.2071371",
                "title": "Topological and Simplicial Models of Identity Types",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1802.03701",
        "mid": "2787105875",
        "abstract": "Ontology learning (OL) is the process of automatically generating an ontological knowledge base from a plain text document. In this paper, we propose a new ontology learning approach and tool, called DLOL, which generates a knowledge base in the description logic (DL) SHOQ(D) from a collection of factual non-negative IS-A sentences in English. We provide extensive experimental results on the accuracy of DLOL, giving experimental comparisons to three state-of-the-art existing OL tools, namely Text2Onto, FRED, and LExO. Here, we use the standard OL accuracy measure, called lexical accuracy, and a novel OL accuracy measure, called instance-based inference model. In our experimental results, DLOL turns out to be about 21 and 46 , respectively, better than the best of the other three approaches.",
        "related_work": "There has been significant literature over the last decade on the problem of Ontology Learning (OL). Most of these works can be categorized into two approaches as discussed earlier: (i) , and (ii) . Light-weight OL from text documents is arguably the most widely used approach in the field of OL @cite_35 . It can be further divided into three general approaches: (i) , (ii) , and (iii) . It is to be noted that the general disadvantage of light-weight OL of not being able to generate definitional T-Box (and corresponding A-Box), as discussed in the introduction section, is inherent in all the three approaches. This is where @math parts away significantly from light-weight OL approaches. We first discuss light-weight OL in the following three sub-sections for providing a contrasting perspective, and then conclude the section with a discussion on formal OL.",
        "ref_abstract": {
            "@cite_35": {
                "mid": "2124436241",
                "abstract": "Ontologies are often viewed as the answer to the need for interoperable semantics in modern information systems. The explosion of textual information on the Read Write Web coupled with the increasing demand for ontologies to power the Semantic Web have made (semi-)automatic ontology learning from text a very promising research area. This together with the advanced state in related areas, such as natural language processing, have fueled research into ontology learning over the past decade. This survey looks at how far we have come since the turn of the millennium and discusses the remaining challenges that will define the research directions in this area in the near future.",
                "doi": "https://doi.org/10.1145/2333112.2333115",
                "title": "Ontology learning from text",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1802.02974",
        "mid": "2787428865",
        "abstract": "Scores of compilers produce JavaScript, enabling programmers to use many languages on the Web, reuse existing code, and even use Web IDEs. Unfortunately, most compilers expose the browser's compromised execution model, so long-running programs freeze the browser tab, infinite loops crash IDEs, and so on. The few compilers that avoid these problems suffer poor performance and are difficult to engineer. This paper presents Stopify, a source-to-source compiler that extends JavaScript with debugging abstractions and blocking operations, and easily integrates with existing compilers. We apply Stopify to 10 programming languages and develop a Web IDE that supports stopping, single-stepping, breakpointing, and long-running computations. For nine languages, Stopify requires no or trivial compiler changes. For eight, our IDE is the first that provides these features. Two of our subject languages have compilers with similar features. Stopify's performance is competitive with these compilers and it makes them dramatically simpler. Stopify's abstractions rely on first-class continuations, which it provides by compiling JavaScript to JavaScript. We also identify sub-languages of JavaScript that compilers implicitly use, and exploit these to improve performance. Finally, Stopify needs to repeatedly interrupt and resume program execution. We use a sampling-based technique to estimate program speed that outperforms other systems.",
        "related_work": "Doppio @cite_41 and Whalesong @cite_46 implement bytecode interpreters in the browser that do not use the JavaScript stack. Therefore, they can suspend and resume execution. However, since these are bytecode interpreters for other platforms (JVM and Racket, respectively), existing compilers and libraries would have to change significantly to use them. Browsix @cite_15 acts as an operating system'' for processes in Web Workers. Therefore, it inherits Web Workers' restrictions: workers cannot share JavaScript values and cannot interact with the Web page. It also does not provide deep stacks. allows code to run in the main browser thread, enabling access to the DOM and allowing execution control for IDEs.",
        "ref_abstract": {
            "@cite_41": {
                "mid": "2065659882",
                "abstract": "Web browsers have become a de facto universal operating system, and JavaScript its instruction set. Unfortunately, running other languages in the browser is not generally possible. Translation to JavaScript is not enough because browsers are a hostile environment for other languages. Previous approaches are either non-portable or require extensive modifications for programs to work in a browser. This paper presents Doppio, a JavaScript-based runtime system that makes it possible to run unaltered applications written in general-purpose languages directly inside the browser. Doppio provides a wide range of runtime services, including a file system that enables local and external (cloud-based) storage, an unmanaged heap, sockets, blocking I O, and multiple threads. We demonstrate DOPPIO's usefulness with two case studies: we extend Emscripten with Doppio, letting it run an unmodified C++ application in the browser with full functionality, and present DoppioJVM, an interpreter that runs unmodified JVM programs directly in the browser. While substantially slower than a native JVM (between 24X and 42X slower on CPU-intensive benchmarks in Google Chrome), DoppioJVM makes it feasible to directly reuse existing, non compute-intensive code.",
                "doi": "https://doi.org/10.1145/2594291.2594293",
                "title": "Doppio",
                "publication_year": 2014
            },
            "@cite_46": {
                "mid": "2125897312",
                "abstract": "JavaScript is the language of the ubiquitous Web, but it only poorly supports event-driven functional programs due to its single-threaded, asynchronous nature and lack of rich control flow operators. We present Whalesong, a compiler from Racket that generates JavaScript code that masks these problems. We discuss the implementation strategy using delimited continuations, an interface to the DOM, and an FFI for adapting JavaScript libraries to add new platform-dependent reactive features. In the process, we also describe extensions to Racket's functional event-driven programming model. We also briefly discuss the implementation details.",
                "doi": "https://doi.org/10.1145/2508168.2508172",
                "title": "Whalesong",
                "publication_year": 2013
            },
            "@cite_15": {
                "mid": "2553125262",
                "abstract": "Applications written to run on conventional operating systems typically depend on OS abstractions like processes, pipes, signals, sockets, and a shared file system. Porting these applications to the web currently requires extensive rewriting or hosting significant portions of code server-side because browsers present a nontraditional runtime environment that lacks OS functionality. This paper presents Browsix, a framework that bridges the considerable gap between conventional operating systems and the browser, enabling unmodified programs expecting a Unix-like environment to run directly in the browser. Browsix comprises two core parts: (1) a JavaScript-only system that makes core Unix features (including pipes, concurrent processes, signals, sockets, and a shared file system) available to web applications; and (2) extended JavaScript runtimes for C, C++, Go, and Node.js that support running programs written in these languages as processes in the browser. Browsix supports running a POSIX shell, making it straightforward to connect applications together via pipes. We illustrate Browsix's capabilities via case studies that demonstrate how it eases porting legacy applications to the browser and enables new functionality. We demonstrate a Browsix-enabled LaTeX editor that operates by executing unmodified versions of pdfLaTeX and BibTeX. This browser-only LaTeX editor can render documents in seconds, making it fast enough to be practical. We further demonstrate how Browsix lets us port a client-server application to run entirely in the browser for disconnected operation. Creating these applications required less than 50 lines of glue code and no code modifications, demonstrating how easily Browsix can be used to build sophisticated web applications from existing parts without modification.",
                "doi": "https://doi.org/10.1145/3037697.3037727",
                "title": "Browsix",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1802.01636",
        "mid": "2591681106",
        "abstract": "With progress in enabling autonomous cars to drive safely on the road, it is time to start asking how they should be driving. A common answer is that they should be adopting their users' driving style. This makes the assumption that users want their autonomous cars to drive like they drive - aggressive drivers want aggressive cars, defensive drivers want defensive cars. In this paper, we put that assumption to the test. We find that users tend to prefer a significantly more defensive driving style than their own. Interestingly, they prefer the style they think is their own, even though their actual driving style tends to be more aggressive. We also find that preferences do depend on the specific driving scenario, opening the door for new ways of learning driving style preference.",
        "related_work": "The typical behavioral patterns of a driver are usually referred to by the term . This includes the choice of driving speed, headway, overtaking of other vehicles, or the tendency to commit traffic violations @cite_16 .",
        "ref_abstract": {
            "@cite_16": {
                "mid": "2045956037",
                "abstract": "The aim of this study was to validate the stability of the different factors of the Multidimensional Driving Style Inventory (MDSI) [16], which was originally developed and validated with participants in different geographical areas of Israel. In this study, the questionnaire was distributed in the Netherlands and Belgium. A factor analysis of the data of 364 participants revealed five of the eight factors that resulted from the original factor analysis: Angry driving, Anxious driving, Dissociative driving, Distress-reduction driving, and Careful driving style. In addition, 24 items divided over the five factors seem to be stable compared to the 44 items divided over the eight factors of the original analysis. The factors revealed through the analysis of these data were used to determine driver profiles, consisting of one or two driving styles. The next step is to compare self-report data on driving style to actual driving behaviour.",
                "doi": "https://doi.org/10.1145/2799250.2799266",
                "title": "Measuring driving styles",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1802.01336",
        "mid": "2964083189",
        "abstract": "We present a framework in Isabelle for verifying asymptotic time complexity of imperative programs. We build upon an extension of Imperative HOL and its separation logic to include running time. Our framework is able to handle advanced techniques for time complexity analysis, such as the use of the Akra\u2013Bazzi theorem and amortized analysis. Various automation is built and incorporated into the auto2 prover to reason about separation logic with time credits, and to derive asymptotic behaviour of functions. As case studies, we verify the asymptotic time complexity (in addition to functional correctness) of imperative algorithms and data structures such as median of medians selection, Karatsuba\u2019s algorithm, and splay trees.",
        "related_work": "They present several examples including binary search, the Bellman--Ford algorithm and union-find. We provide several advanced examples including those involving the Akra--Bazzi method. We also demonstrate that verification of amortized analysis of functional programs @cite_8 can be converted to verification of imperative programs with little additional effort.",
        "ref_abstract": {
            "@cite_8": {
                "mid": "2406041226",
                "abstract": "A framework for the analysis of the amortized complexity of functional data structures is formalized in the proof assistant Isabelle HOL and applied to a number of standard examples and to the following non-trivial ones: skew heaps, splay trees, splay heaps and pairing heaps. The proofs are completely algebraic and are presented in some detail.",
                "doi": "https://doi.org/10.1007/978-3-319-22102-1_21",
                "title": "Amortized Complexity Verified",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1802.01129",
        "mid": "2786092739",
        "abstract": "In this paper, we propose a simple and effective geometric model fitting method to fit and segment multi-structure data even in the presence of severe outliers. We cast the task of geometric model fitting as a representative mode-seeking problem on hypergraphs. Specifically, a hypergraph is first constructed, where the vertices represent model hypotheses and the hyperedges denote data points. The hypergraph involves higher-order similarities (instead of pairwise similarities used on a simple graph), and it can characterize complex relationships between model hypotheses and data points. In addition, we develop a hypergraph reduction technique to remove \u201cinsignificant\u201d vertices while retaining as many \u201csignificant\u201d vertices as possible in the hypergraph. Based on the simplified hypergraph, we then propose a novel mode-seeking algorithm to search for representative modes within reasonable time. Finally, the proposed mode-seeking algorithm detects modes according to two key elements, i.e., the weighting scores of vertices and the similarity analysis between vertices. Overall, the proposed fitting method is able to efficiently and effectively estimate the number and the parameters of model instances in the data simultaneously. Experimental results demonstrate that the proposed method achieves significant superiority over several state-of-the-art model fitting methods on both synthetic data and real images.",
        "related_work": "Recently, some hypergraph based methods, e.g., @cite_15 @cite_6 @cite_32 @cite_29 , have been proposed for robust model fitting due to its effectiveness . For example, Liu and Yan @cite_6 proposed the random consensus graph (RCG) to fit multiple structures in data. @cite_29 proposed to use large hyperedges for face clustering and motion segmentation.",
        "ref_abstract": {
            "@cite_15": {
                "mid": "2158340552",
                "abstract": "The higher-order clustering problem arises when data is drawn from multiple subspaces or when observations fit a higher-order parametric model. Most solutions to this problem either decompose higher-order similarity measures for use in spectral clustering or explicitly use low-rank matrix representations. In this paper we present our approach of Sparse Grassmann Clustering (SGC) that combines attributes of both categories. While we decompose the higher order similarity tensor, we cluster data by directly finding a low dimensional representation without explicitly building a similarity matrix. By exploiting recent advances in online estimation on the Grassmann manifold (GROUSE) we develop an efficient and accurate algorithm that works with individual columns of similarities or partial observations thereof. Since it avoids the storage and decomposition of large similarity matrices, our method is efficient, scalable and has low memory requirements even for large-scale data. We demonstrate the performance of our SGC method on a variety of segmentation problems including planar segmentation of Kinect depth maps and motion segmentation of the Hopkins 155 dataset for which we achieve performance comparable to the state-of-the-art.",
                "doi": "https://doi.org/10.1109/iccv.2013.436",
                "title": "Efficient Higher-Order Clustering on the Grassmann Manifold",
                "publication_year": 2013
            },
            "@cite_29": {
                "mid": "152560764",
                "abstract": "The extension of conventional clustering to hypergraph clustering, which involves higher order similarities instead of pairwise similarities, is increasingly gaining attention in computer vision. This is due to the fact that many grouping problems require an affinity measure that must involve a subset of data of size more than two, i.e., a hyperedge. Almost all previous works, however, have considered the smallest possible hyperedge size, due to a lack of study into the potential benefits of large hyperedges and effective algorithms to generate them. In this paper, we show that large hyperedges are better from both theoretical and empirical standpoints. We then propose a novel guided sampling strategy for large hyperedges, based on the concept of random cluster models. Our method can generate pure large hyperedges that significantly improve grouping accuracy without exponential increases in sampling costs. In the important applications of face clustering and motion segmentation, our method demonstrates substantially better accuracy and efficiency.",
                "doi": "https://doi.org/10.1007/978-3-319-10593-2_44",
                "title": "Clustering with Hypergraphs: The Case for Large Hyperedges",
                "publication_year": 2014
            },
            "@cite_32": {
                "mid": "2012184117",
                "abstract": "Motion segmentation based on point trajectories can integrate information of a whole video shot to detect and separate moving objects. Commonly, similarities are defined between pairs of trajectories. However, pairwise similarities restrict the motion model to translations. Non-translational motion, such as rotation or scaling, is penalized in such an approach. We propose to define similarities on higher order tuples rather than pairs, which leads to hypergraphs. To apply spectral clustering, the hypergraph is transferred to an ordinary graph, an operation that can be interpreted as a projection. We propose a specific nonlinear projection via a regularized maximum operator, and show that it yields significant improvements both compared to pairwise similarities and alternative hypergraph projections.",
                "doi": "https://doi.org/10.1109/cvpr.2012.6247728",
                "title": "Higher order motion models and spectral clustering",
                "publication_year": 2012
            },
            "@cite_6": {
                "mid": "1997991368",
                "abstract": "In this paper, we propose an efficient method to detect the underlying structures in data. The same as RANSAC, we randomly sample MSSs (minimal size samples) and generate hypotheses. Instead of analyzing each hypothesis separately, the consensus information in all hypotheses is naturally fused into a hypergraph, called random consensus graph, with real structures corresponding to its dense subgraphs. The sampling process is essentially a progressive refinement procedure of the random consensus graph. Due to the huge number of hyperedges, it is generally inefficient to detect dense subgraphs on random consensus graphs. To overcome this issue, we construct a pairwise graph which approximately retains the dense subgraphs of the random consensus graph. The underlying structures are then revealed by detecting the dense subgraphs of the pair-wise graph. Since our method fuses information from all hypotheses, it can robustly detect structures even under a small number of MSSs. The graph framework enables our method to simultaneously discover multiple structures. Besides, our method is very efficient, and scales well for large scale problems. Extensive experiments illustrate the superiority of our proposed method over previous approaches, achieving several orders of magnitude speedup along with satisfactory accuracy and robustness.",
                "doi": "https://doi.org/10.1109/cvpr.2012.6247723",
                "title": "Efficient structure detection via random consensus graph",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1802.00634",
        "mid": "2963561466",
        "abstract": "In this paper we consider the problem of human pose estimation in real-world videos of swimmers. Swimming channels allow filming swimmers simultaneously above and below the water surface with a single stationary camera. These recordings can be used to quantitatively assess the athletes' performance. The quantitative evaluation, so far, requires manual annotations of body parts in each video frame. We therefore apply the concept of CNNs in order to automatically infer the required pose information. Starting with an off-the-shelf architecture, we develop extensions to leverage activity information \u2013 in our case the swimming style of an athlete \u2013 and the continuous nature of the video recordings. Our main contributions are threefold: (a) We apply and evaluate a fine-tuned Convolutional Pose Machine architecture as a baseline in our very challenging aquatic environment and discuss its error modes, (b) we propose an extension to input swimming style information into the fully convolutional architecture and (c) modify the architecture for continuous pose estimation in videos. With these additions we achieve reliable pose estimates with up to +16 more correct body joint detections compared to the baseline architecture.",
        "related_work": "Computer vision has been adopted for various applications in the sports domain. Prominent tasks include sports type @cite_17 and activity recognition @cite_4 @cite_19 , tracking athletes and other objects of interest in videos @cite_8 @cite_24 and human pose estimation @cite_20 @cite_31 . @cite_23 offer an overview of a wide range of application.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2279003914",
                "abstract": "Human activity recognition is a fundamental problem in computer vision with many applications such as video retrieval, automatic visual surveillance and human computer interaction. Sports represent one of the most viewed content on digital tv and the web. Automatically collected statistics of team sports game play represent actionable information for many end users such as coaches and broadcast speakers. Many computer vision methods applied to sport activity classification are often based on multi-camera setups, player tracking and exploit information on the ground-plane. In this work we overcome this limitations and propose an approach that exploits the spatio-temporal structure of a video grouping local spatio-temporal features unsupervisedly. Our robust representation allows to measure video similarity making correspondences among arbitrary patterns. We tested our method on two dataset of Volleyball and Soccer actions outperforming previous results by a large margin. Finally we show how our representation allows to highlight discriminative regions for each action.",
                "doi": "https://doi.org/10.1109/iccvw.2015.103",
                "title": "Understanding Sport Activities from Correspondences of Clustered Trajectories",
                "publication_year": 2015
            },
            "@cite_8": {
                "mid": "2235038879",
                "abstract": "Tracking players in sports videos presents numerous challenges due to weak distinguishing features and unpredictable motion. Considerable work has been done to track players in such videos using a combination of appearance and motion modeling, mostly in continuous streams of video. However, in a broadcast sports video, having advertisements, replays and intermittent change of camera view, it becomes a challenging task to keep track of players over an entire game. In this work, we solve a novel problem of tracking over a sequence of temporally disjoint soccer videos without the use of appearance cue, using a Graph based optimization approach. Each team is represented by a graph, in which the nodes correspond to player positions and the edge weights depend on spatial inter-player distance. We use team formation to associate tracks between clips and provide an end-to-end system that is able to perform statistical and tactical analysis of the game. We also introduce a new challenging dataset of an international soccer game.",
                "doi": "https://doi.org/10.1109/iccvw.2015.101",
                "title": "Tracking When the Camera Looks Away",
                "publication_year": 2015
            },
            "@cite_24": {
                "mid": "2243825728",
                "abstract": "Tracking objects like a basketball from a monocular view is challenging due to its small size, potential to move at high velocities as well as the high frequency of occlusion. However, humans with a deep knowledge of a game like basketball can predict with high accuracy the location of the ball even without seeing it due to the location and motion of nearby objects, as well as information of where it was last seen. Learning from tracking data is problematic however, due to the high variance in player locations. In this paper, we show that by simply \"permuting\" the multi-agent data we obtain a compact role-ordered feature which accurately predict the ball owner. We also show that our formulation can incorporate other information sources such as a vision-based ball detector to improve prediction accuracy.",
                "doi": "https://doi.org/10.1109/iccvw.2015.106",
                "title": "Predicting Ball Ownership in Basketball from a Monocular View Using Only Player Trajectories",
                "publication_year": 2015
            },
            "@cite_19": {
                "mid": "2964346040",
                "abstract": "Due to recent advances in technology, the recording and analysis of video data has become an increasingly common component of athlete training programmes. Today it is incredibly easy and affordable to set up a fixed camera and record athletes in a wide range of sports, such as diving, gymnastics, golf, tennis, etc. However, the manual analysis of the obtained footage is a time-consuming task which involves isolating actions of interest and categorizing them using domain-specific knowledge. In order to automate this kind of task, three challenging sub-problems are often encountered: 1) temporally cropping events actions of interest from continuous video; 2) tracking the object of interest; and 3) classifying the events actions of interest.,,,,,,Most previous work has focused on solving just one of the above sub-problems in isolation. In contrast, this paper provides a complete solution to the overall action monitoring task in the context of a challenging real-world exemplar. Specifically, we address the problem of diving classification. This is a challenging problem since the person (diver) of interest typically occupies fewer than 1 of the pixels in each frame. The model is required to learn the temporal boundaries of a dive, even though other divers and bystanders may be in view. Finally, the model must be sensitive to subtle changes in body pose over a large number of frames to determine the classification code. We provide effective solutions to each of the sub-problems which combine to provide a highly functional solution to the task as a whole. The techniques proposed can be easily generalized to video footage recorded from other sports.",
                "doi": "https://doi.org/10.1109/cvprw.2017.18",
                "title": "Extraction and Classification of Diving Clips from Continuous Video Footage",
                "publication_year": 2017
            },
            "@cite_23": {
                "mid": "1939033588",
                "abstract": "The first book of its kind devoted to this topic, this comprehensive text reference presents state-of-the-art research and reviews current challenges in the application of computer vision to problems in sports. Opening with a detailed introduction to the use of computer vision across the entire life-cycle of a sports event, the text then progresses to examine cutting-edge techniques for tracking the ball, obtaining the whereabouts and pose of the players, and identifying the sport being played from video footage. The work concludes by investigating a selection of systems for the automatic analysis and classification of sports play. The insights provided by this pioneering collection will be of great interest to researchers and practitioners involved in computer vision, sports analysis and media production.",
                "doi": "https://doi.org/10.1007/978-3-319-09396-3_10",
                "title": "Classification of Sports Types Using Thermal Imagery",
                "publication_year": 2014
            },
            "@cite_31": {
                "mid": "2739360936",
                "abstract": "Analyzing joint movements of an athlete helps to improve the pose of the athlete. Human pose estimation (HPE) algorithms regress the locations of parts such as wrists, ankles and knees. In this paper, we propose a network that combines global and local information for HPE using a 2D image. Unlike previous works that have used global or local information separately, we use the combined information to enhance the performance of HPE. General information from a global network is used as an input to a local network to refine the location of a part using a variety of regions. The global network is based on ResNet-101 [6] and trained to regress a heatmap representing parts\u2019 locations. The output features from the global network are used as input features for the local network. The local network learns spatial information using position sensitive score maps [11]. Through the end-to-end learning, the global network is affected by the local information. We demonstrate that the proposed HPE method is efficient on the LSP and UCF sports datasets.",
                "doi": "https://doi.org/10.1109/cvprw.2017.20",
                "title": "Athlete Pose Estimation by a Global-Local Network",
                "publication_year": 2017
            },
            "@cite_20": {
                "mid": "2090994350",
                "abstract": "Human pose estimation from monocular video streams is a challenging problem. Much of the work on this problem has focused on developing inference algorithms and probabilistic prior models based on learned measurements. Such algorithms face challenges in generalization beyond the learned dataset. We propose an interactive model-based generative approach for estimating the human pose in 2D from uncalibrated monocular video in unconstrained sports TV footage without any prior learning on motion captured or annotated data. Belief-propagation over a spatio-temporal graph of candidate body part hypotheses is used to estimate a temporally consistent pose between key-frame constraints. Experimental results show that the proposed generative pose estimation framework is capable of estimating pose even in very challenging unconstrained scenarios.",
                "doi": "https://doi.org/10.1109/cvprw.2013.152",
                "title": "Athlete Pose Estimation from Monocular TV Sports Footage",
                "publication_year": 2013
            },
            "@cite_17": {
                "mid": "2042196164",
                "abstract": "Automatic classification of activities in a sports arena is important in order to analyse and optimise the use of the arenas. In this work we classify five sports types based only on occupancy heatmaps produced from position data. Due to privacy issues we use thermal imaging for detecting people and then calculate their positions on the court using homography. Heatmaps are produced by summarising Gaussian distributions respresenting people over 10-minute periods. Before classification the heatmaps are projected to a low-dimensional discriminative space using the principle of Fisherfaces. Our result using two weeks of video are very promising with a correct classification of 90.76 .",
                "doi": "https://doi.org/10.1109/cvprw.2013.145",
                "title": "Sports Type Classification Using Signature Heatmaps",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1801.03595",
        "mid": "2783067984",
        "abstract": "This paper considers the exploitation of unmanned aerial vehicles (UAVs) in wireless networking, with which communication-enabled robots operate as flying wireless relays to provide connectivity or a capacity boost to a ground user. We focus on the particular problem of (automatic) UAV positioning, which greatly affects the end-to-end throughput performance. While existing methods rely on propagation distance minimiza- tion and statistical models for the presence or absence of a line-of-sight (LOS), we propose an approach capable of leveraging local topological information so as to offer better performance guarantees. The proposed method allows to strike a trade-off between minimizing distance path loss and discovering (near) LOS opportunities at locations away from the base station (BS)-user axis. Furthermore, the algorithm is shown to find the global optimal UAV position, although it only requires a local exploration of a signal strength map and the length of search trajectory is only linear to the geographical scale. Hence, it lends itself to online implementation. Significant throughput gains are found when compared to other positioning approaches based on LOS statistical models.",
        "related_work": "One essential issue for positioning the UAV is to predict the air-to-ground channel, since a precise UAV-user channel knowledge is usually not available before the UAV is sent to a target position. As a way to circumvent this problem, some prior works simply assumed LOS propagation from the UAV to the ground user, and hence the channel gain is merely an explicit continuous function of the UAV position. Using the LOS model, @cite_25 @cite_19 @cite_24 @cite_14 focused on UAV navigation problems, and @cite_5 studied the mimo channel to jointly optimize the UAV position and the orientation of a ula mounted at the UAV for the best spatial channel to multiple users. However, LOS models overestimate the channel gain in urban scenarios, especially in low altitude UAV applications, because there the air-to-ground signal is likely blocked by obstacles surrounding the user, and such a user-side shadowing effect significantly dominates the relay performance.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "1975709959",
                "abstract": "This paper investigates the problem of communication relay between ground units using an unmanned aerial vehicle (UAV). The positions of the ground units are considered to be unavailable to the UAV and the objective is to drive the vehicle in real-time to the optimal placement maximizing the strength of communication signals from ground units. To this end, a novel non model-based navigation law is proposed that is solely based on signals strength and their angles of arrival. The stability of the proposed navigation law is proved in two scenarios, driving the UAV to the optimal placement and returning it back to the launch base. Simulation and experimental results are given to show the effectiveness of the proposed approach.",
                "doi": "https://doi.org/10.1109/cdc.2014.7039508",
                "title": "Optimal position seeking for unmanned aerial vehicle communication relay using only signal strength and angle of arrival",
                "publication_year": 2014
            },
            "@cite_24": {
                "mid": "2050744835",
                "abstract": "We investigate the maneuvering control of a UAV-based relay for efficient communication with mobile ground nodes. The objective of the maneuvering control is to maximize the minimum ergodic link capacity which is achieved by any ground node. We propose an efficient maneuvering control scheme of a UAV-based relay which does not require any location or direction information of ground nodes. The proposed control scheme exploits the received signal strengths (RSSs) from ground nodes but it dose not need a channel model to estimate the location of ground nodes. Therefore, the proposed control scheme significantly reduces the communication overhead and or computational complexity while achieving similar performance, compared with that of an optimal scheme which requires the location information of ground nodes.",
                "doi": "https://doi.org/10.1109/iscc.2014.6912558",
                "title": "Low-complexity maneuvering control of a UAV-based relay without location information of mobile ground nodes",
                "publication_year": 2014
            },
            "@cite_19": {
                "mid": "2128983591",
                "abstract": "In this paper, we examine the resource optimization problem in a broadcast relay network where a source broadcasts signals to the user receivers, which are distributed over a service region. The source does not have direct line-of-sight to the service area and the information is delivered through a relay. The objective of this paper is to jointly optimize the relay position as well as the power allocation between the source and relay so that the outage probability of the signal received at the user nodes is minimized. Two different optimization criteria, which respectively minimize the worst-case outage probability and the average outage probability, are used. The analyses are verified by simulation results.",
                "doi": "https://doi.org/10.1109/icassp.2012.6288422",
                "title": "Joint optimization of relay position and power allocation in cooperative broadcast wireless networks",
                "publication_year": 2012
            },
            "@cite_5": {
                "mid": "2039409843",
                "abstract": "We consider a collection of single-antenna ground nodes communicating with a multi-antenna unmanned aerial vehicle (UAV) over a multiple-access ground-to-air communications link. The UAV uses beamforming to mitigate inter-user interference and achieve spatial division multiple access (SDMA). First, we consider a simple scenario with two static ground nodes and analytically investigate the effect of the UAV's heading on the system sum rate. We then study a more general setting with multiple mobile ground-based terminals, and develop an algorithm for dynamically adjusting the UAV heading to maximize the approximate ergodic sum rate of the uplink channel, using a prediction filter to track the positions of the mobile ground nodes. For the common scenario where a strong line-of-sight (LOS) channel exists between the ground nodes and UAV, we use an asymptotic analysis to find simplified versions of the algorithm for low and high SNR. We present simulation results that demonstrate the benefits of adapting the UAV heading in order to optimize the uplink communications performance. The simulation results also show that the simplified algorithms provide near-optimal performance.",
                "doi": "https://doi.org/10.1109/jsac.2012.120614",
                "title": "Optimization of UAV Heading for the Ground-to-Air Uplink",
                "publication_year": 2012
            },
            "@cite_25": {
                "mid": "2043939916",
                "abstract": "Abstract Recently, unmanned aerial vehicles (UAVs) acting as relay platforms have attracted considerable attention due to the advantages of extending coverage and improving connectivity for long-range communications. Specifically, in the scenario where the access point (AP) is mobile, a UAV needs to find an efficient path to guarantee the connectivity of the relay link. Motivated by this fact, this paper proposes an optimal design for beamforming (BF) and UAV path planning. First of all, we study a dual-hop amplify-and-forward (AF) wireless relay network, in which a UAV is used as relay between a mobile AP and a fixed base station (BS). In the network, both of the AP and the BS are equipped with multiple antennas, whereas the UAV has a single antenna. Then, we obtain the output signal-to-noise ratio (SNR) of the dual-hop relay network. Based on the criterion of maximizing the output SNR, we develop an optimal design to obtain the solution of the optimal BF weight vector and the UAV heading angle. Next, we derive the closed-form outage probability (OP) expression to investigate the performance of the dual-hop relay network conveniently. Finally, computer simulations show that the proposed approach can obtain nearly optimal flying path and OP performance, indicating the effectiveness of the proposed algorithm. Furthermore, we find that increasing the antenna number at the BS or the maximal heading angle can significantly improve the performance of the considered relay network.",
                "doi": "https://doi.org/10.1016/j.cja.2014.02.011",
                "title": "Optimization of beamforming and path planning for UAV-assisted wireless relay networks",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1712.09121",
        "mid": "2963948050",
        "abstract": "Computing shortest paths is one of the central problems in the theory of distributed computing. For the last few years, substantial progress has been made on the approximate single source shortest paths problem, culminating in an algorithm of Henzinger, Krinninger, and Nanongkai [STOC\u201916] which deterministically computes (1+o(1))-approximate shortest paths in O(D+\u221an) time, where D is the hop-diameter of the graph. Up to logarithmic factors, this time complexity is optimal, matching the lower bound of Elkin [STOC\u201904]. The question of exact shortest paths however saw no algorithmic progress for decades, until the recent breakthrough of Elkin [STOC\u201917], which established a sublinear-time algorithm for exact single source shortest paths on undirected graphs. Shortly after, [FOCS\u201917] provided improved algorithms for exact all pairs shortest paths problem on directed graphs. In this paper, we provide an alternative single-source shortest path algorithm with complexity O(n3 4D1 4). For polylogarithmic D, this improves on Elkin\u2019s O(n5 6) bound and gets closer to the \u03a9(n1 2) lower bound of Elkin [STOC\u201904]. For larger values of D, we present an improved variant of our algorithm which achieves complexity O(max n3 4+o(1) , n3 4D1 6 + D ), and thus compares favorably with Elkin\u2019s bound of O(max n5 6, n2 3D1 3 + D ) in essentially the entire range of parameters. This algorithm provides also a qualitative improvement, because it works for the more challenging case of directed graph (i.e., graphs where the two directions of an edge can have different weights), constituting the first sublinear-time algorithm for directed graphs. Our algorithm also extends to the case of exact r-source shortest paths, in which we provide the fastest algorithm for moderately small r and D, improving on those of",
        "related_work": "Despite the numerous steps of progress that brought us to an almost complete understanding of the approximate case of SSSP, computing exact SSSP remained open until very recently. But then came the breakthrough of Elkin @cite_26 , which provided the first sublinear-time algorithm for exact SSSP. More precisely, Elkin gave an algorithm that computes exact SSSP in @math rounds if @math , and in @math rounds if @math .",
        "ref_abstract": {
            "@cite_26": {
                "mid": "2592613193",
                "abstract": "The distributed single-source shortest paths problem is one of the most fundamental and central problems in the message-passing distributed computing. Classical Bellman-Ford algorithm solves it in O(n) time, where n is the number of vertices in the input graph G. Peleg and Rubinovich, FOCS'99, showed a lower bound of \u03a9(D + \u221an) for this problem, where D is the hop-diameter of G. Whether or not this problem can be solved in o(n) time when D is relatively small is a major notorious open question. Despite intensive research that yielded near-optimal algorithms for the approximate variant of this problem, no progress was reported for the original problem. In this paper we answer this question in the affirmative. We devise an algorithm that requires O((n logn)5 6) time, for D = O(\u221an logn), and O(D1 3 #183; (n logn)2 3) time, for larger D. This running time is sublinear in n in almost the entire range of parameters, specifically, for D = o(n log2 n). We also generalize our result in two directions. One is when edges have bandwidth b \u2265 1, and the other is the s-sources shortest paths problem. For the former problem, our algorithm provides an improved bound, compared to the unit-bandwidth case. In particular, we provide an all-pairs shortest paths algorithm that requires O(n5 3 #183; log2 3 n) time, even for b = 1, for all values of D. For the latter problem (of s sources), our algorithm also provides bounds that improve upon the previous state-of-the-art in the entire range of parameters. From the technical viewpoint, our algorithm computes a hopset G\u2033 of a skeleton graph G\u2032 of G without first computing G\u2032 itself. We then conduct a Bellman-Ford exploration in G\u2032 \u222a G\u2033, while computing the required edges of G\u2032 on the fly. As a result, our algorithm computes exactly those edges of G\u2032 that it really needs, rather than computing approximately the entire G\u2032.",
                "doi": "https://doi.org/10.1145/3055399.3055452",
                "title": "Distributed exact shortest paths in sublinear time",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1712.09025",
        "mid": "2786894871",
        "abstract": "Many methods have been proposed to solve the domain adaptation problem recently. However, the success of them implicitly funds on the assumption that the information of domains are fully transferrable. If the assumption is not satisfied, the effect of negative transfer may degrade domain adaptation. In this paper, a better learning network has been proposed by considering three tasks - domain adaptation, disentangled representation, and style transfer simultaneously. Firstly, the learned features are disentangled into common parts and specific parts. The common parts represent the transferrable features, which are suitable for domain adaptation with less negative transfer. Conversely, the specific parts characterize the unique style of each individual domain. Based on this, the new concept of feature exchange across domains, which can not only enhance the transferability of common features but also be useful for image style transfer, is introduced. These designs allow us to introduce five types of training objectives to realize the three challenging tasks at the same time. The experimental results show that our architecture can be adaptive well to full transfer learning and partial transfer learning upon a well-learned disentangled representation. Besides, the trained network also demonstrates high potential to generate style-transferred images.",
        "related_work": "Partial Transfer Learning : Partial transfer learning was proposed in @cite_10 , where the target domain label space is a subspace of the source domain label space. Because the extra source classes might cause negative transfer when classifying the target domain, it makes the domain adaptation problem more challenging. In this work, in order to solve the partial transfer problem, instead of using single-discriminator domain adversarial network, the authors proposed to use multi-discriminator domain adversarial network, each discriminator is responsible for matching the source and target domain data associated with each label.",
        "ref_abstract": {
            "@cite_10": {
                "mid": "2738463471",
                "abstract": "Adversarial learning has been successfully embedded into deep networks to learn transferable features, which reduce distribution discrepancy between the source and target domains. Existing domain adversarial networks assume fully shared label space across domains. In the presence of big data, there is strong motivation of transferring both classification and representation models from existing big domains to unknown small domains. This paper introduces partial transfer learning, which relaxes the shared label space assumption to that the target label space is only a subspace of the source label space. Previous methods typically match the whole source domain to the target domain, which are prone to negative transfer for the partial transfer problem. We present Selective Adversarial Network (SAN), which simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive transfer by maximally matching the data distributions in the shared label space. Experiments demonstrate that our models exceed state-of-the-art results for partial transfer learning tasks on several benchmark datasets.",
                "doi": "https://doi.org/10.48550/arxiv.1707.07901",
                "title": "Partial Transfer Learning with Selective Adversarial Networks",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1712.03280",
        "mid": "2773451980",
        "abstract": "Nintendo's Super Smash Bros. Melee fighting game can be emulated on modern hardware allowing us to inspect internal memory states, such as character positions. We created an AI that avoids being hit by training using these internal memory states and outputting controller button presses. After training on a month's worth of Melee matches, our best agent learned to avoid the toughest AI built into the game for a full minute 74.6 of the time.",
        "related_work": "DeepMind Technologies has successfully learned to play Atari 2600 games using an emulator @cite_3 . DeepMind used a convolutional neural network combined with a Q-learning variant to learn to play games using just the games' pixel output. We plan to build on this success by focusing on a relatively newer game with more complex interactions. Instead of pixels, we look at the game's internal memory, including features such as characters' positions and velocities.",
        "ref_abstract": {
            "@cite_3": {
                "mid": "2145339207",
                "abstract": "An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.",
                "doi": "https://doi.org/10.1038/nature14236",
                "title": "Human-level control through deep reinforcement learning",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1712.01238",
        "mid": "2771317710",
        "abstract": "We introduce an interactive learning framework for the development and testing of intelligent visual systems, called learning-by-asking (LBA). We explore LBA in context of the Visual Question Answering (VQA) task. LBA differs from standard VQA training in that most questions are not observed during training time, and the learner must ask questions it wants answers to. Thus, LBA more closely mimics natural learning and has the potential to be more data-efficient than the traditional VQA setting. We present a model that performs LBA on the CLEVR dataset, and show that it automatically discovers an easy-to-hard curriculum when learning interactively from an oracle. Our LBA generated data consistently matches or outperforms the CLEVR train data and is more sample efficient. We also show that our model asks questions that generalize to state-of-the-art VQA models and to novel test time distributions.",
        "related_work": "(VQG) was recently proposed as an alternative to image captioning . Our work is related to VQG in the sense that we require the learner to generate questions about images, however, our objective in doing so is different. Whereas VQG focuses on asking questions that are relevant to the image content, LBA requires the learner to ask questions that are both relevant and informative to the learner when answered. A positive side effect is that LBA circumvents the difficulty of evaluating the quality of generated questions (which also hampers image captioning ), because the question-answering accuracy of our final model directly correlates with the quality of the questions asked. Such evaluation has also been used in recent works in the language community @cite_28 @cite_27 .",
        "ref_abstract": {
            "@cite_28": {
                "mid": "2622069672",
                "abstract": "We propose a generative machine comprehension model that learns jointly to ask and answer questions based on documents. The proposed model uses a sequence-to-sequence framework that encodes the document and generates a question (answer) given an answer (question). Significant improvement in model performance is observed empirically on the SQuAD corpus, confirming our hypothesis that the model benefits from jointly learning to perform both tasks. We believe the joint model's novelty offers a new perspective on machine comprehension beyond architectural engineering, and serves as a first step towards autonomous information seeking.",
                "doi": "https://doi.org/10.48550/arxiv.1706.01450",
                "title": "A Joint Model for Question Answering and Question Generation",
                "publication_year": 2017
            },
            "@cite_27": {
                "mid": "2586581008",
                "abstract": "We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",
                "doi": "https://doi.org/10.48550/arxiv.1702.02206",
                "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1711.08502",
        "mid": "2769568343",
        "abstract": "Despite the growing discriminative capabilities of modern deep learning methods for recognition tasks, the inner workings of the state-of-art models still remain mostly black-boxes. In this paper, we propose a systematic interpretation of model parameters and hidden representations of Residual Temporal Convolutional Networks (Res-TCN) for action recognition in time-series data. We also propose a Feature Map Decoder as part of the interpretation analysis, which outputs a representation of model's hidden variables in the same domain as the input. Such analysis empowers us to expose model's characteristic learning patterns in an interpretable way. For example, through the diagnosis analysis, we discovered that our model has learned to achieve view-point invariance by implicitly learning to perform rotational normalization of the input to a more discriminative view. Based on the findings from the model interpretation analysis, we propose a targeted refinement technique, which can generalize to various other recognition models. The proposed work introduces a three-stage paradigm for model learning: training, interpretable diagnosis and targeted refinement. We validate our approach on skeleton based 3D human action recognition benchmark of NTU RGB+D. We show that the proposed workflow is an effective model learning strategy and the resulting Multi-stream Residual Temporal Convolutional Network (MS-Res-TCN) achieves the state-of-the-art performance on NTU RGB+D.",
        "related_work": "In action segmentation and detection, a new class of temporal models, Temporal Convolutional Network (TCN) @cite_16 , is proposed to capture long-range temporal dependencies, which outperforms other LSTM-based Recurrent Networks in both accuracy and training time. The work of @cite_6 employs a re-designed TCN model (Res-TCN) with residual connections @cite_30 @cite_13 to skeleton based human action recognition task. The proposed Res-TCN also learns both spatial and temporal attention for human action recognition. The authors of @cite_6 argue that Res-TCN is specifically designed to enhance the interpretability of model parameters and features compared to other recurrent models. However, the work of @cite_6 remains mostly intuitive and unsystematic as opposed to our work.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "2949650786",
                "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57 error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                "doi": "https://doi.org/10.48550/arxiv.1512.03385",
                "title": "Deep Residual Learning for Image Recognition",
                "publication_year": 2015
            },
            "@cite_16": {
                "mid": "2550143307",
                "abstract": "The ability to identify and temporally segment fine-grained human actions throughout a video is crucial for robotics, surveillance, education, and beyond. Typical approaches decouple this problem by first extracting local spatiotemporal features from video frames and then feeding them into a temporal classifier that captures high-level temporal patterns. We describe a class of temporal models, which we call Temporal Convolutional Networks (TCNs), that use a hierarchy of temporal convolutions to perform fine-grained action segmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling to efficiently capture long-range temporal patterns whereas our Dilated TCN uses dilated convolutions. We show that TCNs are capable of capturing action compositions, segment durations, and long-range dependencies, and are over a magnitude faster to train than competing LSTM-based Recurrent Neural Networks. We apply these models to three challenging fine-grained datasets and show large improvements over the state of the art.",
                "doi": "https://doi.org/10.1109/cvpr.2017.113",
                "title": "Temporal Convolutional Networks for Action Segmentation and Detection",
                "publication_year": 2017
            },
            "@cite_13": {
                "mid": "2302255633",
                "abstract": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https: github.com KaimingHe resnet-1k-layers.",
                "doi": "https://doi.org/10.1007/978-3-319-46493-0_38",
                "title": "Identity Mappings in Deep Residual Networks",
                "publication_year": 2016
            },
            "@cite_6": {
                "mid": "2613570903",
                "abstract": "The discriminative power of modern deep learning models for 3D human action recognition is growing ever so potent. In conjunction with the recent resurgence of 3D human action representation with 3D skeletons, the quality and the pace of recent progress have been significant. However, the inner workings of state-of-the-art learning based methods in 3D human action recognition still remain mostly black-box. In this work, we propose to use a new class of models known as Temporal Convolutional Neural Networks (TCN) for 3D human action recognition. TCN provides us a way to explicitly learn readily interpretable spatio-temporal representations for 3D human action recognition. Through this work, we wish to take a step towards a spatio-temporal model that is easier to understand, explain and interpret. The resulting model, Res-TCN, achieves state-of-the-art results on the largest 3D human action recognition dataset, NTU-RGBD.",
                "doi": "https://doi.org/10.1109/cvprw.2017.207",
                "title": "Interpretable 3D Human Action Analysis with Temporal Convolutional Networks",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1711.04731",
        "mid": "2898410387",
        "abstract": "In the prose style transfer task a system, provided with text input and a target prose style, produces output which preserves the meaning of the input text but alters the style. These systems require parallel data for evaluation of results and usually make use of parallel data for training. Currently, there are few publicly available corpora for this task. In this work, we identify a high-quality source of aligned, stylistically distinct text in different versions of the Bible. We provide a standardized split, into training, development and testing data, of the public domain versions in our corpus. This corpus is highly parallel since many Bible versions are included. Sentences are aligned due to the presence of chapter and verse numbers within all versions of the text. In addition to the corpus, we present the results, as measured by the BLEU and PINC metrics, of several models trained on our data which can serve as baselines for future research. While we present these data as a style transfer corpus, we believe that it is of unmatched quality and may be useful for other natural language tasks as well.",
        "related_work": "The clearest connection is to work in traditional language-to-language translation. The Seq2Seq model was first created and used in conjunction with statistical methods to perform machine translation @cite_41 . The model consists of a recurrent neural network acting as an encoder which produces an embedding of the full sequence of inputs. This sentence embedding is then used by another recurrent neural network which acts as a decoder and produces a sequence corresponding to the original input sequence.",
        "ref_abstract": {
            "@cite_41": {
                "mid": "2950635152",
                "abstract": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
                "doi": "https://doi.org/10.48550/arxiv.1406.1078",
                "title": "Learning Phrase Representations using RNN Encoder-Decoder for\n  Statistical Machine Translation",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1710.11383",
        "mid": "2766856052",
        "abstract": "We consider the problem of training generative models with deep neural networks as generators, i.e. to map latent codes to data points. Whereas the dominant paradigm combines simple priors over codes with complex deterministic models, we argue that it might be advantageous to use more flexible code distributions. We demonstrate how these distributions can be induced directly from the data. The benefits include: more powerful generative models, better modeling of latent structure and explicit control of the degree of generalization.",
        "related_work": "Finally, we note that the importance of using an appropriate prior for GAN models has also been discussed in @cite_10 which suggested to infer the continuous latent factors in order to maximize the data log-likelihood. However this approach still makes use of a simple fixed prior distribution over the latent factors and does not use the inferred latent factors to construct a prior as suggested by our approach.",
        "ref_abstract": {
            "@cite_10": {
                "mid": "2519766107",
                "abstract": "This paper proposes an alternating back-propagation algorithm for learning the generator network model. The model is a non-linear generalization of factor analysis. In this model, the mapping from the continuous latent factors to the observed signal is parametrized by a convolutional neural network. The alternating back-propagation algorithm iterates the following two steps: (1) Inferential back-propagation, which infers the latent factors by Langevin dynamics or gradient descent. (2) Learning back-propagation, which updates the parameters given the inferred latent factors by gradient descent. The gradient computations in both steps are powered by back-propagation, and they share most of their code in common. We show that the alternating back-propagation algorithm can learn realistic generator models of natural images, video sequences, and sounds. Moreover, it can also be used to learn from incomplete or indirect training data.",
                "doi": "https://doi.org/10.1609/aaai.v31i1.10902",
                "title": "Alternating Back-Propagation for Generator Network",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1710.10225",
        "mid": "2765325603",
        "abstract": "During the past two years, Flash malware has become one of the most insidious threats to detect, with almost 600 critical vulnerabilities targeting Adobe Flash Player disclosed in the wild. Research has shown that machine learning can be successfully used to tackle this increasing variability and sophistication of Flash malware, by simply leveraging static analysis to extract information from the structure of the file or from its bytecode. However, the robustness of such systems against well-crafted evasion attempts - also known as adversarial examples - has never been investigated. In this paper, we first discuss how to craft adversarial Flash malware examples, and show that it suffices to only slightly manipulate them to evade detection. We then empirically demonstrate that popular defense techniques proposed to mitigate such threat, including re-training on adversarial examples, may not always be effective. We argue that this occurs when the feature vectors extracted from adversarial examples become indistinguishable from those of benign data, meaning that the given feature representation is intrinsically vulnerable. In this respect, we are the first to formally define and quantitatively characterize this vulnerability, highlighting when an attack can be countered by solely improving the security of the learning algorithm, or when it requires also considering additional features. We conclude the paper by suggesting alternative research directions to improve the security of learning-based Flash malware detectors.",
        "related_work": "As -based malicious attacks started to considerably grow in @math , the number of detection approaches is rather limited. FlashDetect @cite_30 is one of the first approaches to the detection of @math -based malware. The authors instrumented Lightspark , an open source viewer, to perform dynamic analysis of malicious files. From this analysis, the system extracts features such as the number of ByteArray -related method calls, the presence of the loadBytes method, and so forth. FlashDetect was employed inside the Wepawet service, which is sadly not available anymore.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "1773541209",
                "abstract": "Adobe Flash is present on nearly every PC, and it is increasingly being targeted by malware authors. Despite this, research into methods for detecting malicious Flash files has been limited. Similarly, there is very little documentation available about the techniques commonly used by Flash malware. Instead, most research has focused on JavaScript malware. This paper discusses common techniques such as heap spraying, JIT spraying, and type confusion exploitation in the context of Flash malware. Where applicable, these techniques are compared to those used in malicious JavaScript. Subsequently, FlashDetect is presented, an offline Flash file analyzer that uses both dynamic and static analysis, and that can detect malicious Flash files using ActionScript 3. FlashDetect classifies submitted files using a naive Bayesian classifier based on a set of predefined features. Our experiments show that FlashDetect has high classification accuracy, and that its efficacy is comparable with that of commercial anti-virus products.",
                "doi": "https://doi.org/10.1007/978-3-642-33338-5_14",
                "title": "FlashDetect: ActionScript 3 Malware Detection",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1710.06699",
        "mid": "2765264240",
        "abstract": "In this paper, we propose an approach for the detection of clickbait posts in online social media (OSM). Clickbait posts are short catchy phrases that attract a user's attention to click to an article. The approach is based on a machine learning (ML) classifier capable of distinguishing between clickbait and legitimate posts published in OSM. The suggested classifier is based on a variety of features, including image related features, linguistic analysis, and methods for abuser detection. In order to evaluate our method, we used two datasets provided by Clickbait Challenge 2017. The best performance obtained by the ML classifier was an AUC of 0.8, an accuracy of 0.812, precision of 0.819, and recall of 0.966. In addition, as opposed to previous studies, we found that clickbait post titles are statistically significant shorter than legitimate post titles. Finally, we found that counting the number of formal English words in the given content is useful for clickbait detection.",
        "related_work": "In 2015, Blom and Hansen @cite_13 mapped the use of forward-referencing headlines in online news by analyzing 100,000 headlines published in ten different Danish news websites. They found that commercialization and tabloidization seem to lead to the recurrent use of forward-referencing in Danish online news headlines.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "2046881809",
                "abstract": "Abstract This is why you should read this article. Although such an opening statement does not make much sense read in isolation, journalists often write headlines like this on news websites. They use the forward-referring technique as a stylistic and narrative luring device trying to induce anticipation and curiosity so the readers click (or tap on) the headline and read on. In this article, we map the use of forward-referring headlines in online news journalism by conducting an analysis of 100,000 headlines from 10 different Danish news websites. The results show that commercialization and tabloidization seem to lead to a recurrent use of forward-reference in Danish online news headlines. In addition, the article contributes to reference theory by expanding previous models on phoricity to include multimodal references on the web.",
                "doi": "https://doi.org/10.1016/j.pragma.2014.11.010",
                "title": "Click bait: Forward-reference as lure in online news headlines",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1710.04502",
        "mid": "2762175147",
        "abstract": "Understanding driving behaviors is essential for improving safety and mobility of our transportation systems. Data is usually collected via simulator-based studies or naturalistic driving studies. Those techniques allow for understanding relations between demographics, road conditions and safety. On the other hand, they are very costly and time consuming. Thanks to the ubiquity of smartphones, we have an opportunity to substantially complement more traditional data collection techniques with data extracted from phone sensors, such as GPS, accelerometer gyroscope and camera. We developed statistical models that provided insight into driver behavior in the San Francisco metro area based on tens of thousands of driver logs. We used novel data sources to support our work. We used cell phone sensor data drawn from five hundred drivers in San Francisco to understand the speed of traffic across the city as well as the maneuvers of drivers in different areas. Specifically, we clustered drivers based on their driving behavior. We looked at driver norms by street and flagged driving behaviors that deviated from the norm.",
        "related_work": "Driving behavior analysis is an interdisciplinary field and has drawn increasing attention in the recent decade, specifically in the context of vehicle automation @cite_4 . An in-depth recent review of the literature on driving style analysis frameworks is given in @cite_5 .",
        "ref_abstract": {
            "@cite_5": {
                "mid": "1931305913",
                "abstract": "Objective:The aim of this study was to outline a conceptual framework for understanding driving style and, on this basis, review the state-of-the-art research on driving styles in relation to road safety.Background:Previous research has indicated a relationship between the driving styles adopted by drivers and their crash involvement. However, a comprehensive literature review of driving style research is lacking.Method:A systematic literature search was conducted, including empirical, theoretical, and methodological research, on driving styles related to road safety.Results:A conceptual framework was proposed whereby driving styles are viewed in terms of driving habits established as a result of individual dispositions as well as social norms and cultural values. Moreover, a general scheme for categorizing and operationalizing driving styles was suggested. On this basis, existing literature on driving styles and indicators was reviewed. Links between driving styles and road safety were identified and ind...",
                "doi": "https://doi.org/10.1177/0018720815591313",
                "title": "A Review of Research on Driving Styles and Road Safety",
                "publication_year": 2015
            },
            "@cite_4": {
                "mid": "2463627759",
                "abstract": "This paper highlights the role of humans in the next generation of driver assistance and intelligent vehicles. Understanding, modeling, and predicting human agents are discussed in three domains where humans and highly automated or self-driving vehicles interact: 1) inside the vehicle cabin, 2) around the vehicle, and 3) inside surrounding vehicles. Efforts within each domain, integrative frameworks across domains, and scientific tools required for future developments are discussed to provide a human-centered perspective on research in intelligent vehicles.",
                "doi": "https://doi.org/10.1109/tiv.2016.2571067",
                "title": "Looking at Humans in the Age of Self-Driving and Highly Automated Vehicles",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1710.03287",
        "mid": "2764111657",
        "abstract": "In this paper we consider memoryless one-bit compressed sensing with randomly subsampled Gaussian circulant matrices. We show that in a small sparsity regime and for small enough accuracy @math , @math measurements suffice to reconstruct the direction of any @math -sparse vector up to accuracy @math via an efficient program. We derive this result by proving that partial Gaussian circulant matrices satisfy an @math RIP-property. Under a slightly worse dependence on @math , we establish stability with respect to approximate sparsity, as well as full vector recovery results.",
        "related_work": "Standard compressive sensing with partial circulant matrices. In standard (unquantized) compressive sensing, the task is to recover an (approximately) sparse vector @math from measurements @math , where @math with @math . A number of reconstruction algorithms have been introduced, most notably @math -minimization which computes the minimizer of [ z ^N |z |_1 subject to A z = Ax. ] The ( @math -)restricted isometry property is a classical way of analyzing the performance of compressive sensing @cite_9 . The restricted isometry constant @math is defined as the smallest constant @math such that If @math then all @math -sparse signals can be reconstructed via @math -minimization exactly, see e.g. @cite_17 @cite_9 . Stability under noise and sparsity defect can be shown as well and similar guarantees also hold for other reconstruction algorithms @cite_9 . It is well-known that Gaussian random matrices satisfy @math with probability at least @math if @math [Chapter 9] fora13 .",
        "ref_abstract": {
            "@cite_9": {
                "mid": "2032291279",
                "abstract": "In this expository note, we give a modern proof of Hanson-Wright inequality for quadratic forms in sub-gaussian random variables.We deduce a useful concentration inequality for sub-gaussian random vectors.Two examples are given to illustrate these results: a concentration of distances between random vectors and subspaces, and a bound on the norms of products of random and deterministic matrices.",
                "doi": "https://doi.org/10.1214/ecp.v18-2865",
                "title": "Hanson-Wright inequality and sub-gaussian concentration",
                "publication_year": 2013
            },
            "@cite_17": {
                "mid": "2000150201",
                "abstract": "This paper considers compressed sensing and affine rank minimization in both noiseless and noisy cases and establishes sharp restricted isometry conditions for sparse signal and low-rank matrix recovery. The analysis relies on a key technical tool, which represents points in a polytope by convex combinations of sparse vectors. The technique is elementary while yielding sharp results. It is shown that for any given constant t \u2265 4 3, in compressed sensing, \u03b4tkA 0, \u03b4tkA <; \u221a(t-1 t) + e is not sufficient to guarantee the exact recovery of all k-sparse signals for large k. Similar results also hold for matrix recovery. In addition, the conditions \u03b4tkA <; \u221a((t-)1 t) and \u03b4trM <; \u221a((t-1) t) are also shown to be sufficient, respectively, for stable recovery of approximately sparse signals and low-rank matrices in the noisy case.",
                "doi": "https://doi.org/10.1109/tit.2013.2288639",
                "title": "Sparse Representation of a Polytope and Recovery of Sparse Signals and Low-Rank Matrices",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1906.01373",
        "mid": "2948292485",
        "abstract": "While word embeddings have been shown to implicitly encode various forms of attributional knowledge, the extent to which they capture relational information is far more limited. In previous work, this limitation has been addressed by incorporating relational knowledge from external knowledge bases when learning the word embedding. Such strategies may not be optimal, however, as they are limited by the coverage of available resources and conflate similarity with other forms of relatedness. As an alternative, in this paper we propose to encode relational knowledge in a separate word embedding, which is aimed to be complementary to a given standard word embedding. This relational word embedding is still learned from co-occurrence statistics, and can thus be used even when no external knowledge base is available. Our analysis shows that relational word vectors do indeed capture information that is complementary to what is encoded in standard word embeddings.",
        "related_work": "A number of approaches have been proposed that are aimed at learning relation vectors for a given set of word pairs ( @math , @math ), based on sentences in which these word pairs co-occur. For instance, introduced a method called Latent Relational Analysis (LRA), which relies on first identifying a set of sufficiently frequent lexical patterns and then constructs a matrix which encodes for each considered word pair ( @math , @math ) how frequently each pattern @math appears in between @math and @math in sentences that contain both words. Relation vectors are then obtained using singular value decomposition. More recently, proposed an approach inspired by the GloVe word embedding model @cite_22 to learn relation vectors based on co-occurrence statistics between the target word pair @math and other words. Along similar lines, learn relation vectors based on the distribution of words occurring in sentences that contain @math and @math by averaging the word vectors of these co-occurring words. Then, a conditional autoencoder is used to obtain lower-dimensional relation vectors.",
        "ref_abstract": {
            "@cite_22": {
                "mid": "2250539671",
                "abstract": "Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75 on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",
                "doi": "https://doi.org/10.3115/v1/d14-1162",
                "title": "Glove: Global Vectors for Word Representation",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1709.06341",
        "mid": "2756279203",
        "abstract": "Limited capture range, and the requirement to provide high quality initialization for optimization-based 2D 3D image registration methods, can significantly degrade the performance of 3D image reconstruction and motion compensation pipelines. Challenging clinical imaging scenarios, which contain significant subject motion such as fet al in-utero imaging, complicate the 3D image and volume reconstruction process. In this paper we present a learning based image registration method capable of predicting 3D rigid transformations of arbitrarily oriented 2D image slices, with respect to a learned canonical atlas co-ordinate system. Only image slice intensity information is used to perform registration and canonical alignment, no spatial transform initialization is required. To find image transformations we utilize a Convolutional Neural Network (CNN) architecture to learn the regression function capable of mapping 2D image slices to a 3D canonical atlas space. We extensively evaluate the effectiveness of our approach quantitatively on simulated Magnetic Resonance Imaging (MRI), fet al brain imagery with synthetic motion and further demonstrate qualitative results on real fet al MRI data where our method is integrated into a full reconstruction and motion compensation pipeline. Our learning based registration achieves an average spatial prediction error of 7 mm on simulated data and produces qualitatively improved reconstructions for heavily moving fetuses with gestational ages of approximately 20 weeks. Our model provides a general and computationally efficient solution to the 2D 3D registration initialization problem and is suitable for real-time scenarios.",
        "related_work": "@cite_30 use Convolutional Neural Networks (CNNs) to automatically estimate the spatial arrangement of landmarks in projection images. Their method utilizes a CNN to regress transformation residuals, @math , which refines the required transformation to register a source volume to a target X-ray image from an initially assumed position @math . Registration is then performed iteratively using synthetic Digitally Reconstructed Radiography (DRR) images generated from the source volume using @math . To address inaccurate transformation mappings caused by direct regression of transformation parameters, train their CNN using Pose-index features (landmarks) extracted from source and target image pairs and learn their @math . Pose-index features are insensitive to transform parameters @math yet sensitive to change in @math . This insensitivity to @math can be expressed as @math . The method requires a robust landmark detection algorithm, which is domain and scanner specific and the detection quality degrades for motion-corrupted data.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "2344328023",
                "abstract": "In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D 3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D 3-D registration with a significantly enlarged capture range when compared to intensity-based methods.",
                "doi": "https://doi.org/10.1109/tmi.2016.2521800",
                "title": "A CNN Regression Approach for Real-Time 2D/3D Registration",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1709.05278",
        "mid": "2754800125",
        "abstract": "Recommendation systems are recognised as being hugely important in industry, and the area is now well understood. At News UK, there is a requirement to be able to quickly generate recommendations for users on news items as they are published. However, little has been published about systems that can generate recommendations in response to changes in recommendable items and user behaviour in a very short space of time. In this paper we describe a new algorithm for updating collaborative filtering models incrementally, and demonstrate its effectiveness on clickstream data from The Times. We also describe the architecture that allows recommendations to be generated on the fly, and how we have made each component scalable. The system is currently being used in production at News UK.",
        "related_work": "Diaz- @cite_1 describe an algorithm for real-time recommendations called Stream-Ranking Matrix Factorization (RMFX) in the context of recommending for social media. This performs matrix factorization and ranking of recommendations on streaming data. However their system requires specifying the set of users and items in advance, which is not appropriate in our setting where we must handle new users and items (in our case new articles) all the time.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2001575213",
                "abstract": "The Social Web is successfully established, and steadily growing in terms of users, content and services. People generate and consume data in real-time within social networking services, such as Twitter, and increasingly rely upon continuous streams of messages for real-time access to fresh knowledge about current affairs. In this paper, we focus on analyzing social streams in real-time for personalized topic recommendation and discovery. We consider collaborative filtering as an online ranking problem and present Stream Ranking Matrix Factorization - RMFX -, which uses a pairwise approach to matrix factorization in order to optimize the personalized ranking of topics. Our novel approach follows a selective sampling strategy to perform online model updates based on active learning principles, that closely simulates the task of identifying relevant items from a pool of mostly uninteresting ones. RMFX is particularly suitable for large scale applications and experiments on the \"476 million Twitter tweets\" dataset show that our online approach largely outperforms recommendations based on Twitter's global trend, and it is also able to deliver highly competitive Top-N recommendations faster while using less space than Weighted Regularized Matrix Factorization (WRMF), a state-of-the-art matrix factorization technique for Collaborative Filtering, demonstrating the efficacy of our approach.",
                "doi": "https://doi.org/10.1145/2365952.2365968",
                "title": "Real-time top-n recommendation in social streams",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1709.02780",
        "mid": "2751096757",
        "abstract": "Recently, there has been a growing interest in analyzing human daily activities from data collected by wearable cameras. Since the hands are involved in a vast set of daily tasks, detecting hands in egocentric images is an important step towards the recognition of a variety of egocentric actions. However, besides extreme illumination changes in egocentric images, hand detection is not a trivial task because of the intrinsic large variability of hand appearance. We propose a hand detector that exploits skin modeling for fast hand proposal generation and Convolutional Neural Networks for hand recognition. We tested our method on UNIGE-HANDS dataset and we showed that the proposed approach achieves competitive hand detection results.",
        "related_work": "In recent years, one of the first attempts to segment hands from egocentric images was proposed by @cite_18 . In order to determine regions containing hands and active objects, they modeled the background pixels using texture and boundary features. From the extracted foreground pixels, they distinguish between hands and objects using color histograms. Additionally, they introduced the Georgia Tech Ego-centric Activity (GTEA) dataset to test their model.",
        "ref_abstract": {
            "@cite_18": {
                "mid": "2031688197",
                "abstract": "This paper addresses the problem of learning object models from egocentric video of household activities, using extremely weak supervision. For each activity sequence, we know only the names of the objects which are present within it, and have no other knowledge regarding the appearance or location of objects. The key to our approach is a robust, unsupervised bottom up segmentation method, which exploits the structure of the egocentric domain to partition each frame into hand, object, and background categories. By using Multiple Instance Learning to match object instances across sequences, we discover and localize object occurrences. Object representations are refined through transduction and object-level classifiers are trained. We demonstrate encouraging results in detecting novel object instances using models produced by weakly-supervised learning.",
                "doi": "https://doi.org/10.1109/cvpr.2011.5995444",
                "title": "Learning to recognize objects in egocentric activities",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1709.02054",
        "mid": "2750938222",
        "abstract": "Scene text recognition has been a hot research topic in computer vision due to its various applications. The state of the art is the attention-based encoder-decoder framework that learns the mapping between input images and output sequences in a purely data-driven way. However, we observe that existing attention-based methods perform poorly on complicated and or low-quality images. One major reason is that existing methods cannot get accurate alignments between feature areas and targets for such images. We call this phenomenon \"attention drift\". To tackle this problem, in this paper we propose the FAN (the abbreviation of Focusing Attention Network) method that employs a focusing attention mechanism to automatically draw back the drifted attention. FAN consists of two major components: an attention network (AN) that is responsible for recognizing character targets as in the existing methods, and a focusing network (FN) that is responsible for adjusting attention by evaluating whether AN pays attention properly on the target areas in the images. Furthermore, different from the existing methods, we adopt a ResNet-based network to enrich deep representations of scene text images. Extensive experiments on various benchmarks, including the IIIT5k, SVT and ICDAR datasets, show that the FAN method substantially outperforms the existing methods.",
        "related_work": "Though has been observed in attention training of speech recognition @cite_2 , where the authors proposed an MTL framework that combines CTC and AN to handle this issue, our paper is the first work that formally puts forward the concept of . Furthermore, we design a focus-mechanism to solve this problem. It is worth of noting that we have tried to use CTC and AN to solve the attention drift problem in scene text recognition, unfortunately our extensive experiments showed that this idea does not work well, so we discarded it.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "2952470929",
                "abstract": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the performance of the attention has shown poor results in noisy condition and is hard to learn in the initial training stage with long input sequences. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 5.4-14.6 relative improvements in Character Error Rate (CER).",
                "doi": "https://doi.org/10.48550/arxiv.1609.06773",
                "title": "Joint CTC-Attention based End-to-End Speech Recognition using Multi-task\n  Learning",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1709.01058",
        "mid": "2753613501",
        "abstract": "We propose a query-based generative model for solving both tasks of question generation (QG) and question an- swering (QA). The model follows the classic encoder- decoder framework. The encoder takes a passage and a query as input then performs query understanding by matching the query with the passage from multiple per- spectives. The decoder is an attention-based Long Short Term Memory (LSTM) model with copy and coverage mechanisms. In the QG task, a question is generated from the system given the passage and the target answer, whereas in the QA task, the answer is generated given the question and the passage. During the training stage, we leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias, a major prob- lem resulted from sequence learning with cross-entropy loss. For the QG task, our experiments show higher per- formances than the state-of-the-art results. When used as additional training data, the automatically generated questions even improve the performance of a strong ex- tractive QA system. In addition, our model shows bet- ter performance than the state-of-the-art baselines of the generative QA task.",
        "related_work": "For question generation (QG), our work extends previous work by performing query understanding. @cite_2 @cite_12 joins the QG task with the QA task, but they still conduct the QG task. The only difference is that they directly optimize the QA performance rather than a general metric (such as BLEU). On the other hand, our model can conduct both tasks of QG and QA.",
        "ref_abstract": {
            "@cite_12": {
                "mid": "2586581008",
                "abstract": "We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",
                "doi": "https://doi.org/10.48550/arxiv.1702.02206",
                "title": "Semi-Supervised QA with Generative Domain-Adaptive Nets",
                "publication_year": 2017
            },
            "@cite_2": {
                "mid": "2624022918",
                "abstract": "We study the problem of joint question answering (QA) and question generation (QG) in this paper. Our intuition is that QA and QG have intrinsic connections and these two tasks could improve each other. On one side, the QA model judges whether the generated question of a QG model is relevant to the answer. On the other side, the QG model provides the probability of generating a question given the answer, which is a useful evidence that in turn facilitates QA. In this paper we regard QA and QG as dual tasks. We propose a training framework that trains the models of QA and QG simultaneously, and explicitly leverages their probabilistic correlation to guide the training process of both models. We implement a QG model based on sequence-to-sequence learning, and a QA model based on recurrent neural network. As all the components of the QA and QG models are differentiable, all the parameters involved in these two models could be conventionally learned with back propagation. We conduct experiments on three datasets. Empirical results show that our training framework improves both QA and QG tasks. The improved QA model performs comparably with strong baseline approaches on all three datasets.",
                "doi": "https://doi.org/10.48550/arxiv.1706.02027",
                "title": "Question Answering and Question Generation as Dual Tasks",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1708.09251",
        "mid": "2702283731",
        "abstract": "The optimization of functions to find the best solution according to one or several objectives has a central role in many engineering and research fields. Recently, a new family of optimization algorithms, named Quality-Diversity optimization, has been introduced, and contrasts with classic algorithms. Instead of searching for a single solution, Quality-Diversity algorithms are searching for a large collection of both diverse and high-performing solutions. The role of this collection is to cover the range of possible solution types as much as possible, and to contain the best solution for each type. The contribution of this paper is threefold. Firstly, we present a unifying framework of Quality-Diversity optimization algorithms that covers the two main algorithms of this family (Multi-dimensional Archive of Phenotypic Elites and the Novelty Search with Local Competition), and that highlights the large variety of variants that can be investigated within this family. Secondly, we propose algorithms with a new selection mechanism for Quality-Diversity algorithms that outperforms all the algorithms tested in this paper. Lastly, we present a new collection management that overcomes the erosion issues observed when using unstructured collections. These three contributions are supported by extensive experimental comparisons of Quality-Diversity algorithms on three different experimental scenarios.",
        "related_work": "The main hypothesis behind this approach is that, in some cases, the optimal solutions cannot be found by simply maximizing the objective function. This is because the algorithm first needs to find stepping stones that are ineffective according to the objective function, but lead to promising solutions afterwards. A good illustration of this problem is the deceptive maze'' @cite_33 in which following the objective function inevitably leads to a dead-end (a local extremum). The algorithm has to investigate solutions that lead the agent further from the goal before being able to find solutions that actually solve the task.",
        "ref_abstract": {
            "@cite_33": {
                "mid": "2151083897",
                "abstract": "In evolutionary computation, the fitness function normally measures progress toward an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search toward dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution. Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.",
                "doi": "https://doi.org/10.1162/evco_a_00025",
                "title": "Abandoning Objectives: Evolution Through the Search for Novelty Alone",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1708.08705",
        "mid": "2752693045",
        "abstract": "The recently proposed multilayer convolutional sparse coding (ML-CSC) model, consisting of a cascade of convolutional sparse layers, provides a new interpretation of convolutional neural networks (CNNs). Under this framework, the forward pass in a CNN is equivalent to a pursuit algorithm aiming to estimate the nested sparse representation vectors from a given input signal. Despite having served as a pivotal connection between CNNs and sparse modeling, a deeper understanding of the ML-CSC is still lacking. In this paper, we propose a sound pursuit algorithm for the ML-CSC model by adopting a projection approach. We provide new and improved bounds on the stability of the solution of such pursuit and we analyze different practical alternatives to implement this in practice. We show that the training of the filters is essential to allow for nontrivial signals in the model, and we derive an online algorithm to learn the dictionaries from real data, effectively resulting in cascaded sparse convolutional layers. Last, but not least, we demonstrate the applicability of the ML-CSC model for several applications in an unsupervised setting, providing competitive results. Our work represents a bridge between matrix factorization, sparse dictionary learning, and sparse autoencoders, and we analyze these connections in detail.",
        "related_work": "Finally, and due to the fact that our formulation effectively provides a convolutional network with sparse kernels, our approach is reminiscent of works attempting to sparsify the filters in deep learning models. For instance, the work in @cite_30 showed that the weights of learned deep convolutional networks can be sparsified without considerable degradation of classification accuracy. Nevertheless, one should perpend the fact that these works are motivated merely by cheaper and faster implementations, whereas our model is intrinsically built by theoretically justified sparse kernels. We do not attempt to compare our approach to such sparsifying methods at this stage, and we defer this to future work.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "1935978687",
                "abstract": "Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a fine-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90 of parameters, with a drop of accuracy that is less than 1 on the ILSVRC2012 dataset. We also propose an efficient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efficiency than the off-the-shelf sparse matrix libraries, with a significant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve significant speedups.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298681",
                "title": "Sparse Convolutional Neural Networks",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1905.08955",
        "mid": "2945674404",
        "abstract": "Point cloud data from 3D LiDAR sensors are one of the most crucial sensor modalities for versatile safety-critical applications such as self-driving vehicles. Since the annotations of point cloud data is an expensive and time-consuming process, therefore recently the utilisation of simulated environments and 3D LiDAR sensors for this task started to get some popularity. With simulated sensors and environments, the process for obtaining an annotated synthetic point cloud data became much easier. However, the generated synthetic point cloud data are still missing the artefacts usually exist in point cloud data from real 3D LiDAR sensors. As a result, the performance of the trained models on this data for perception tasks when tested on real point cloud data is degraded due to the domain shift between simulated and real environments. Thus, in this work, we are proposing a domain adaptation framework for bridging this gap between synthetic and real point cloud data. Our proposed framework is based on the deep cycle-consistent generative adversarial networks (CycleGAN) architecture. We have evaluated the performance of our proposed framework on the task of vehicle detection from a bird's eye view (BEV) point cloud images coming from real 3D LiDAR sensors. The framework has shown competitive results with an improvement of more than 7 in average precision score over other baseline approaches when tested on real BEV point cloud images.",
        "related_work": "On the other hand, in @cite_17 proposed deep-learning based approach for thermal infra-red object tracking. To overcome the scarcity of thermal images dataset, they utilised DA based on the CycleGAN architecture to transform images from visual domain to the thermal infra-red domain.",
        "ref_abstract": {
            "@cite_17": {
                "mid": "2805902878",
                "abstract": "The usage of both off-the-shelf and end-to-end trained deep networks have significantly improved the performance of visual tracking on RGB videos. However, the lack of large labeled datasets hampers the usage of convolutional neural networks for tracking in thermal infrared (TIR) images. Therefore, most state-of-the-art methods on tracking for TIR data are still based on handcrafted features. To address this problem, we propose to use image-to-image translation models. These models allow us to translate the abundantly available labeled RGB data to synthetic TIR data. We explore both the usage of paired and unpaired image translation models for this purpose. These methods provide us with a large labeled dataset of synthetic TIR sequences, on which we can train end-to-end optimal features for tracking. To the best of our knowledge, we are the first to train end-to-end features for TIR tracking. We perform extensive experiments on the VOT-TIR2017 dataset. We show that a network trained on a large dataset of synthetic TIR data obtains better performance than one trained on the available real TIR data. Combining both data sources leads to further improvement. In addition, when we combine the network with motion features, we outperform the state of the art with a relative gain of over 10 , clearly showing the efficiency of using synthetic data to train end-to-end TIR trackers.",
                "doi": "https://doi.org/10.1109/tip.2018.2879249",
                "title": "Synthetic Data Generation for End-to-End Thermal Infrared Tracking",
                "publication_year": 2019
            }
        }
    },
    {
        "aid": "1905.08231",
        "mid": "2945951473",
        "abstract": "State-of-the-art 3D human pose estimation approaches typically estimate pose from the entire RGB image in a single forward run. In this paper, we develop a post-processing step to refine 3D human pose estimation from body part patches. Using local patches as input has two advantages. First, the fine details around body parts are zoomed in to high resolution for preciser 3D pose prediction. Second, it enables the part appearance to be shared between poses to benefit rare poses. In order to acquire informative representation of patches, we explore different input modalities and validate the superiority of fusing predicted segmentation with RGB. We show that our method consistently boosts the accuracy of state-of-the-art 3D human pose methods.",
        "related_work": "3D human pose estimation has basically been approached in two ways. The first way is to decompose the problem into two steps where the first step estimates 2D from RGB, and the second step lifts 2D to 3D. @cite_49 demonstrate very promising result with a simple multi-layer perceptron using 2D skelet al joints as the only input. In similar work, @cite_47 propose to estimate relative depth from skeleton label map @cite_15 . More recently, @cite_25 explore different input representations and establish a very solid system using color-encoded segmentation alone. The performance of these methods is limited, though, owing to the inherent depth ambiguity problem from 2D-3D lifting. @cite_37 argue that generating multiple hypotheses is more reasonable provided this fundamental depth ambiguity nature. We take inspiration from the representation in @cite_25 and merge it with original RGB cue.",
        "ref_abstract": {
            "@cite_37": {
                "mid": "2964291722",
                "abstract": "We propose a method to generate multiple diverse and valid human pose hypotheses in 3D all consistent with the 2D detection of joints in a monocular RGB image. We use a novel generative model uniform (unbiased) in the space of anatomically plausible 3D poses. Our model is compositional (produces a pose by combining parts) and since it is restricted only by anatomical constraints it can generalize to every plausible human 3D pose. Removing the model bias intrinsically helps to generate more diverse 3D pose hypotheses. We argue that generating multiple pose hypotheses is more reasonable than generating only a single 3D pose based on the 2D joint detection given the depth ambiguity and the uncertainty due to occlusion and imperfect 2D joint detection. We hope that the idea of generating multiple consistent pose hypotheses can give rise to a new line of future work that has not received much attention in the literature. We used the Human3.6M dataset for empirical evaluation.",
                "doi": "https://doi.org/10.1109/iccvw.2017.100",
                "title": "Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with 2D Joint Detections",
                "publication_year": 2017
            },
            "@cite_15": {
                "mid": "2750282596",
                "abstract": "Human pose estimation and semantic part segmentation are two complementary tasks in computer vision. In this paper, we propose to solve the two tasks jointly for natural multi-person images, in which the estimated pose provides object-level shape prior to regularize part segments while the part-level segments constrain the variation of pose locations. Specifically, we first train two fully convolutional neural networks (FCNs), namely Pose FCN and Part FCN, to provide initial estimation of pose joint potential and semantic part potential. Then, to refine pose joint location, the two types of potentials are fused with a fully-connected conditional random field (FCRF), where a novel segment-joint smoothness term is used to encourage semantic and spatial consistency between parts and joints. To refine part segments, the refined pose and the original part potential are integrated through a Part FCN, where the skeleton feature from pose serves as additional regularization cues for part segments. Finally, to reduce the complexity of the FCRF, we induce human detection boxes and infer the graph inside each box, making the inference forty times faster. Since there's no dataset that contains both part segments and pose labels, we extend the PASCAL VOC part dataset with human pose joints and perform extensive experiments to compare our method against several most recent strategies. We show that on this dataset our algorithm surpasses competing methods by a large margin in both tasks.",
                "doi": "https://doi.org/10.48550/arxiv.1708.03383",
                "title": "Joint Multi-Person Pose Estimation and Semantic Part Segmentation",
                "publication_year": 2017
            },
            "@cite_49": {
                "mid": "2612706635",
                "abstract": "Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3- dimensional positions.,,With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, \"lifting\" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feedforward network outperforms the best reported result by about 30 on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (i.e., using images as input) yields state of the art results \u2013 this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation.",
                "doi": "https://doi.org/10.1109/iccv.2017.288",
                "title": "A Simple Yet Effective Baseline for 3d Human Pose Estimation",
                "publication_year": 2017
            },
            "@cite_47": {
                "mid": "2774948913",
                "abstract": "Despite recent success on 2D human pose estimation, 3D human pose estimation still remains an open problem. A key challenge is the ill-posed depth ambiguity nature. This paper presents a novel intermediate feature representation named skeleton map for regression. It distills structural context from irrelavant properties of RGB image e.g. illumination and texture. It is simple, clean and can be easily generated via deconvolution network. For the first time, we show that training regression network from skeleton map alone is capable of meeting the performance of state-of-theart 3D human pose estimation works. We further exploit the power of multiple 3D hypothesis generation to obtain reasonbale 3D pose in consistent with 2D pose detection. The effectiveness of our approach is validated on challenging in-the-wild dataset MPII and indoor dataset Human3.6M.",
                "doi": "https://doi.org/10.48550/arxiv.1711.10796",
                "title": "DeepSkeleton: Skeleton Map for 3D Human Pose Regression",
                "publication_year": 2017
            },
            "@cite_25": {
                "mid": "2962754033",
                "abstract": "Direct prediction of 3D body pose and shape parameters remains a challenge even for highly parameterized, deep learning models. The representation of the prediction space is difficult to map to from the plain 2D image space, perspective ambiguities make the loss function noisy and training data is scarce. In this paper, we propose a novel approach (Neural Body Fitting (NBF)) that integrates a statistical body model as a layer within a CNN leveraging both reliable bottom-up body part segmentation and robust top-down body model constraints. NBF is fully differentiable and can be trained end-to-end from both 2D and 3D annotations. In detailed experiments we analyze how the components of our model improve model performance and present a robust, easy to use, end-to-end trainable framework for 3D human pose estimation from single 2D images.",
                "doi": "https://doi.org/10.1109/3dv.2018.00062",
                "title": "Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1905.05453",
        "mid": "2951195068",
        "abstract": "UAVs are increasingly being employed to carry out surveillance, parcel delivery, communication-support and other specific tasks. Their equipment and mission plan are carefully selected to minimize the carried load an overall resource consumption. Typically, several single task UAVs are dispatched to perform different missions. In certain cases, (part of) the geographical area of operation may be common to these single task missions (such as those supporting post-disaster recovery) and it may be more efficient to have multiple tasks carried out as part of a single UAV mission using common or even additional specialized equipment. In this paper, we propose and investigate a joint planning of multitask missions leveraging a fleet of UAVs equipped with a standard set of accessories enabling heterogeneous tasks. To this end, an optimization problem is formulated yielding the optimal joint planning and deriving the resulting quality of the delivered tasks. In addition, a heuristic solution is developed for large-scale environments to cope with the increased complexity of the optimization framework. The developed joint planning of multitask missions is applied to a specific post-disaster recovery scenario of a flooding in the San Francisco area. The results show the effectiveness of the proposed solutions and the potential savings in the number of UAVs needed to carry out all the tasks with the required level of quality.",
        "related_work": "Beside military and security operations, the usage of UAVs is envisioned in a plethora of civil applications, ranging from agriculture to environmental monitoring and disaster management (see @cite_19 for a thorough taxonomy and survey). In the following, we focus on the three types of tasks encompassed in the scenario under study.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "2792557859",
                "abstract": "Unmanned aerial vehicles (UAVs), or aerial drones, are an emerging technology with significant market potential. UAVs may lead to substantial cost savings in, for instance, monitoring of difficult-to-access infrastructure, spraying fields and performing surveillance in precision agriculture, as well as in deliveries of packages. In some applications, like disaster management, transport of medical supplies, or environmental monitoring, aerial drones may even help save lives. In this article, we provide a literature survey on optimization approaches to civil applications of UAVs. Our goal is to provide a fast point of entry into the topic for interested researchers and operations planning specialists. We describe the most promising aerial drone applications and outline characteristics of aerial drones relevant to operations planning. In this review of more than 200 articles, we provide insights into widespread and emerging modeling approaches. We conclude by suggesting promising directions for future research.",
                "doi": "https://doi.org/10.1002/net.21818",
                "title": "Optimization approaches for civil applications of unmanned aerial vehicles (UAVs) or aerial drones: A survey",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1905.05137",
        "mid": "2944645079",
        "abstract": "Adversarial attacks have been widely studied in the field of computer vision but their impact on network security applications remains an area of open research. As IoT, 5G and AI continue to converge to realize the promise of the fourth industrial revolution (Industry 4.0), security incidents and events on IoT networks have increased. Deep learning techniques are being applied to detect and mitigate many of such security threats against IoT networks. Feedforward Neural Networks (FNN) have been widely used for classifying intrusion attacks in IoT networks. In this paper, we consider a variant of the FNN known as the Self-normalizing Neural Network (SNN) and compare its performance with the FNN for classifying intrusion attacks in an IoT network. Our analysis is performed using the BoT-IoT dataset from the Cyber Range Lab of the center of UNSW Canberra Cyber. In our experimental results, the FNN outperforms the SNN for intrusion detection in IoT networks based on multiple performance metrics such as accuracy, precision, and recall as well as multi-classification metrics such as Cohen's Kappa score. However, when tested for adversarial robustness, the SNN demonstrates better resilience against the adversarial samples from the IoT dataset, presenting a promising future in the quest for safer and more secure deep learning in IoT networks.",
        "related_work": "While previous research @cite_4 have utilized deep learning techniques for intrusion detection in traditional networks, in this study, we extend this research area by specifically applying deep learning for intrusion detection in the context of IoT. We then demonstrate that deep learning models used for intrusion detection in IoT can be confused with adversarial samples.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2853623529",
                "abstract": "Deep neural networks have demonstrated their effectiveness in most machine learning tasks, with intrusion detection included. Unfortunately, recent research found that deep neural networks are vulnerable to adversarial examples in the image classification domain, i.e., they leave some opportunities for an attacker to fool the networks into misclassification by introducing imperceptible changes to the original pixels in an image. The vulnerability raises some concerns in applying deep neural networks in security-critical areas, such as intrusion detection. In this paper, we investigate the performances of the state-of-the-art attack algorithms against deep learning-based intrusion detection on the NSL-KDD data set. The vulnerabilities of neural networks employed by the intrusion detection systems are experimentally validated. The roles of individual features in generating adversarial examples are explored. Based on our findings, the feasibility and applicability of the attack methodologies are discussed.",
                "doi": "https://doi.org/10.1109/access.2018.2854599",
                "title": "Deep Learning-Based Intrusion Detection With Adversaries",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1905.04789",
        "mid": "2944349999",
        "abstract": "Recovery of articulated 3D structure from 2D observations is a challenging computer vision problem with many applications. Current learning-based approaches achieve state-of-the-art performance on public benchmarks but are limited to the specific types of objects and motions covered by the training datasets. Model-based approaches do not rely on training data but show lower accuracy on public benchmarks. In this paper, we introduce a new model-based method called Structure from Articulated Motion (SfAM). SfAM includes a new articulated structure term which ensures consistency of bone lengths throughout the whole image sequence and recovers a scene-specific configuration of the articulated structure. The proposed approach is highly robust to noisy 2D annotations, generalizes to arbitrary objects and motion types and does not rely on training data. It achieves state-of-the-art accuracy and scales across different scenarios which is shown in extensive experiments on public benchmarks and real video sequences.",
        "related_work": "Recently, many learning-based approaches for human pose and hand pose estimation have been presented in the literature @cite_37 @cite_31 @cite_81 @cite_26 @cite_4 @cite_16 @cite_39 . These methods are highly specialized and rely on large collections of training data. In contrast, our SfAM is a general approach which can cope with different articulated structures, with no need for labeled datasets.",
        "ref_abstract": {
            "@cite_37": {
                "mid": "2792747672",
                "abstract": "We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D poses of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our Localization-Classification-Regression architecture, named LCR-Net, contains 3 main components: 1) the pose proposal generator that suggests candidate poses at different locations in the image; 2) a classifier that scores the different pose proposals; and 3) a regressor that refines pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The final pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our method recovers full-body 2D and 3D poses, hallucinating plausible body parts when the persons are partially occluded or truncated by the image boundary. Our approach significantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the MPII 2D pose benchmark.",
                "doi": "https://doi.org/10.1109/tpami.2019.2892985",
                "title": "LCR-Net++: Multi-person 2D and 3D Pose Detection in Natural Images",
                "publication_year": 2019
            },
            "@cite_26": {
                "mid": "2557698284",
                "abstract": "This paper addresses the problem of 3D human pose estimation from a single image. We follow a standard two-step pipeline by first detecting the 2D position of the N body joints, and then using these observations to infer 3D pose. For the first step, we use a recent CNN-based detector. For the second step, most existing approaches perform 2N-to-3N regression of the Cartesian joint coordinates. We show that more precise pose estimates can be obtained by representing both the 2D and 3D human poses using NxN distance matrices, and formulating the problem as a 2D-to-3D distance matrix regression. For learning such a regressor we leverage on simple Neural Network architectures, which by construction, enforce positivity and symmetry of the predicted matrices. The approach has also the advantage to naturally handle missing observations and allowing to hypothesize the position of non-observed joints. Quantitative results on Humaneva and Human3.6M datasets demonstrate consistent performance gains over state-of-the-art. Qualitative evaluation on the images in-the-wild of the LSP dataset, using the regressor learned on Human3.6M, reveals very promising generalization results.",
                "doi": "https://doi.org/10.1109/cvpr.2017.170",
                "title": "3D Human Pose Estimation from a Single Image via Distance Matrix Regression",
                "publication_year": 2017
            },
            "@cite_4": {
                "mid": "2612706635",
                "abstract": "Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3- dimensional positions.,,With the goal of understanding these sources of error, we set out to build a system that given 2d joint locations predicts 3d positions. Much to our surprise, we have found that, with current technology, \"lifting\" ground truth 2d joint locations to 3d space is a task that can be solved with a remarkably low error rate: a relatively simple deep feedforward network outperforms the best reported result by about 30 on Human3.6M, the largest publicly available 3d pose estimation benchmark. Furthermore, training our system on the output of an off-the-shelf state-of-the-art 2d detector (i.e., using images as input) yields state of the art results \u2013 this includes an array of systems that have been trained end-to-end specifically for this task. Our results indicate that a large portion of the error of modern deep 3d pose estimation systems stems from their visual analysis, and suggests directions to further advance the state of the art in 3d human pose estimation.",
                "doi": "https://doi.org/10.1109/iccv.2017.288",
                "title": "A Simple Yet Effective Baseline for 3d Human Pose Estimation",
                "publication_year": 2017
            },
            "@cite_39": {
                "mid": "2771471422",
                "abstract": "Articulated hand pose estimation is a challenging task for human-computer interaction. The state-of-the-art hand pose estimation algorithms work only with one or a few subjects for which they have been calibrated or trained. Particularly, the hybrid methods based on learning followed by model fitting or model based deep learning do not explicitly consider varying hand shapes and sizes. In this work, we introduce a novel hybrid algorithm for estimating the 3D hand pose as well as bone-lengths of the hand skeleton at the same time, from a single depth image. The proposed CNN architecture learns hand pose parameters and scale parameters associated with the bone-lengths simultaneously. Subsequently, a new hybrid forward kinematics layer employs both parameters to estimate 3D joint positions of the hand. For end-to-end training, we combine three public datasets NYU, ICVL and MSRA-2015 in one unified format to achieve large variation in hand shapes and sizes. Among hybrid methods, our method shows improved accuracy over the state-of-the-art on the combined dataset and the ICVL dataset that contain multiple subjects. Also, our algorithm is demonstrated to work well with unseen images.",
                "doi": "https://doi.org/10.1109/3dv.2017.00069",
                "title": "Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a Single Depth Image",
                "publication_year": 2017
            },
            "@cite_81": {
                "mid": "2798646183",
                "abstract": "Our ability to train end-to-end systems for 3D human pose estimation from single images is currently constrained by the limited availability of 3D annotations for natural images. Most datasets are captured using Motion Capture (MoCap) systems in a studio setting and it is difficult to reach the variability of 2D human pose datasets, like MPII or LSP. To alleviate the need for accurate 3D ground truth, we propose to use a weaker supervision signal provided by the ordinal depths of human joints. This information can be acquired by human annotators for a wide range of images and poses. We showcase the effectiveness and flexibility of training Convolutional Networks (ConvNets) with these ordinal relations in different settings, always achieving competitive performance with ConvNets trained with accurate 3D joint coordinates. Additionally, to demonstrate the potential of the approach, we augment the popular LSP and MPII datasets with ordinal depth annotations. This extension allows us to present quantitative and qualitative evaluation in non-studio conditions. Simultaneously, these ordinal annotations can be easily incorporated in the training procedure of typical ConvNets for 3D human pose. Through this inclusion we achieve new state-of-the-art performance for the relevant benchmarks and validate the effectiveness of ordinal depth supervision for 3D human pose.",
                "doi": "https://doi.org/10.1109/cvpr.2018.00763",
                "title": "Ordinal Depth Supervision for 3D Human Pose Estimation",
                "publication_year": 2018
            },
            "@cite_31": {
                "mid": "2778680124",
                "abstract": "We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allow our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, reprojection loss alone is highly under constrained. In this work we address this problem by introducing an adversary trained to tell whether a human body parameter is real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any coupled 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detection and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimizationbased methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.",
                "doi": "https://doi.org/10.48550/arxiv.1712.06584",
                "title": "End-to-end Recovery of Human Shape and Pose",
                "publication_year": 2017
            },
            "@cite_16": {
                "mid": "2889146482",
                "abstract": "Articulated hand pose and shape estimation is an important problem for vision-based applications such as augmented reality and animation.In contrast to the existing methods which optimize only for joint positions, we propose a fully supervised deep network which learns to jointly estimate a full 3D hand mesh representation and pose from a single depth image.To this end, a CNN architecture is employed to estimate parametric representations i.e. hand pose, bone scales and complex shape parameters. Then, a novel hand pose and shape layer, embedded inside our deep framework, produces 3D joint positions and hand mesh. Lack of sufficient training data with varying hand shapes limits the generalized performance of learning based methods. Also, manually annotating real data is suboptimal. Therefore, we present SynHand5M: a million-scale synthetic benchmark with accurate joint annotations, segmentation masks and mesh files of depth maps. Among model based learning (hybrid) methods, we show improved results on two of the public benchmarks i.e. NYU and ICVL. Also, by employing a joint training strategy with real and synthetic data, we recover 3D hand mesh and pose from real images in 30ms.",
                "doi": "https://doi.org/10.1109/3dv.2018.00023",
                "title": "DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning from Synthetic Depth",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1905.03681",
        "mid": "2944426505",
        "abstract": "Reliable anticipation of pedestrian trajectory is imperative for the operation of autonomous vehicles and can significantly enhance the functionality of advanced driver assistance systems. While significant progress has been made in the field of pedestrian detection, forecasting pedestrian trajectories remains a challenging problem due to the unpredictable nature of pedestrians and the huge space of potentially useful features. In this work, we present a deep learning approach for pedestrian trajectory forecasting using a single vehicle-mounted camera. Deep learning models that have revolutionized other areas in computer vision have seen limited application to trajectory forecasting, in part due to the lack of richly annotated training data. We address the lack of training data by introducing a scalable machine annotation scheme that enables our model to be trained using a large dataset without human annotation. In addition, we propose Dynamic Trajectory Predictor (DTP), a model for forecasting pedestrian trajectory up to one second into the future. DTP is trained using both human and machine-annotated data, and anticipates dynamic motion that is not captured by linear models. Experimental evaluation confirms the benefits of the proposed model.",
        "related_work": "Our proposed approach builds on the substantial progress made in pedestrian detection and human action recognition. However, in this section, we concentrate on literature more directly relevant to our contributions, that are focused on (a) pedestrian trajectory forecasting and (b) alternative supervision methods for training models in the absence of large-scale human annotated datasets. For pedestrian detection, see recent surveys such as @cite_2 @cite_33 . For action recognition, see recent surveys such as @cite_5 @cite_34 .",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2963218601",
                "abstract": "Understanding human actions in visual data is tied to advances in complementary research areas including object recognition, human dynamics, domain adaptation and semantic segmentation. Over the last decade, human action analysis evolved from earlier schemes that are often limited to controlled environments to nowadays advanced solutions that can learn from millions of videos and apply to almost all daily activities. Given the broad range of applications from video surveillance to humancomputer interaction, scientific milestones in action recognition are achieved more rapidly, eventually leading to the demise of what used to be good in a short time. This motivated us to provide a comprehensive review of the notable steps taken towards recognizing human actions. To this end, we start our discussion with the pioneering methods that use handcrafted representations, and then, navigate into the realm of deep learning based approaches. We aim to remain objective throughout this survey, touching upon encouraging improvements as well as inevitable fallbacks, in the hope of raising fresh questions and motivating new research directions for the reader. We provide a detailed review of the work on human action recognition over the past decade.We refer to actions as meaningful human motions.Including Hand-crafted representations methods, we review the impact of Deep-nets on action recognition.We follow a systematic taxonomy to highlight the essence of both Hand-crafted and Deep-net solutions.We present a comparison of methods at their algorithmic level and performance.",
                "doi": "https://doi.org/10.1016/j.imavis.2017.01.010",
                "title": "Going deeper into action recognition: A survey",
                "publication_year": 2017
            },
            "@cite_34": {
                "mid": "2810685774",
                "abstract": "Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in the action recognition and prediction. Existing models, popular algorithms, technical difficulties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.",
                "doi": "https://doi.org/10.1007/s11263-022-01594-9",
                "title": "Human Action Recognition and Prediction: A Survey",
                "publication_year": 2022
            },
            "@cite_33": {
                "mid": "2610165754",
                "abstract": "Encouraged by the recent progress in pedestrian detection, we investigate the gap between current state-of-the-art methods and the \u201cperfect single frame detector\u201d. We enable our analysis by creating a human baseline for pedestrian detection (over the Caltech pedestrian dataset). After manually clustering the frequent errors of a top detector, we characterise both localisation and background-versus-foreground errors. To address localisation errors we study the impact of training annotation noise on the detector performance, and show that we can improve results even with a small portion of sanitised training data. To address background foreground discrimination, we study convnets for pedestrian detection, and discuss which factors affect their performance. Other than our in-depth analysis, we report top performance on the Caltech pedestrian dataset, and provide a new sanitised set of training and test annotations.",
                "doi": "https://doi.org/10.1109/tpami.2017.2700460",
                "title": "Towards Reaching Human Performance in Pedestrian Detection",
                "publication_year": 2018
            },
            "@cite_2": {
                "mid": "1650122911",
                "abstract": "Paper-by-paper results make it easy to miss the forest for the trees.We analyse the remarkable progress of the last decade by dis- cussing the main ideas explored in the 40+ detectors currently present in the Caltech pedestrian detection benchmark. We observe that there exist three families of approaches, all currently reaching similar detec- tion quality. Based on our analysis, we study the complementarity of the most promising ideas by combining multiple published strategies. This new decision forest detector achieves the current best known performance on the challenging Caltech-USA dataset.",
                "doi": "https://doi.org/10.1007/978-3-319-16181-5_47",
                "title": "Ten Years of Pedestrian Detection, What Have We Learned?",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1905.02870",
        "mid": "2947637691",
        "abstract": "We propose a novel causal coding scheme with forward error correction (FEC) for a point-to-point communication link with delayed feedback. The proposed model can learn the erasure pattern in the channel, and adaptively adjust its transmission and FEC rate based on the burstiness of the channel and the feedback. We investigate the throughput, and the in-order delivery delay of the adaptive causal coding algorithm, and contrast its performance with the one of the selective repeat (SR) ARQ. We demonstrate via an experimental study of the protocol that our model can double the throughput gains, and triple the gain in terms of mean in-order delivery delay when the channel is bursty, while keeping the difference between the maximum and mean in-order delivery delay is much smaller than SR ARQ. Closing the delay gap along with boosting the throughput is very promising for enabling ultra-reliable low-latency communications applications. We validate the performance of data delivery under the traces of Intel.",
        "related_work": "Systematic codes have been proposed in @cite_6 , which are coded generalizations of selective repeat ARQ, and the adaptive coded ARQ model with cumulative feedback as in @cite_5 .",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2962717093",
                "abstract": "Future 5G systems will need to support ultra-reliable low-latency communications scenarios. From a latency-reliability viewpoint, it is inefficient to rely on average utility-based system design. Therefore, we introduce the notion of guaranteeable delay which is the average delay plus three standard deviations of the mean. We investigate the trade-off between guaranteeable delay and throughput for the point-to-point wireless erasure links with unreliable and delayed feedback, by bringing together signal flow techniques to the area of coding. We use tiny codes, i.e., sliding window by coding with just 2 packets, and design three variations of selective-repeat ARQ protocols, by building on the baseline scheme, i.e., uncoded ARQ, developed by Ausavapattanakun and Nosratinia: (i) Hybrid ARQ with soft combining at the receiver; (ii) cumulative feedback-based ARQ without rate adaptation; and (iii) coded ARQ with rate adaptation based on the cumulative feedback. Contrasting the performance of these protocols with uncoded ARQ, we demonstrate that the HARQ performs only slightly better, the cumulative feedback-based ARQ does not provide significant throughput while it has a better average delay, and the Coded ARQ can provide gains up to about 40 in terms of throughput. The Coded ARQ also provides delay guarantees, and is robust to various challenges such as imperfect and delayed feedback, burst erasures, and round-trip time fluctuations. This feature may be preferable for meeting the strict end-to-end latency and reliability requirements of the future use cases of ultra-reliable low-latency communications in 5G, such as mission-critical communications and industrial control for critical control messaging.",
                "doi": "https://doi.org/10.1109/jsac.2019.2898747",
                "title": "Tiny Codes for Guaranteeable Delay",
                "publication_year": 2019
            },
            "@cite_6": {
                "mid": "1568554573",
                "abstract": "Reducing the in-order delivery, or playback, delay of reliable transport layer protocols over error prone networks can significantly improve application layer performance. This is especially true for applications that have time sensitive constraints such as streaming services. We explore the benefits of a coded generalization of selective repeat ARQ for minimizing the in-order delivery delay. An analysis of the delay's first two moments is provided so that we can determine when and how much redundancy should be added to meet a user's requirements. Numerical results help show the gains over selective repeat ARQ, as well as the trade-offs between meeting the user's delay constraints and the costs inflicted on the achievable rate. Finally, the analysis is compared with experimental results to help illustrate how our work can be used to help inform system decisions.",
                "doi": "https://doi.org/10.1109/infocom.2015.7218601",
                "title": "A coded generalization of selective repeat ARQ",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1905.01991",
        "mid": "2943108019",
        "abstract": "Email has remained a principal form of communication among people, both in enterprise and social settings. With a deluge of emails crowding our mailboxes daily, there is a dire need of smart email systems that can recover important emails and make personalized recommendations. In this work, we study the problem of predicting user triage actions to incoming emails where we take the reply prediction as a working example. Different from existing methods, we formulate the triage action prediction as a recommendation problem and focus on the content-based approach, where the users are represented using the content of current and past emails. We also introduce additional similarity features to further explore the affinities between users and emails. Experiments on the publicly available Avocado email collection demonstrate the advantages of our proposed recommendation framework and our method is able to achieve better performance compared to the state-of-the-art deep recommendation methods. More importantly, we provide valuable insight into the effectiveness of different textual and user representations and show that traditional bag-of-words approaches, with the help from the similarity features, compete favorably with the more advanced neural embedding methods.",
        "related_work": "Personalized email prioritization through collaborative filtering has also been studied for broadcast emails @cite_13 , where the prediction is based on the feedback from a small subset of recipients of the same email. Despite showing great performance, the problem scope considered is rather limited. It can only be applied to emails sent to large number of recipients and can not perform the prediction on delivery time since it needs to wait for responses from some subset of the recipients. Therefore, it can not be applied to the general prediction problem including predictions for emails sent to limited number of recipients and time-sensitive situations.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "2339136142",
                "abstract": "Email is one of the most important communication tools today, but email overload resulting from the large number of unimportant or irrelevant emails is causing trillion-level economy loss every year. Thus personalized email prioritization algorithms are of urgent need. Despite lots of previous effort on this topic, broadcast email, an important type of email, is overlooked in previous literature. Broadcast emails are significantly different from normal emails, introducing both new challenges and opportunities. On one hand, lack of real senders and limited user interactions invalidate the key features exploited by traditional email prioritization algorithms; on the other hand, thousands of receivers for one broadcast email bring us the opportunity to predict importance through collaborative filtering. However, broadcast emails face a severe cold-start problem which hinders the direct application of collaborative filtering. In this paper, we propose the first framework for broadcast email prioritization by designing a novel active learning model that considers the collaborative filtering, implicit feedback and time sensitive responsiveness features of broadcast emails. Our method is thoroughly evaluated on a large scale real world industrial dataset from Samsung Electronics. Our method is proved highly effective and outperforms state-of-the-art personalized email prioritization methods.",
                "doi": "https://doi.org/10.1145/2872427.2883049",
                "title": "Which to View",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1904.09273",
        "mid": "2939608634",
        "abstract": "By their nature, the composition of black box models is opaque. This makes the ability to generate explanations for the response to stimuli challenging. The importance of explaining black box models has become increasingly important given the prevalence of AI and ML systems and the need to build legal and regulatory frameworks around them. Such explanations can also increase trust in these uncertain systems. In our paper we present RICE, a method for generating explanations of the behaviour of black box models by (1) probing a model to extract model output examples using sensitivity analysis; (2) applying CNPInduce, a method for inductive logic program synthesis, to generate logic programs based on critical input-output pairs; and (3) interpreting the target program as a human-readable explanation. We demonstrate the application of our method by generating explanations of an artificial neural network trained to follow simple traffic rules in a hypothetical self-driving car simulation. We conclude with a discussion on the scalability and usability of our approach and its potential applications to explanation-critical scenarios.",
        "related_work": "Interpretable models already exist. Of note, there are a number of classification models that are readily interpretable, for example decision trees, rule-based systems, and nearest-neighbour methods, each with varying levels of usability @cite_23 . Decision trees generate a tree-like structure representing a series of tests on different features in a training dataset where leaf nodes represent various labeled classifications. Rules-based systems explicitly map an input to an action through some explicitly defined series of logical assertions ( rules). Nearest-neighbour algorithms qualify a classification based on the values of attributes in the immediate neighbourhood of some input.",
        "ref_abstract": {
            "@cite_23": {
                "mid": "2026905436",
                "abstract": "The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.",
                "doi": "https://doi.org/10.1145/2594473.2594475",
                "title": "Comprehensible classification models",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1904.07002",
        "mid": "2939738131",
        "abstract": "Synthesising 3D facial motion from speech is a crucial problem manifesting in a multitude of applications such as computer games and movies. Recently proposed methods tackle this problem in controlled conditions of speech. In this paper, we introduce the first methodology for 3D facial motion synthesis from speech captured in arbitrary recording conditions (\"in-the-wild\") and independent of the speaker. For our purposes, we captured 4D sequences of people uttering 500 words, contained in the Lip Reading Words (LRW) a publicly available large-scale in-the-wild dataset, and built a set of 3D blendshapes appropriate for speech. We correlate the 3D shape parameters of the speech blendshapes to the LRW audio samples by means of a novel time-warping technique, named Deep Canonical Attentional Warping (DCAW), that can simultaneously learn hierarchical non-linear representations and a warping path in an end-to-end manner. We thoroughly evaluate our proposed methods, and show the ability of a deep learning model to synthesise 3D facial motion in handling different speakers and continuous speech signals in uncontrolled conditions.",
        "related_work": ". Language-based methods take advantage of the mapping between phonemes and their visual counterpart visemes. For example, @cite_34 proposed the JAw and LIp (JALI) model, a two-dimensional space that represents the jaw and lip movements of a facial animation based on psycholinguistic considerations. The main disadvantage of their study is the need for the speech signal, its text transcript and their alignment to create the facial animation. In another study, @cite_18 first proposed generating dynamic units for visual speech for realistic visual speech animation. In a more recent study, the authors @cite_27 initially transcribe the speech signal to phoneme labels, which are then fed to a deep fully connected network to predict person-specific shape and appearance parameters obtained by Active Appearance Model (AAM). A main limitation of this work is the need for speech to phoneme labels conversion.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2737658251",
                "abstract": "We introduce a simple and effective deep learning approach to automatically generate natural looking speech animation that synchronizes to input speech. Our approach uses a sliding window predictor that learns arbitrary nonlinear mappings from phoneme label input sequences to mouth movements in a way that accurately captures natural motion and visual coarticulation effects. Our deep learning approach enjoys several attractive properties: it runs in real-time, requires minimal parameter tuning, generalizes well to novel input speech sequences, is easily edited to create stylized and emotional speech, and is compatible with existing animation retargeting approaches. One important focus of our work is to develop an effective approach for speech animation that can be easily integrated into existing production pipelines. We provide a detailed description of our end-to-end approach, including machine learning design decisions. Generalized speech animation results are demonstrated over a wide range of animation clips on a variety of characters and voices, including singing and foreign language input. Our approach can also generate on-demand speech animation in real-time from user speech input.",
                "doi": "https://doi.org/10.1145/3072959.3073699",
                "title": "A deep learning approach for generalized speech animation",
                "publication_year": 2017
            },
            "@cite_18": {
                "mid": "2000911139",
                "abstract": "We present a new method for generating a dynamic, concatenative, unit of visual speech that can generate realistic visual speech animation. We redefine visemes as temporal units that describe distinctive speech movements of the visual speech articulators. Traditionally visemes have been surmized as the set of static mouth shapes representing clusters of contrastive phonemes (e.g. p, b, m , and f, v ). In this work, the motion of the visual speech articulators are used to generate discrete, dynamic visual speech gestures. These gestures are clustered, providing a finite set of movements that describe visual speech, the visemes. Dynamic visemes are applied to speech animation by simply concatenating viseme units. We compare to static visemes using subjective evaluation. We find that dynamic visemes are able to produce more accurate and visually pleasing speech animation given phonetically annotated audio, reducing the amount of time that an animator needs to spend manually refining the animation.",
                "doi": "https://doi.org/10.5555/2422356.2422395",
                "title": "Dynamic units of visual speech",
                "publication_year": 2012
            },
            "@cite_34": {
                "mid": "2468212864",
                "abstract": "The rich signals we extract from facial expressions imposes high expectations for the science and art of facial animation. While the advent of high-resolution performance capture has greatly improved realism, the utility of procedural animation warrants a prominent place in facial animation workflow. We present a system that, given an input audio soundtrack and speech transcript, automatically generates expressive lip-synchronized facial animation that is amenable to further artistic refinement, and that is comparable with both performance capture and professional animator output. Because of the diversity of ways we produce sound, the mapping from phonemes to visual depictions as visemes is many-valued. We draw from psycholinguistics to capture this variation using two visually distinct anatomical actions: Jaw and Lip, wheresound is primarily controlled by jaw articulation and lower-face muscles, respectively. We describe the construction of a transferable template jali 3D facial rig, built upon the popular facial muscle action unit representation facs. We show that acoustic properties in a speech signal map naturally to the dynamic degree of jaw and lip in visual speech. We provide an array of compelling animation clips, compare against performance capture and existing procedural animation, and report on a brief user study.",
                "doi": "https://doi.org/10.1145/2897824.2925984",
                "title": "JALI",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1904.04971",
        "mid": "2938497181",
        "abstract": "Conditional computation aims to increase the size and accuracy of a network, at a small increase in inference cost. Previous hard-routing models explicitly route the input to a subset of experts. We propose soft conditional computation, which, in contrast, utilizes all experts while still permitting efficient inference through parameter routing. Concretely, for a given convolutional layer, we wish to compute a linear combination of @math experts @math , where @math are functions of the input learned through gradient descent. A straightforward evaluation requires @math convolutions. We propose an equivalent form of the above computation, @math , which requires only a single convolution. We demonstrate the efficacy of our method, named CondConv, by scaling up the MobileNetV1, MobileNetV2, and ResNet-50 model architectures to achieve higher accuracy while retaining efficient inference. On the ImageNet classification dataset, CondConv improves the top-1 validation accuracy of the MobileNetV1(0.5x) model from 63.8 to 71.6 while only increasing inference cost by 27 . On COCO object detection, CondConv improves the minival mAP of a MobileNetV1(1.0x) SSD model from 20.3 to 22.4 with just a 4 increase in inference cost.",
        "related_work": "Hydranets @cite_13 is a hard-routing approach for vision that can also be trained with gradient descent, but requires an unsupervised clustering-based method to partition examples to perform well. Increasing the number of experts evaluated at inference time improves performance of Hydranets, but results in increased inference cost. does not require auxiliary loss functions for learning the routing functions, and allows for the use of all experts at a small inference cost.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "2798722023",
                "abstract": "There is growing interest in improving the design of deep network architectures to be both accurate and low cost. This paper explores semantic specialization as a mechanism for improving the computational efficiency (accuracy-per-unit-cost) of inference in the context of image classification. Specifically, we propose a network architecture template called HydraNet, which enables state-of-the-art architectures for image classification to be transformed into dynamic architectures which exploit conditional execution for efficient inference. HydraNets are wide networks containing distinct components specialized to compute features for visually similar classes, but they retain efficiency by dynamically selecting only a small number of components to evaluate for any one input image. This design is made possible by a soft gating mechanism that encourages component specialization during training and accurately performs component selection during inference. We evaluate the HydraNet approach on both the CIFAR-100 and ImageNet classification tasks. On CIFAR, applying the HydraNet template to the ResNet and DenseNet family of models reduces inference cost by 2-4A\u2014 while retaining the accuracy of the baseline architectures. On ImageNet, applying the HydraNet template improves accuracy up to 2.5 when compared to an efficient baseline architecture with similar inference cost.",
                "doi": "https://doi.org/10.1109/cvpr.2018.00843",
                "title": "HydraNets: Specialized Dynamic Architectures for Efficient Inference",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1904.04542",
        "mid": "2938579232",
        "abstract": "In this paper, we propose RECOUP, a reliable group communication routing protocol for IoT networks. RECOUP efficiently uses a low-overhead cluster-based multicast routing technique on top of the RPL protocol. RECOUP increases the probability of message delivery to the intended destination(s), irrespective of the network size and faults (such as broken links or non-responsive nodes), and in the presence of misbehaving nodes.We show that the cluster-based routing mechanism of RECOUP remains robust in presence of various topology (i.e., rank and sybil) and data communication (i.e., blackhole, wormhole, and jamming) attacks targeting the IoT networking infrastructure.An implementation of RECOUP is realized in Contiki. Our results show the effectiveness of RECOUP over state-of-art protocols concerning packet delivery ratio to 25 , end-to-end delay down to 100 ms, low radio transmissions required for per packet delivery to 6 mJ, and most importantly, it improves the robustness and scalability of data communication process in thewhole network.",
        "related_work": "As RPL or an extension of RPL are the most used routing protocols in IoT networks. We now briefly discuss the security challenges that these protocols might face during the routing process. Authors in @cite_1 propose a sinkhole attack mitigation method that integrates rank authentication with parent fail-over. The proposal uses DIO message along with the one way hash function technique for rank authentication. The root node generates hash value by selecting random numbers, and broadcast these values through DIO messages. When the root node again broadcast the initially selected random number securely then intermediate nodes can verify its parent rank using the intermediate hops number. @cite_5 authors propose a Merkle tree authentication based solution which can be used to prevent wormhole attack on RPL protocol. In this proposal, the RPL tree is formed in the reverse direction by using the node ID and public key which are used to calculate the hash values. After the Merkle tree formation, the authentication for any node starts from the root node and if any intermediate node fails to authenticate, a possible wormhole is detected.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "1999949961",
                "abstract": "Smart metering application has received a lot of attention from the research community lately. Usually, LLN based network runs on RPL protocol which constructs a DAG structure for its normal operation. In this paper, we devise a wormhole attack scenario in such a network. Furthermore, we propose a Merkle tree based authentication protocol which runs on the notion of constructing a tree of hashed security information. We evaluated the approach by formulating the wormhole problem as graph theoretic problem and have shown the effectiveness of the proposed Merkle tree based approach for authenticating communications. Furthermore, we perform simulations in NS 2 to observe the network performance by adopting Merkle tree based to prevent from disrupting the links and observed boost in throughput, reduction in jitter and end to end delay. In the end, we have taken the step towards optimizing the performance of the algorithm by proposing an effective tree traversal algorithm which works on the notion of electing nodes as root in a large scale network from where a Merkle tree will be originated this will assist in managing the authentication in a huge sized network broken down into many trees avoiding wormhole attacks in the network.",
                "doi": "https://doi.org/10.1109/icufn.2013.6614801",
                "title": "Wormhole attack prevention mechanism for RPL based LLN network",
                "publication_year": 2013
            },
            "@cite_1": {
                "mid": "2023532542",
                "abstract": "In this work, we present the results of a study on the detrimental effects of sinkhole attacks on Wireless Sensor Networks (WSNs) which employ the Routing Protocol for LLNs (Low-power and Lossy Networks). A sinkhole is a compromised node which attempts to capture traffic with the intent to drop messages, thus degrading the end-to-end delivery performance, that is, reducing the number of messages successfully delivered to their destination. The mechanism by which the sinkhole captures traffic is by advertising an attractive route to its neighbors. We evaluate two countermeasures addressing the sinkhole problem: a parent fail-over and a rank authentication technique. We show via simulation that while each technique, applied alone, does not work all that well, the combination of the two techniques significantly improves the performance of a network under attack. We also demonstrate that, with the defenses described, increasing the density of the network can combat a penetration of sinkholes nodes, without needing to identify the sinkholes.",
                "doi": "https://doi.org/10.1109/icnp.2012.6459948",
                "title": "Evaluating sinkhole defense techniques in RPL networks",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1904.01367",
        "mid": "2927655437",
        "abstract": "Residual connections significantly boost the performance of deep neural networks. However, there are few theoretical results that address the influence of residuals on the hypothesis complexity and the generalization ability of deep neural networks. This paper studies the influence of residual connections on the hypothesis complexity of the neural network in terms of the covering number of its hypothesis space. We prove that the upper bound of the covering number is the same as chain-like neural networks, if the total numbers of the weight matrices and nonlinearities are fixed, no matter whether they are in the residuals or not. This result demonstrates that residual connections may not increase the hypothesis complexity of the neural network compared with the chain-like counterpart. Based on the upper bound of the covering number, we then obtain an @math margin-based multi-class generalization bound for ResNet, as an exemplary case of any deep neural network with residual connections. Generalization guarantees for similar state-of-the-art neural network architectures, such as DenseNet and ResNeXt, are straight-forward. From our generalization bound, a practical implementation is summarized: to approach a good generalization ability, we need to use regularization terms to control the magnitude of the norms of weight matrices not to increase too much, which justifies the standard technique of weight decay.",
        "related_work": "conduct systematic experiments to explore the generalization ability of deep neural networks @cite_43 . They show that neural networks can almost perfectly fit the training data even when the training labels are random. This paper attracts the community of learning theory to the important topic that how to theoretically interpret the success of deep neural networks.",
        "ref_abstract": {
            "@cite_43": {
                "mid": "2566079294",
                "abstract": "Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. @PARASPLIT Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. @PARASPLIT We interpret our experimental findings by comparison with traditional models.",
                "doi": "https://doi.org/10.1145/3446776",
                "title": "Understanding deep learning (still) requires rethinking generalization",
                "publication_year": 2021
            }
        }
    },
    {
        "aid": "1903.07013",
        "mid": "2921897330",
        "abstract": "Whole Slide Imaging (WSI) has become an important topic during the last decade. Even though significant progress in both medical image processing and computational resources has been achieved, there are still problems in WSI that need to be solved. A major challenge is the scan size. The dimensions of digitized tissue samples may exceed 100,000 by 100,000 pixels causing memory and efficiency obstacles for real-time processing. The main contribution of this work is representing a WSI by selecting a small number of patches for algorithmic processing (e.g., indexing and search). As a result, we reduced the search time and storage by various factors between ( @math ), while losing only a few percentages in the patch retrieval accuracy. A self-organizing map (SOM) has been applied on local binary patterns (LBP) and deep features of the KimiaPath24 dataset in order to cluster patches that share the same characteristics. We used a Gaussian mixture model (GMM) to represent each class with a rather small ( @math ) portion of patches. The results showed that LBP features can outperform deep features. By selecting only @math of all patches after SOM clustering and GMM patch selection, we received @math accuracy for retrieval of the best match, while the maximum accuracy (using all patches) was @math .",
        "related_work": "Tissue examination under a microscope reveals important information to render accurate diagnosis and thus, provide effective treatment for different diseases @cite_19 . DP offers several opportunities and also presents challenges to the image processing architectures @cite_9 . Presently, only a small fraction of glass slides are digitized @cite_5 , but even if WSI was more widely available, there are a number of technical issues that would need to be addressed for their effective usage. One of the main challenges is data management and storage @cite_19 . Most importantly, the large dimensions of the WSI files require a large amount of memory and a expensive computational power.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "1974467617",
                "abstract": "Pathology is a medical subspecialty that practices the diagnosis of disease. Microscopic examination of tissue reveals information enabling the pathologist to render accurate diagnoses and to guide therapy. The basic process by which anatomic pathologists render diagnoses has remained relatively unchanged over the last century, yet advances in information technology now offer significant opportunities in image-based diagnostic and research applications. Pathology has lagged behind other healthcare practices such as radiology where digital adoption is widespread. As devices that generate whole slide images become more practical and affordable, practices will increasingly adopt this technology and eventually produce an explosion of data that will quickly eclipse the already vast quantities of radiology imaging data. These advances are accompanied by significant challenges for data management and storage, but they also introduce new opportunities to improve patient care by streamlining and standardizing diagnostic approaches and uncovering disease mechanisms. Computer-based image analysis is already available in commercial diagnostic systems, but further advances in image analysis algorithms are warranted in order to fully realize the benefits of digital pathology in medical discovery and patient care. In coming decades, pathology image analysis will extend beyond the streamlining of diagnostic workflows and minimizing interobserver variability and will begin to provide diagnostic assistance, identify therapeutic targets, and predict patient outcomes and therapeutic responses.",
                "doi": "https://doi.org/10.1109/jproc.2011.2182074",
                "title": "Digital Pathology: Data-Intensive Frontier in Medical Imaging",
                "publication_year": 2012
            },
            "@cite_9": {
                "mid": "2470965540",
                "abstract": "Abstract With the rise in whole slide scanner technology, large numbers of tissue slides are being scanned and represented and archived digitally. While digital pathology has substantial implications for telepathology, second opinions, and education there are also huge research opportunities in image computing with this new source of \u201cbig data\u201d. It is well known that there is fundamental prognostic data embedded in pathology images. The ability to mine \u201csub-visual\u201d image features from digital pathology slide images, features that may not be visually discernible by a pathologist, offers the opportunity for better quantitative modeling of disease appearance and hence possibly improved prediction of disease aggressiveness and patient outcome. However the compelling opportunities in precision medicine offered by big digital pathology data come with their own set of computational challenges. Image analysis and computer assisted detection and diagnosis tools previously developed in the context of radiographic images are woefully inadequate to deal with the data density in high resolution digitized whole slide images. Additionally there has been recent substantial interest in combining and fusing radiologic imaging and proteomics and genomics based measurements with features extracted from digital pathology images for better prognostic prediction of disease aggressiveness and patient outcome. Again there is a paucity of powerful tools for combining disease specific features that manifest across multiple different length scales. The purpose of this review is to discuss developments in computational image analysis tools for predictive modeling of digital pathology images from a detection, segmentation, feature extraction, and tissue classification perspective. We discuss the emergence of new handcrafted feature approaches for improved predictive modeling of tissue appearance and also review the emergence of deep learning schemes for both object detection and tissue classification. We also briefly review some of the state of the art in fusion of radiology and pathology images and also combining digital pathology derived image measurements with molecular \u201comics\u201d features for better predictive modeling. The review ends with a brief discussion of some of the technical and computational challenges to be overcome and reflects on future opportunities for the quantitation of histopathology.",
                "doi": "https://doi.org/10.1016/j.media.2016.06.037",
                "title": "Image analysis and machine learning in digital pathology: Challenges and opportunities",
                "publication_year": 2016
            },
            "@cite_5": {
                "mid": "2088887361",
                "abstract": "Digital whole slide imaging (WSI) is an emerging technology for pathology interpretation; however, little is known about pathologists\u2019 practice patterns or perceptions regarding WSI. A national sample (N = 252) of pathologists from New Hampshire, Vermont, Washington, Oregon, Arizona, Alaska, Maine, and Minnesota were surveyed in this cross-sectional study (2011\u20132013). The survey included questions on pathologists\u2019 experience, WSI practice patterns, and perceptions using a six-point Likert scale. Agreement was summarized with descriptive statistics to characterize pathologists\u2019 use and perceptions of WSI. The majority of participating pathologists were males (63 ) between 40 and 59 years of age (70 ) and not affiliated with an academic medical center (72 ). Experience with WSI was reported by 49 . Types of use reported included CME board exams teaching (28 ), tumor board clinical conference (22 ), archival purposes (6 ), consultative diagnosis (4 ), research (4 ), and other uses (12 ). Most respondents (79 ) agreed that accurate diagnoses can be made with this technology, and that WSI is useful for obtaining a second opinion (88 ). However, 78 of pathologists agreed that digital slides are too slow for routine clinical interpretation. Fifty-nine percent agreed that the benefits of WSI outweigh concerns. The respondents were equally split as to whether they would like to adopt WSI (51 ) or not (49 ). About half of pathologists reported experience with the WSI technology, largely for CME, licensure board exams, and teaching. Positive perceptions regarding WSI slightly outweigh negative perceptions. Understanding practice patterns with WSI as dissemination advances may facilitate concordance of perceptions with adoption of the technology.",
                "doi": "https://doi.org/10.1007/s10278-014-9683-2",
                "title": "Digitized Whole Slides for Breast Pathology Interpretation: Current Practices and Perceptions",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1903.06607",
        "mid": "2922306113",
        "abstract": "This paper explores the problem of matching entities across different knowledge graphs. Given a query entity in one knowledge graph, we wish to find the corresponding real-world entity in another knowledge graph. We formalize this problem and present two large-scale datasets for this task based on exiting cross-ontology links between DBpedia and Wikidata, focused on several hundred thousand ambiguous entities. Using a classification-based approach, we find that a simple multi-layered perceptron based on representations derived from RDF2Vec graph embeddings of entities in each knowledge graph is sufficient to achieve high accuracy, with only small amounts of training data. The contributions of our work are datasets for examining this problem and strong baselines on which future work can be based.",
        "related_work": "Research related to knowledge graph integration comes from the database community and focuses on ontology matching---referred to as record-linkage, entity resolution, or deduplication. Examples include @cite_3 , @cite_8 , and the work of . The primary difference between this work and ours is that they assume relational structure and that the tables to be matched have been already aligned using schema matching techniques. These systems cannot be directly applied to entity matching across knowledge graphs due to differences in structure between the relational model and the RDF model.",
        "ref_abstract": {
            "@cite_3": {
                "mid": "2542998387",
                "abstract": "Entity matching (EM) has been a long-standing challenge in data management. Most current EM works focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present as a solution Magellan, a new kind of EM systems. Magellan is novel in four important aspects. (1) It provides how-to guides that tell users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data analysis and Big Data stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provides a powerful scripting environment to facilitate interactive experimentation and quick \"patching\" of the system. We describe research challenges raised by Magellan, then present extensive experiments with 44 students and users at several organizations that show the promise of the Magellan approach.",
                "doi": "https://doi.org/10.14778/2994509.2994535",
                "title": "Magellan",
                "publication_year": 2016
            },
            "@cite_8": {
                "mid": "2957204582",
                "abstract": "Despite the efforts in 70+ years in all aspects of entity resolution (ER), there is still a high demand for democratizing ER - by reducing the heavy human involvement in labeling data, performing feature engineering, tuning parameters, and defining blocking functions. With the recent advances in deep learning, in particular distributed representations of words (a.k.a. word embeddings), we present a novel ER system, called DeepER, that achieves good accuracy, high efficiency, as well as ease-of-use (i.e., much less human efforts). We use sophisticated composition methods, namely uni- and bi-directional recurrent neural networks (RNNs) with long short term memory (LSTM) hidden units, to convert each tuple to a distributed representation (i.e., a vector), which can in turn be used to effectively capture similarities between tuples. We consider both the case where pre-trained word embeddings are available as well the case where they are not; we present ways to learn and tune the distributed representations that are customized for a specific ER task under different scenarios. We propose a locality sensitive hashing (LSH) based blocking approach that takes all attributes of a tuple into consideration and produces much smaller blocks, compared with traditional methods that consider only a few attributes. We evaluate our algorithms on multiple datasets (including benchmarks, biomedical data, as well as multi-lingual data) and the extensive experimental results show that DeepER outperforms existing solutions.",
                "doi": "https://doi.org/10.5555/3236187.3269461",
                "title": "Distributed representations of tuples for entity resolution",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1903.06141",
        "mid": "2921285217",
        "abstract": "LiDAR-camera calibration is a precondition for many heterogeneous systems that fuse data from LiDAR and camera. However, the constraint from the common field of view and the requirement for strict time synchronization make the calibration a challenging problem. In this paper, we propose a hybrid LiDAR-camera calibration method aiming to solve these two difficulties. The configuration between LiDAR and camera is free from their common field of view as we move the camera to cover the scenario observed by LiDAR. 3D visual reconstruction of the environment can be achieved from the sequential visual images obtained by the moving camera, which later can be aligned with the single 3D laser scan captured when both the scene and the equipment are stationary. Under this design, our method can further get rid of the influence from time synchronization between LiDAR and camera. Moreover, the extended field of view obtained by the moving camera can improve the calibration accuracy. We derive the conditions of minimal observability for our method and discuss the influence on calibration accuracy from different placements of chessboards, which can be utilized as a guideline for designing high-accuracy calibration procedures. We validate our method on both simulation platform and real-world datasets. Experiments show that our method can achieve higher accuracy than other comparable methods.",
        "related_work": "Unlike the approaches above, @cite_27 utilizes the reflectance intensity to estimate the corners of the chessboard from the 3D laser point cloud. If the corners of the 3D laser point cloud are identified, the extrinsic calibration is converted to a 3D-2D matching problem. However, the noise and sparsity of the point cloud data present a challenge in this approach. Moreover, like the other appearance-based methods, this method has a limitation where there must be a shared common field of view. Even in the application scenario where the condition is met, the requirement of the common field of view constrains the scale of the scene and limits the number of targets that can be detected, thus affecting the accuracy of the calibration.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2746193234",
                "abstract": "This paper presents a novel method for fully automatic and convenient extrinsic calibration of a 3D LiDAR and a panoramic camera with a normally printed chessboard. The proposed method is based on the 3D corner estimation of the chessboard from the sparse point cloud generated by one frame scan of the LiDAR. To estimate the corners, we formulate a full-scale model of the chessboard and fit it to the segmented 3D points of the chessboard. The model is fitted by optimizing the cost function under constraints of correlation between the reflectance intensity of laser and the color of the chessboard\u2019s patterns. Powell\u2019s method is introduced for resolving the discontinuity problem in optimization. The corners of the fitted model are considered as the 3D corners of the chessboard. Once the corners of the chessboard in the 3D point cloud are estimated, the extrinsic calibration of the two sensors is converted to a 3D-2D matching problem. The corresponding 3D-2D points are used to calculate the absolute pose of the two sensors with Unified Perspective-n-Point (UPnP). Further, the calculated parameters are regarded as initial values and are refined using the Levenberg-Marquardt method. The performance of the proposed corner detection method from the 3D point cloud is evaluated using simulations. The results of experiments, conducted on a Velodyne HDL-32e LiDAR and a Ladybug3 camera under the proposed re-projection error metric, qualitatively and quantitatively demonstrate the accuracy and stability of the final extrinsic calibration parameters.",
                "doi": "https://doi.org/10.3390/rs9080851",
                "title": "Reflectance Intensity Assisted Automatic and Accurate Extrinsic Calibration of 3D LiDAR and Panoramic Camera Using a Printed Chessboard",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1903.04249",
        "mid": "2921739743",
        "abstract": "This work provides a comprehensive analysis on naturalistic driving behavior for highways based on the highD data set. Two thematic fields are considered. First, some macroscopic and microscopic traffic statistics are provided. These include the traffic flow rate and the traffic density, as well as velocity, acceleration and distance distributions. Additionally, the dependencies to each other are examined and compared to related work. The second part investigates the distributions of criticality measures. The Time-To-Collision, Time-Headway and a third measure, which couples both, are analyzed. These measures are also combined with other indicators. Scenarios, in which these measures reach a critical level, are separately discussed. The results are compared to related work as well. The two main contributions of this work can be stated as follows. First, the analysis on the criticality measures can be used to find suitable thresholds for rare traffic scenarios. Second, the statistics provided in this work can also be utilized for traffic modeling, for example in simulation environments.",
        "related_work": "The publication @cite_7 proposes three risk level thresholds for THW and TTC. Level 1 describes scenarios, which exceed normal'' driving. Level 2 scenarios show a higher collision risk and level 3 scenarios indicate an imminent collision risk. The THW is combined with a relative velocity threshold. The TTC threshold is combined with a brakelight indication, see Table in Section . Longitudinal and lateral accelerations can increase or decrease the level according to their defined tresholds, see Section . This paper provides occurrences of scenarios in the highD data set according to that risk definition. The authors in @cite_7 state, that the false positive rate for detecting critical scenarios was reduced considering the driver intention, which can be achieved for example by considering the yaw-rate, turn or brake signals, accessible from the bus system. Another aspect stated in @cite_7 is, that drivers usually see the presence or absence of a vehicle in front of their leader vehicle and adjust their driving strategy to that.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "1977446065",
                "abstract": "Fahrzeugdaten aus Feldversuchen werden vorrangig dazu verwendet, die Wirkung von Fahrerassistenzsystemen und den Sicherheitsgewinn durch ihren Einsatz zu bewerten. Die Auswertung ist aufgrund der grossen Datenmenge mit einem hohen Zeitaufwand verbunden. Im Rahmen des euroFOT-Projekts wurde am Institut fuer Kraftfahrzeuge (ika) der RWTH Aachen University ein automatisierter Prozess zur Erkennung von systemrelevanten Fahrsituationen auf Basis von Fahrzeugdaten entwickelt. Dadurch kann dieser fuer die Auswertung essenzielle Schritt mit geringerem Zeitaufwand durchgefuehrt werden. Fuer die automatisierte Situationserkennung wurde ein Algorithmus fuer die systemrelevanten Fahrsituationen definiert. Grundsaetzlich wird bei diesem Ansatz zwischen zwei Arten von kritischen Ergebnissen unterschieden (Fahrdynamik und Abstandsverhalten). Fuer kritische Ereignisse aufgrund des Abstandsverhaltens wird zusaetzlich die Fahrerreaktion bei der Bewertung der Kritikalitaet beruecksichtigt. Basierend auf Videodaten aus der Pilotphase wurde durch die Anpassung der Parameter eine signifikante Reduktion der Fehldetektionen erreicht. Die aktuelle Version des Algorithmus zur Erkennung von kritischen Ereignissen hat ihre Zuverlaessigkeit in der Pilotphase unter verschiedenen Bedingungen sowie fuer unterschiedliche Fahrzeugtypen und Fahrer unter Beweis gestellt. Diese Version wird aktuell in mehreren Fahrzeugmanagementzentren im euroFOT-Projekt zur Auswertung verwendet.",
                "doi": "https://doi.org/10.1007/s35148-012-0485-x",
                "title": "Erkennung und Klassifizierung Kritischer Fahrsituationen Mittels Fahrzeugdaten",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1903.04416",
        "mid": "2922491876",
        "abstract": "We introduce the diffusion @math -means clustering method on Riemannian submanifolds, which maximizes the within-cluster connectedness based on the diffusion distance. The diffusion @math -means constructs a random walk on the similarity graph with vertices as data points randomly sampled on the manifolds and edges as similarities given by a kernel that captures the local geometry of manifolds. Thus the diffusion @math -means is a multi-scale clustering tool that is suitable for data with non-linear and non-Euclidean geometric features in mixed dimensions. Given the number of clusters, we propose a polynomial-time convex relaxation algorithm via the semidefinite programming (SDP) to solve the diffusion @math -means. In addition, we also propose a nuclear norm (i.e., trace norm) regularized SDP that is adaptive to the number of clusters. In both cases, we show that exact recovery of the SDPs for diffusion @math -means can be achieved under suitable between-cluster separability and within-cluster connectedness of the submanifolds, which together quantify the hardness of the manifold clustering problem. We further propose the localized diffusion @math -means by using the local adaptive bandwidth estimated from the nearest neighbors. We show that exact recovery of the localized diffusion @math -means is fully adaptive to the local probability density and geometric structures of the underlying submanifolds.",
        "related_work": "On the contrary, literature on theoretical guarantees for manifold clustering is rather scarce, with the exception @cite_6 . Near-optimal exact recovery of some emblematic clustering methods based on pairwise distances of data is derived under a condition that the minimal signal separation strength over all pairs of submanifolds is larger than a threshold. Compared with our diffusion @math -means with local scaling, results established in @cite_6 are non-adaptive to the local density and (geometric) structures of the submanifolds (cf. Theorem and ahead).",
        "ref_abstract": {
            "@cite_6": {
                "mid": "2104094124",
                "abstract": "In the context of clustering, we consider a generative model in a Euclidean ambient space with clusters of different shapes, dimensions, sizes, and densities. In an asymptotic setting where the number of points becomes large, we obtain theoretical guaranties for some emblematic methods based on pairwise distances: a simple algorithm based on the extraction of connected components in a neighborhood graph; hierarchical clustering with single linkage; and the spectral clustering method of Ng, Jordan, and Weiss. The methods are shown to enjoy some near-optimal properties in terms of separation between clusters and robustness to outliers. The local scaling method of Zelnik-Manor and Perona is shown to lead to a near-optimal choice for the scale in the first and third methods. We also provide a lower bound on the spectral gap to consistently choose the correct number of clusters in the spectral method.",
                "doi": "https://doi.org/10.1109/tit.2011.2104630",
                "title": "Clustering Based on Pairwise Distances When the Data is of Mixed Dimensions",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1903.01715",
        "mid": "2919231070",
        "abstract": "Adversarial examples are a challenging open problem for deep neural networks. We propose in this paper to add a penalization term that forces the decision function to be at in some regions of the input space, such that it becomes, at least locally, less sensitive to attacks. Our proposition is theoretically motivated and shows on a rst set of carefully conducted experiments that it behaves as expected when used alone, and seems promising when coupled with adversarial training.",
        "related_work": "The difference between double backpropagation and our gradient penalization is that we penalize the @math -norm of the gradient of each output with respect to the input while, in backpropagation, the @math -norm of the loss is penalized. Hence, we should not have the problem that occurs when the penalization term is multiplied by a small error vector. Nevertheless, this might not be a problem when using a different loss function. Although empirical evidence show double backpropagation's efficiency to enhance generalization, it is insufficient to defend against adversarial examples. That is what @cite_1 highlights saying limiting sensitivity to infinitesimal perturbation [e.g., using double backpropagation][] drucker1992improving only provides constraints very near training examples, so it does not solve the adversarial perturbation problem . But there are evidence that coupling that gradient penalty with adversarial training increases the robustness.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2964082701",
                "abstract": "Deep learning algorithms have been shown to perform extremely well on manyclassical machine learning problems. However, recent studies have shown thatdeep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force adeep neural network (DNN) to provide adversary-selected outputs. Such attackscan seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles canbe crashed, illicit or illegal content can bypass content filters, or biometricauthentication systems can be manipulated to allow improper access. In thiswork, we introduce a defensive mechanism called defensive distillationto reduce the effectiveness of adversarial samples on DNNs. We analyticallyinvestigate the generalizability and robustness properties granted by the useof defensive distillation when training DNNs. We also empirically study theeffectiveness of our defense mechanisms on two DNNs placed in adversarialsettings. The study shows that defensive distillation can reduce effectivenessof sample creation from 95 to less than 0.5 on a studied DNN. Such dramaticgains can be explained by the fact that distillation leads gradients used inadversarial sample creation to be reduced by a factor of 1030. We alsofind that distillation increases the average minimum number of features thatneed to be modified to create adversarial samples by about 800 on one of theDNNs we tested.",
                "doi": "https://doi.org/10.1109/sp.2016.41",
                "title": "Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1903.01411",
        "mid": "2919826171",
        "abstract": "Princeton WordNet is one of the most important resources for natural language processing, but is only available for English. While it has been translated using the expand approach to many other languages, this is an expensive manual process. Therefore it would be beneficial to have a high-quality automatic translation approach that would support NLP techniques, which rely on WordNet in new languages. The translation of wordnets is fundamentally complex because of the need to translate all senses of a word including low frequency senses, which is very challenging for current machine translation approaches. For this reason we leverage existing translations of WordNet in other languages to identify contextual information for wordnet senses from a large set of generic parallel corpora. We evaluate our approach using 10 translated wordnets for European languages. Our experiment shows a significant improvement over translation without any contextual information. Furthermore, we evaluate how the choice of pivot languages affects performance of multilingual word sense disambiguation.",
        "related_work": "@cite_3 present a method for WordNet construction and enlargement with the help of sense tagged parallel corpora. Since parallel sense tagged data are not always available, they use to translate a manually sense tagged corpus. In addition they apply automatic sense tagging of a manually translated parallel corpus, whereby they report worse performance compared to the previous approach. We try to overcome this issue by engaging up to ten languages to improve the performance of the automatic sense tagging. Similarly, BabelNet @cite_34 aligns the lexicographic knowledge from WordNet to the encyclopaedic knowledge of Wikipedia. This is done by assigning WordNet synsets to Wikipedia entries, and making these relations multilingual through the interlingual links. For languages, which do not have the corresponding Wikipedia entry, the authors use to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language.",
        "ref_abstract": {
            "@cite_34": {
                "mid": "2120699290",
                "abstract": "We present an automatic approach to the construction of BabelNet, a very large, wide-coverage multilingual semantic network. Key to our approach is the integration of lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition, Machine Translation is applied to enrich the resource with lexical information for all languages. We first conduct in vitro experiments on new and existing gold-standard datasets to show the high quality and coverage of BabelNet. We then show that our lexical resource can be used successfully to perform both monolingual and cross-lingual Word Sense Disambiguation: thanks to its wide lexical coverage and novel semantic relations, we are able to achieve state-of the-art results on three different SemEval evaluation tasks.",
                "doi": "https://doi.org/10.1016/j.artint.2012.07.001",
                "title": "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
                "publication_year": 2012
            },
            "@cite_3": {
                "mid": "112978228",
                "abstract": "In this paper we present a methodology for WordNet construction based on the exploitation of parallel corpora with semantic annotation of the English source text. We are using this methodology for the enlargement of the Spanish and Catalan versions of WordNet 3.0, but the methodology can also be used for other languages. As big parallel corpora with semantic annotation are not usually available, we explore two strategies to overcome this problem: to use monolingual sense tagged corpora and machine translation, on the one hand; and to use parallel corpora and automatic sense tagging on the source text, on the other. With these resources, the problem of acquiring a WordNet from parallel corpora can be seen as a word alignment task. Fortunately, this task is well known, and some aligning algorithms are freely available.",
                "doi": "https://doi.org/10.1007/978-3-642-28601-8_10",
                "title": "Parallel Corpora for WordNet Construction: Machine Translation vs. Automatic Sense Tagging",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1708.01884",
        "mid": "2742326713",
        "abstract": "In this paper we present the Sampling Privacy mechanism for privately releasing personal data. Sampling Privacy is a sampling based privacy mechanism that satisfies differential privacy.",
        "related_work": "Distributional privacy @cite_21 is a privacy mechanism which says that the released aggregate information only reveals the underlying ground truth distribution and nothing morre. Each data owner is protected by the randomness of the other randomly selected data owners rather than by adding explicit privacy noise to the output. The indistinguishability from the underlying distribution protects individual data owners and is strictly stronger than differential privacy. However, it is computationally inefficient though can work over a large class of queries known as Vapnik-Chervonenkis (VC) dimension.",
        "ref_abstract": {
            "@cite_21": {
                "mid": "2042469398",
                "abstract": "In this article, we demonstrate that, ignoring computational constraints, it is possible to release synthetic databases that are useful for accurately answering large classes of queries while preserving differential privacy. Specifically, we give a mechanism that privately releases synthetic data useful for answering a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries. We show that this in particular implies a mechanism for counting queries that gives error guarantees that grow only with the VC-dimension of the class of queries, which itself grows at most logarithmically with the size of the query class. We also show that it is not possible to release even simple classes of queries (such as intervals and their generalizations) over continuous domains with worst-case utility guarantees while preserving differential privacy. In response to this, we consider a relaxation of the utility guarantee and give a privacy preserving polynomial time algorithm that for any halfspace query will provide an answer that is accurate for some small perturbation of the query. This algorithm does not release synthetic data, but instead another data structure capable of representing an answer for each query. We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis-aligned rectangles of constant dimension over discrete domains.",
                "doi": "https://doi.org/10.1145/2450142.2450148",
                "title": "A learning theory approach to noninteractive database privacy",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1902.10528",
        "mid": "2932020536",
        "abstract": "Person attributes are often exploited as mid-level human semantic information to help promote the performance of person re-identification task. In this paper, unlike most existing methods simply taking attribute learning as a classification problem, we perform it in a different way with the motivation that attributes are related to specific local regions, which refers to the perceptual ability of attributes. We utilize the process of attribute detection to generate corresponding attribute-part detectors, whose invariance to many influences like poses and camera views can be guaranteed. With detected local part regions, our model extracts local features to handle the body part misalignment problem, which is another major challenge for person re-identification. The local descriptors are further refined by fused attribute information to eliminate interferences caused by detection deviation. Extensive experiments on two popular benchmarks with attribute annotations demonstrate the effectiveness of our model and competitive performance compared with state-of-the-art algorithms.",
        "related_work": "Most popular person Re-ID algorithms can be categorized into two classes, feature representation learning and metric learning. For the first category, usually the human identity labels are exploited as the supervision for training a classifier for different identities and can be considered as a classification problem. During recent years, CNN-based feature representation learning has been dominating various research fields because of its excellent performance and is no exception in person Re-ID community. Xiao al @cite_27 propose a joint learning strategy to train a single classifier for multiple domains at the same time, and then fine-tune to adapt to each single domain with a domain guided dropout policy.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2342611082",
                "abstract": "Learning generic and robust feature representations with data from multiple domains for the same problem is of great value, especially for the problems that have multiple datasets but none of them are large enough to provide abundant data variations. In this work, we present a pipeline for learning deep feature representations from multiple domains with Convolutional Neural Networks (CNNs). When training a CNN with data from all the domains, some neurons learn representations shared across several domains, while some others are effective only for a specific one. Based on this important observation, we propose a Domain Guided Dropout algorithm to improve the feature learning procedure. Experiments show the effectiveness of our pipeline and the proposed algorithm. Our methods on the person re-identification problem outperform stateof-the-art methods on multiple datasets by large margins.",
                "doi": "https://doi.org/10.1109/cvpr.2016.140",
                "title": "Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1902.05295",
        "mid": "2914663930",
        "abstract": "In LoRa (Long Range), when a collision occurs in the network, each end-device has to retransmit its colliding frame, which reduces the throughput, and increases the energy consumption of the end-devices and the delay of the frames. In this paper, we propose an algorithm to decode colliding synchronized LoRa signals and thus improve the overall performance of the network. Indeed, we use successive transmissions of bitmaps by the end-devices to determine the correct symbols of each colliding frame, instead of retransmitting the whole frames. Simulation results show that our algorithm is able to significantly improve the overall throughput of LoRaWAN, and to decrease the energy consumption and the delay of the transmitters.",
        "related_work": "In @cite_3 , the author proposed an analysis of packet collision and packet loss probabilities in LoRaWAN, and developed theoretical expressions for both of them. The author showed that his theoretical expressions are more accurate than a Poisson distributed process to describe the collisions.",
        "ref_abstract": {
            "@cite_3": {
                "mid": "2766303497",
                "abstract": "Internet of things (IoT) is considered as the next technological revolution. Therefore, many solutions are developed either in free, i.e. ISM bands or in non free bands with the ultimate aim of affording connectivity over several kilometers. Based on this feature, in urban environment the density of IoT devices will be extremely high. In this paper we propose to analyze the collision and packet loss when LoRaWAN is considered. Based on the LoRaWAN features, we develop closed-form expressions of collision and packet loss probabilities. Simulation results confirm our theoretical developments. We also show that our theoretical expressions are more accurate than the Poisson distributed process to describe the collisions.",
                "doi": "https://doi.org/10.23919/eusipco.2017.8081678",
                "title": "Collision and packet loss analysis in a LoRaWAN network",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1902.02144",
        "mid": "2913152967",
        "abstract": "Anatomical landmark segmentation and pathology localization are important steps in automated analysis of medical images. They are particularly challenging when the anatomy or pathology is small, as in retinal images and cardiac MRI, or when the image is of low quality due to device acquisition parameters as in magnetic resonance (MR) scanners. We propose an image super-resolution method using progressive generative adversarial networks (P-GAN) that can take as input a low-resolution image and generate a high resolution image of desired scaling factor. The super resolved images can be used for more accurate detection of landmarks and pathology. Our primary contribution is in proposing a multistage model where the output image quality of one stage is progressively improved in the next stage by using a triplet loss function. The triplet loss enables stepwise image quality improvement by using the output of the previous stage as the baseline. This facilitates generation of super resolved images of high scaling factor while maintaining good image quality. Experimental results for image super-resolution show that our proposed multistage P-GAN outperforms competing methods and baseline GAN.",
        "related_work": "Single image based SR methods downsample a given image to create a LR image and learn the mapping between the original and LR version. The learnt mapping is then applied to the original image to generate a SR image. In @cite_14 HR and LR dictionaries are learned from MRI to generate SR images. These methods depend on learning the dictionaries on external LR-HR images and assume that the test image is a representative of the training data. Since this is not always the case the results are unsatisfactory. These approaches are computationally demanding as the candidate patches have to be searched in the training dataset to find the most suitable HR candidate. Instead, compact and generative models can be learned from the training data to define the mapping between LR and HR patches.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2157466038",
                "abstract": "Abstract Resolution in Magnetic Resonance (MR) is limited by diverse physical, technological and economical considerations. In conventional medical practice, resolution enhancement is usually performed with bicubic or B-spline interpolations, strongly affecting the accuracy of subsequent processing steps such as segmentation or registration. This paper presents a sparse-based super-resolution method, adapted for easily including prior knowledge, which couples up high and low frequency information so that a high-resolution version of a low-resolution brain MR image is generated. The proposed approach includes a whole-image multi-scale edge analysis and a dimensionality reduction scheme, which results in a remarkable improvement of the computational speed and accuracy, taking nearly 26 min to generate a complete 3D high-resolution reconstruction. The method was validated by comparing interpolated and reconstructed versions of 29 MR brain volumes with the original images, acquired in a 3T scanner, obtaining a reduction of 70 in the root mean squared error, an increment of 10.3 dB in the peak signal-to-noise ratio, and an agreement of 85 in the binary gray matter segmentations. The proposed method is shown to outperform a recent state-of-the-art algorithm, suggesting a substantial impact in voxel-based morphometry studies.",
                "doi": "https://doi.org/10.1016/j.media.2012.09.003",
                "title": "Single-image super-resolution of brain MR images using overcomplete dictionaries",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1902.01980",
        "mid": "2912093838",
        "abstract": "A semi-supervised learning framework using the feedforward-designed convolutional neural networks (FF-CNNs) is proposed for image classification in this work. One unique property of FF-CNNs is that no backpropagation is used in model parameters determination. Since unlabeled data may not always enhance semi-supervised learning, we define an effective quality score and use it to select a subset of unlabeled data in the training process. We conduct experiments on the MNIST, SVHN, and CIFAR-10 datasets, and show that the proposed semi-supervised FF-CNN solution outperforms the CNN trained by backpropagation (BP-CNN) when the amount of labeled data is reduced. Furthermore, we develop an ensemble system that combines the output decision vectors of different semi-supervised FF-CNNs to boost classification accuracy. The ensemble systems can achieve further performance gains on all three benchmarking datasets.",
        "related_work": "The construction of conv layers is realized by multi-stage Saab transforms @cite_19 . The Saab transform is a variant of the principal component analysis (PCA) with a constant bias vector to annihilate activation's nonlinearity. The Saab transform can reduce feature redundancy in the spectral domain, yet there still exists correlation among spatial dimensions of the same spectral component. This is especially true in low-frequency spectral components. Thus, a channel-wise PCA (C-PCA) was proposed in @cite_1 to reduce spatial redundancy of Saab coefficients furthermore. Since the construction of conv layers is unsupervised, they can be fully adopted in an SSL system.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "2949474319",
                "abstract": "The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable feedforward (FF) design without any BP as a reference. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace Approximation with Adjusted Bias) transform. It is a variant of the principal component analysis (PCA) with an added bias vector to annihilate activation's nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected (FC) layers, we construct them using a cascade of multi-stage linear least squared regressors (LSRs). The classification and robustness (against adversarial attacks) performances of BP- and FF-designed CNNs applied to the MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.",
                "doi": "https://doi.org/10.48550/arxiv.1810.02786",
                "title": "Interpretable Convolutional Neural Networks via Feedforward Design",
                "publication_year": 2018
            },
            "@cite_1": {
                "mid": "2910090831",
                "abstract": "An ensemble method that fuses the output decision vectors of multiple feedforward-designed convolutional neural networks (FF-CNNs) to solve the image classification problem is proposed in this work. To enhance the performance of the ensemble system, it is critical to increasing the diversity of FF-CNN models. To achieve this objective, we introduce diversities by adopting three strategies: 1) different parameter settings in convolutional layers, 2) flexible feature subsets fed into the Fully-connected (FC) layers, and 3) multiple image embeddings of the same input source. Furthermore, we partition input samples into easy and hard ones based on their decision confidence scores. As a result, we can develop a new ensemble system tailored to hard samples to further boost classification accuracy. Experiments are conducted on the MNIST and CIFAR-10 datasets to demonstrate the effectiveness of the ensemble method.",
                "doi": "https://doi.org/10.48550/arxiv.1901.02154",
                "title": "Ensembles of feedforward-designed convolutional neural networks",
                "publication_year": 2019
            }
        }
    },
    {
        "aid": "1902.01239",
        "mid": "2948647439",
        "abstract": "We study a multiplayer stochastic multi-armed bandit problem in which players cannot communicate, and if two or more players pull the same arm, a collision occurs and the involved players receive zero reward. We consider the challenging heterogeneous setting, in which different arms may have different means for different players, and propose a new, efficient algorithm that combines the idea of leveraging forced collisions for implicit communication and that of performing matching eliminations. We give a finite-time analysis of our algorithm, bounding its regret by O((log T)^ 1+ ) for any fixed >0. If the optimal assignment of players to arms is unique, we further show that it attains the optimal O(log(T)) regret, solving an open question raised at NeurIPS 2018.",
        "related_work": "Our problem can be viewed as a challenging extension of two complex multi-armed bandit problems. First, relaxing the need for decentralization, i.e. when a central controller is requested to jointly select @math , the problem coincides with a combinatorial bandit problem with semi-bandit feedback, as explained in , where we review the achievable regret in the centralized setting. Second, the particular case of a common utility for all players, i.e. @math for all @math , has been studied extensively, and we review in the achievable regret in this multi-player bandit problem, presenting in particular some inspiring ideas to design algorithms for our more general problem. Finally in we discuss the Game-of-Thrones algorithm of @cite_16 , which is our only competitor in the fully distributed setting considered in this paper.",
        "ref_abstract": {
            "@cite_16": {
                "mid": "2912712349",
                "abstract": "We consider a multi-armed bandit game where N players compete for M arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed or Markovian. When two or more players choose the same arm, they all receive zero reward. Performance is measured using the expected sum of regrets, compared to optimal assignment of arms to players. We assume that each player only knows her actions and the reward she received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-O ( T ). This is the first algorithm to achieve a near order optimal regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.",
                "doi": "https://doi.org/10.1287/moor.2020.1051",
                "title": "Game of Thrones: Fully Distributed Learning for Multiplayer Bandits",
                "publication_year": 2021
            }
        }
    },
    {
        "aid": "1708.00251",
        "mid": "2741128077",
        "abstract": "Due to the increasing availability of whole slide scanners facilitating digitization of histopathological tissue, there is a strong demand for the development of computer based image analysis systems. In this work, the focus is on the segmentation of the glomeruli constituting a highly relevant structure in renal histopathology, which has not been investigated before in combination with CNNs. We propose two different CNN cascades for segmentation applications with sparse objects. These approaches are applied to the problem of glomerulus segmentation and compared with conventional fully-convolutional networks. Overall, with the best performing cascade approach, single CNNs are outperformed and a pixel-level Dice similarity coefficient of 0.90 is obtained. Combined with qualitative and further object-level analyses the obtained results are assessed as excellent also compared to recent approaches. In conclusion, we can state that especially one of the proposed cascade networks proved to be a highly powerful tool for segmenting the renal glomeruli providing best segmentation accuracies and also keeping the computing time at a low level.",
        "related_work": "Few approaches were proposed to perform segmentation of the glomeruli. @cite_12 introduced a separate detection and segmentation stage, both based on training a supervised classification model, specifically a linear support vector machine. For detection and segmentation different versions of histogram of oriented gradient descriptors were applied. Detection was performed by means of the sliding window approach. For segmentation, a polygon-fitting technique was utilized to determine the precise outline of the objects by adding a circularity constraint. For learning to distinguish between border region and non-border regions (and not between inside and outside of a glomerulus), this method requires highly precise annotations for training. This method was developed for a specific staining (desmin) which highlights cells in glomeruli.",
        "ref_abstract": {
            "@cite_12": {
                "mid": "2112028161",
                "abstract": "The detection of the glomeruli is a key step in the histopathological evaluation of microscopic images of the kidneys. However, the task of automatic detection of the glomeruli poses challenges owing to the differences in their sizes and shapes in renal sections as well as the extensive variations in their intensities due to heterogeneity in immunohistochemistry staining.",
                "doi": "https://doi.org/10.1186/s12859-015-0739-1",
                "title": "Segmental HOG: new descriptor for glomerulus detection in kidney microscopy image",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1707.07465",
        "mid": "2737803841",
        "abstract": "Patterns stored within pre-trained deep neural networks compose large and powerful descriptive languages that can be used for many different purposes. Typically, deep network representations are implemented within vector embedding spaces, which enables the use of traditional machine learning algorithms on top of them. In this short paper we propose the construction of a graph embedding space instead, introducing a methodology to transform the knowledge coded within a deep convolutional network into a topological space (i.e. a network). We outline how such graph can hold data instances, data features, relations between instances and features, and relations among features. Finally, we introduce some preliminary experiments to illustrate how the resultant graph embedding space can be exploited through graph analytics algorithms.",
        "related_work": "The relation between graphs and deep neural networks have been previously explored, but most contributions do so from a different perspective. While our proposal is to obtain a graph representation of the embedding produced by a CNN when processing an image, most related work focuses on training DNNs for processing graph data. For example, DeepWalk @cite_1 uses random walks in a graph ( Flickr or YouTube networks) to feed a SkipGram model, and then evaluate community detection methods on those graphs. Similarly to DeepWalk, the work of also processes graphs as input, but it uses a probabilistic approach on weighted graphs to feed an autoencoder. In contrast, we are performing community detection on a dataset of images, something that, to the best of our knowledge, had not been done before.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2154851992",
                "abstract": "We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10 higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60 less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",
                "doi": "https://doi.org/10.1145/2623330.2623732",
                "title": "DeepWalk",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1707.04642",
        "mid": "2737152053",
        "abstract": "The work presented here applies deep learning to the task of automated cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We describe an automated heart sound classification algorithm that combines the use of time-frequency heat map representations with a deep convolutional neural network (CNN). Given the cost-sensitive nature of misclassification, our CNN architecture is trained using a modified loss function that directly optimizes the trade-off between sensitivity and specificity. We evaluated our algorithm at the 2016 PhysioNet Computing in Cardiology challenge where the objective was to accurately classify normal and abnormal heart sounds from single, short, potentially noisy recordings. Our entry to the challenge achieved a final specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved the greatest specificity score out of all challenge entries and, using just a single CNN, our algorithm differed in overall score by only 0.02 compared to the top place finisher, which used an ensemble approach.",
        "related_work": "While there have been many previous efforts applied to automated heart sound analysis, gauging the success of historical approaches has been somewhat difficult, due to differences in dataset quality, number of recordings available for training and testing algorithms, recorded signal lengths and the environment in which data was collected (e.g. clinical vs. non-clinical settings). Moreover, some existing works have not performed appropriate train-test data splits and have reported results on training or validation data, which is highly likely to produce optimistic results due to overfitting @cite_27 . In this work, we report results from the 2016 PhysioNet Computing in Cardiology Challenge, which evaluated entries on a large test-set that was not made publicly available. To reduce overfitting, no recordings from the same subject were included in both the training and the test set and a variety of both and PCG recordings, which exhibited very poor signal quality, were included to encourage the development of accurate and robust algorithms.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2557139718",
                "abstract": "This is an author-created, un-copyedited version of an article published in Physiological Measurement. IOP Publishing Ltd is not responsible for any errors or omissions in this version of the manuscript or any version derived from it. The Version of Record is available online at https: doi.org 10.1088 0967-3334 37 12 2181",
                "doi": "https://doi.org/10.1088/0967-3334/37/12/2181",
                "title": "An open access database for the evaluation of heart sound algorithms",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1706.09993",
        "mid": "2730184663",
        "abstract": "We consider the problem of phase retrieval, i.e. that of solving systems of quadratic equations. A simple variant of the randomized Kaczmarz method was recently proposed for phase retrieval, and it was shown numerically to have a computational edge over state-of-the-art Wirtinger flow methods. In this paper, we provide the first theoretical guarantee for the convergence of the randomized Kaczmarz method for phase retrieval. We show that it is sufficient to have as many Gaussian measurements as the dimension, up to a constant factor. Along the way, we introduce a sufficient condition on measurement sets for which the randomized Kaczmarz method is guaranteed to work. We show that Gaussian sampling vectors satisfy this property with high probability; this is proved using a chaining argument coupled with bounds on VC dimension and metric entropy.",
        "related_work": "During the preparation of this manuscript, we became aware of independent simultaneous work done by Jeong and G \"u nt \"u rk. They also studied the randomized Kaczmarz method adapted to phase retrieval, and obtained almost the same result that we did (see @cite_14 and Theorem 1.1 therein). In order to prove their guarantee, they use a stopping time argument similar to ours, but replace the ACW condition with a stronger condition called . They prove that measurement systems comprising vectors drawn independently and uniformly from the sphere satisfy this property with high probability, and the main tools they use in their proof are hyperplane tessellations and a net argument together with Lipschitz relaxation of indicator functions.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2462226338",
                "abstract": "Estimating low-rank positive-semidefinite (PSD) matrices from symmetric rank-one measurements is of great importance in many applications, such as high-dimensional data processing, quantum state tomography, and phase retrieval. When the rank is known a priori , this problem can be regarded as solving a system of quadratic equations of a low-dimensional subspace. The authors develop a fast iterative algorithm based on an adaptation of the Kaczmarz method, which is traditionally used for solving overdetermined linear systems. In particular, the authors characterize the dynamics of the algorithm when the measurement vectors are composed of standard Gaussian entries in the online setting. Numerical simulations demonstrate the compelling performance of the proposed algorithm.",
                "doi": "https://doi.org/10.1109/lsp.2016.2590468",
                "title": "Kaczmarz Method for Solving Quadratic Equations",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1706.09430",
        "mid": "2727243376",
        "abstract": "Clinical observations indicate that during critical care at the hospitals, patients sleep positioning and motion affect recovery. Unfortunately, there is no formal medical protocol to record, quantify, and analyze patient motion. There is a small number of clinical studies, which use manual analysis of sleep poses and motion recordings to support medical benefits of patient positioning and motion monitoring. Manual processes are not scalable, are prone to human errors, and strain an already taxed healthcare workforce. This study introduces DECU (Deep Eye-CU): an autonomous mulitmodal multiview system, which addresses these issues by autonomously monitoring healthcare environments and enabling the recording and analysis of patient sleep poses and motion. DECU uses three RGB-D cameras to monitor patient motion in a medical Intensive Care Unit (ICU). The algorithms in DECU estimate pose direction at different temporal resolutions and use keyframes to efficiently represent pose transition dynamics. DECU combines deep features computed from the data with a modified version of Hidden Markov Model to more flexibly model sleep pose duration, analyze pose patterns, and summarize patient motion. Extensive experimental results are presented. The performance of DECU is evaluated in ideal (BC: Bright and Clear occlusion-free) and natural (DO: Dark and Occluded) scenarios at two motion resolutions in a mock-up and a real ICU. The results indicate that deep features allow DECU to match the classification performance of engineered features in BC scenes and increase the accuracy by up to 8 in DO scenes. In addition, the overall pose history summarization tracing accuracy shows an average detection rate of 85 in BC and of 76 in DO scenes. The proposed keyframe estimation algorithm allows DECU to reach an average 78 transition classification accuracy.",
        "related_work": "There is a large body of research that focuses on recognizing and tracking human motion. The latest developments in deep features and convolutional neural network architectures achieve impressive performance; however, these require large amounts of data @cite_5 , @cite_14 , @cite_13 , and @cite_7 . These methods tackle the recognition of actions performed at the center of the camera plane, except for @cite_3 , which uses static cameras to analyze actions. Method @cite_3 allows actions to not be centered on the plane; however, it requires scenes with good illumination and no occlusions. At its current stage of development the DECU framework cannot collect the large number of samples necessary to train a deep network without disrupting the hospital.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2951208315",
                "abstract": "The long short-term memory (LSTM) neural network is capable of processing complex sequential information since it utilizes special gating schemes for learning representations from long input sequences. It has the potential to model any sequential time-series data, where the current hidden state has to be considered in the context of the past hidden states. This property makes LSTM an ideal choice to learn the complex dynamics of various actions. Unfortunately, the conventional LSTMs do not consider the impact of spatio-temporal dynamics corresponding to the given salient motion patterns, when they gate the information that ought to be memorized through time. To address this problem, we propose a differential gating scheme for the LSTM neural network, which emphasizes on the change in information gain caused by the salient motions between the successive frames. This change in information gain is quantified by Derivative of States (DoS), and thus the proposed LSTM model is termed as differential Recurrent Neural Network (dRNN). We demonstrate the effectiveness of the proposed model by automatically recognizing actions from the real-world 2D and 3D human action datasets. Our study is one of the first works towards demonstrating the potential of learning complex time-series representations via high-order derivatives of states.",
                "doi": "https://doi.org/10.48550/arxiv.1504.06678",
                "title": "Differential Recurrent Neural Networks for Action Recognition",
                "publication_year": 2015
            },
            "@cite_7": {
                "mid": "2952633803",
                "abstract": "We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8 accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.",
                "doi": "https://doi.org/10.48550/arxiv.1412.0767",
                "title": "Learning Spatiotemporal Features with 3D Convolutional Networks",
                "publication_year": 2014
            },
            "@cite_3": {
                "mid": "2235735853",
                "abstract": "We all have experienced forgetting habitual actions among our daily activities. For example, we probably have forgotten to turn the lights off before leaving a room or turn the stove off after cooking. In this paper, we propose a solution to the problem of issuing notifications on actions that may be missed. This involves learning about interdependencies between actions and being able to predict an ongoing action while segmenting the input video stream. In order to show a proof of concept, we collected a new egocentric dataset, in which people wear a camera while making lattes. We show promising results on the extremely challenging task of issuing correct and timely reminders. We also show that our model reliably segments the actions, while predicting the ongoing one when only a few frames from the beginning of the action are observed. The overall prediction accuracy is 46.2 when only 10 frames of an action are seen (2 3 of a sec). Moreover, the overall recognition and segmentation accuracy is shown to be 72.7 when the whole activity sequence is observed. Finally, the online prediction and segmentation accuracy is 68.3 when the prediction is made at every time step.",
                "doi": "https://doi.org/10.1109/iccv.2015.530",
                "title": "Generating Notifications for Missing Actions: Don't Forget to Turn the Lights Off!",
                "publication_year": 2015
            },
            "@cite_5": {
                "mid": "1744759976",
                "abstract": "This work targets human action recognition in video. While recent methods typically represent actions by statistics of local video features, here we argue for the importance of a representation derived from human pose. To this end we propose a new Pose-based Convolutional Neural Network descriptor (P-CNN) for action recognition. The descriptor aggregates motion and appearance information along tracks of human body parts. We investigate different schemes of temporal aggregation and experiment with P-CNN features obtained both for automatically estimated and manually annotated human poses. We evaluate our method on the recent and challenging JHMDB and MPII Cooking datasets. For both datasets our method shows consistent improvement over the state of the art.",
                "doi": "https://doi.org/10.1109/iccv.2015.368",
                "title": "P-CNN: Pose-Based CNN Features for Action Recognition",
                "publication_year": 2015
            },
            "@cite_13": {
                "mid": "28988658",
                "abstract": "We propose in this paper a fully automated deep model, which learns to classify human actions without using any prior knowledge. The first step of our scheme, based on the extension of Convolutional Neural Networks to 3D, automatically learns spatio-temporal features. A Recurrent Neural Network is then trained to classify each sequence considering the temporal evolution of the learned features for each timestep. Experimental results on the KTH dataset show that the proposed approach outperforms existing deep models, and gives comparable results with the best related works.",
                "doi": "https://doi.org/10.1007/978-3-642-25446-8_4",
                "title": "Sequential Deep Learning for Human Action Recognition",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1706.08260",
        "mid": "2721538148",
        "abstract": "Automatic photo adjustment is to mimic the photo retouching style of professional photographers and automatically adjust photos to the learned style. There have been many attempts to model the tone and the color adjustment globally with low-level color statistics. Also, spatially varying photo adjustment methods have been studied by exploiting high-level features and semantic label maps. Those methods are semantics-aware since the color mapping is dependent on the high-level semantic context. However, their performance is limited to the pre-computed hand-crafted features and it is hard to reflect user's preference to the adjustment. In this paper, we propose a deep neural network that models the semantics-aware photo adjustment. The proposed network exploits bilinear models that are the multiplicative interaction of the color and the contexual features. As the contextual features we propose the semantic adjustment map, which discovers the inherent photo retouching presets that are applied according to the scene context. The proposed method is trained using a robust loss with a scene parsing task. The experimental results show that the proposed method outperforms the existing method both quantitatively and qualitatively. The proposed method also provides users a way to retouch the photo by their own likings by giving customized adjustment maps.",
        "related_work": "There has been a number of studies for the automatic photo adjustment. Several methods focus on the global tonal adjustment @cite_23 , the color enhancement @cite_9 , and the personalized enhancement @cite_7 . Those methods are global adjustment approaches based on hand-crafted low-level features such as the color histogram, the scene brightness, and the highlight clipping. @cite_7 , Kapoor al proposed a method that discovers the clusters of users that have similar preferences of image enhancement for the personalized adjustment. While the concept of our method may be similar to those methods, the main difference is that we aim to discover the retouching presets that vary according to the local semantics.",
        "ref_abstract": {
            "@cite_9": {
                "mid": "2113636985",
                "abstract": "We present a machine-learned ranking approach for automatically enhancing the color of a photograph. Unlike previous techniques that train on pairs of images before and after adjustment by a human user, our method takes into account the intermediate steps taken in the enhancement process, which provide detailed information on the person's color preferences. To make use of this data, we formulate the color enhancement task as a learning-to-rank problem in which ordered pairs of images are used for training, and then various color enhancements of a novel input image can be evaluated from their corresponding rank values. From the parallels between the decision tree structures we use for ranking and the decisions made by a human during the editing process, we posit that breaking a full enhancement sequence into individual steps can facilitate training. Our experiments show that this approach compares well to existing methods for automatic color enhancement.",
                "doi": "https://doi.org/10.1109/cvpr.2014.382",
                "title": "A Learning-to-Rank Approach for Image Color Enhancement",
                "publication_year": 2014
            },
            "@cite_7": {
                "mid": "2139842681",
                "abstract": "This paper presents methods for personalization of image enhancement, which could be deployed in photo editing software and also in cloud-based image sharing services. We observe that users do have different preferences for enhancing images and that there are groups of people that share similarities in preferences. Our goal is to predict enhancements for novel images belonging to a particular user based on her specific taste, to facilitate the retouching process on large image collections. To that end, we describe an enhancement framework that can learn user preferences in an individual or collaborative way. The proposed system is based on a novel interactive application that allows to collect user's enhancement preferences. We propose algorithms to predict personalized enhancements by learning a preference model from the provided information. Furthermore, the algorithm improves prediction performance as more enhancement examples are progressively added. We conducted experiments via Amazon Mechanical Turk to collect preferences from a large group of people. Results show that the proposed framework can suggest image enhancements more targeted to individual users than commercial tools with global auto-enhancement functionalities.",
                "doi": "https://doi.org/10.1007/s11263-013-0675-3",
                "title": "Collaborative Personalization of Image Enhancement",
                "publication_year": 2013
            },
            "@cite_23": {
                "mid": "2025328853",
                "abstract": "Adjusting photographs to obtain compelling renditions requires skill and time. Even contrast and brightness adjustments are challenging because they require taking into account the image content. Photographers are also known for having different retouching preferences. As the result of this complexity, rule-based, one-size-fits-all automatic techniques often fail. This problem can greatly benefit from supervised machine learning but the lack of training data has impeded work in this area. Our first contribution is the creation of a high-quality reference dataset. We collected 5,000 photos, manually annotated them, and hired 5 trained photographers to retouch each picture. The result is a collection of 5 sets of 5,000 example input-output pairs that enable supervised learning. We first use this dataset to predict a user's adjustment from a large training set. We then show that our dataset and features enable the accurate adjustment personalization using a carefully chosen set of training photos. Finally, we introduce difference learning: this method models and predicts difference between users. It frees the user from using predetermined photos for training. We show that difference learning enables accurate prediction using only a handful of examples.",
                "doi": "https://doi.org/10.1109/cvpr.2011.5995332",
                "title": "Learning photographic global tonal adjustment with a database of input / output image pairs",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1706.05259",
        "mid": "2626103914",
        "abstract": "Learning with streaming data has attracted much attention during the past few years. Though most studies consider data stream with fixed features, in real practice the features may be evolvable. For example, features of data gathered by limited-lifespan sensors will change when these sensors are substituted by new ones. In this paper, we propose a novel learning paradigm: where old features would vanish and new features would occur. Rather than relying on only the current features, we attempt to recover the vanished features and exploit it to improve performance. Specifically, we learn two models from the recovered features and the current features, respectively. To benefit from the recovered features, we develop two ensemble methods. In the first method, we combine the predictions from two models and theoretically show that with the assistance of old features, the performance on new features can be improved. In the second approach, we dynamically select the best single prediction and establish a better performance guarantee when the best model switches. Experiments on both synthetic and real data validate the effectiveness of our proposal.",
        "related_work": "The most related work is @cite_27 , which also handles evolving features in streaming data. Different to our setting where there are overlapping periods, @cite_27 handles situations where there is no overlapping period but there are overlapping features. Thus, the technical challenges and solutions are different.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2412351563",
                "abstract": "In many real tasks the features are evolving, with some features being vanished and some other features augmented. For example, in environment monitoring some sensors expired whereas some new ones deployed; in mobile game recommendation some games dropped whereas some new ones added. Learning with such incremental and decremental features is crucial but rarely studied, particularly when the data coming like a stream and thus it is infeasible to keep the whole data for optimization. In this paper, we study this challenging problem and present the OPID approach. Our approach attempts to compress important information of vanished features into functions of survived features, and then expand to include the augmented features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store the whole data, and thus satisfy the evolving streaming data nature. The effectiveness of our approach is validated theoretically and empirically.",
                "doi": "https://doi.org/10.48550/arxiv.1605.09082",
                "title": "One-Pass Learning with Incremental and Decremental Features",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1706.04507",
        "mid": "2949978853",
        "abstract": "The recent approval of the General Data Protection Regulation (GDPR) imposes new data protection requirements on data controllers and processors with respect to the processing of European Union (EU) residents' data. These requirements consist of a single set of rules that have binding legal status and should be enforced in all EU member states. In light of these requirements, we propose in this paper the use of a blockchain-based approach to support data accountability and provenance tracking. Our approach relies on the use of publicly auditable contracts deployed in a blockchain that increase the transparency with respect to the access and usage of data. We identify and discuss three different models for our approach with different granularity and scalability requirements where contracts can be used to encode data usage policies and provenance tracking information in a privacy-friendly way. From these three models we designed, implemented, and evaluated a model where contracts are deployed by data subjects for each data controller, and a model where subjects join contracts deployed by data controllers in case they accept the data handling conditions. Our implementations show in practice the feasibility and limitations of contracts for the purposes identified in this paper.",
        "related_work": "An example of users' data protection and privacy policy enforcement on a blockchain is provided in @cite_2 . The authors propose to control access permissions to private data collected by a service (e.g., location from a mobile phone) through a Bitcoin blockchain. Every time a users subscribes to a service a new transaction specifies the access permissions and another contains the hash of the data, which are stored in a off-chain database. Policies encoded in a protocol executed by the blockchain grant or deny access to the data referenced in the chain. Although this solution is in part similar to ours, their proposed policies only specify simple allow deny enforcement (i.e. white blacklisting) without the possibility to express more complex policy conditions. Moreover, scalability is not taken into consideration: issuing minimum two transactions for every subscription of a user application to an online service would easily saturate a Bitcoin network even considering few service providers.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "1559136758",
                "abstract": "The recent increase in reported incidents of surveillance and security breaches compromising users' privacy call into question the current model, in which third-parties collect and control massive amounts of personal data. Bit coin has demonstrated in the financial space that trusted, auditable computing is possible using a decentralized network of peers accompanied by a public ledger. In this paper, we describe a decentralized personal data management system that ensures users own and control their data. We implement a protocol that turns a block chain into an automated access-control manager that does not require trust in a third party. Unlike Bit coin, transactions in our system are not strictly financial -- they are used to carry instructions, such as storing, querying and sharing data. Finally, we discuss possible future extensions to block chains that could harness them into a well-rounded solution for trusted computing problems in society.",
                "doi": "https://doi.org/10.1109/spw.2015.27",
                "title": "Decentralizing Privacy: Using Blockchain to Protect Personal Data",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1705.08630",
        "mid": "2618439667",
        "abstract": "In this paper we study the geometric location of periodic points of power series defined over fields of prime characteristic @math . More specifically, we find a lower bound for the absolute value of all periodic points in the open unit disk of minimal period @math of 2-ramified power series. We prove that this bound is optimal for a large class of power series. Our main technical result is a computation of the first significant terms of the @math th iterate of 2-ramified power series. As a by-product we obtain a self-contained proof of the characterization of 2-ramified power series.",
        "related_work": "In @cite_5 the authors give a corresponding result of Theorem for minimally ramified power series, where @math is expressed in terms of the coefficients of the first two non-linear terms. Provided the information from Theorem we can make a corresponding version of Corollary for minimally ramified power series, where the conditions for optimality are expressed in terms of the four lowest degree non-linear terms.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2135702680",
                "abstract": "We study analytic germs in one variable with a parabolic fixed point at the origin, over an ultrametric ground field of positive characteristic. It is conjectured that for such a germ the origin is isolated as a periodic point. Our main result is an affirmative solution of this conjecture in the case of a generic germ with a prescribed multiplier. The genericity condition is explicit: the power series is minimally ramified, i.e. the degree of the first nonlinear term of each of its iterates is as small as possible. Our main technical result is a computation of the first significant terms of a minimally ramified power series. From this we obtain a lower bound for the norm of nonzero periodic points, from which we deduce our main result. As a by-product we give a new and self-contained proof of a characterization of minimally ramified power series in terms of the iterative residue.",
                "doi": "https://doi.org/10.1088/0951-7715/29/5/1596",
                "title": "Generic parabolic points are isolated in positive characteristic",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1705.08320",
        "mid": "2618149497",
        "abstract": "Explaining and reasoning about processes which underlie observed black-box phenomena enables the discovery of causal mechanisms, derivation of suitable abstract representations and the formulation of more robust predictions. We propose to learn high level functional programs in order to represent abstract models which capture the invariant structure in the observed data. We introduce the @math -machine (program-induction machine) -- an architecture able to induce interpretable LISP-like programs from observed data traces. We propose an optimisation procedure for program learning based on backpropagation, gradient descent and A* search. We apply the proposed method to three problems: system identification of dynamical systems, explaining the behaviour of a DQN agent and learning by demonstration in a human-robot interaction scenario. Our experimental results show that the @math -machine can efficiently induce interpretable programs from individual data traces.",
        "related_work": "Determining how many input output examples or execution traces are required in order to generalise well is still an open research problem. However, in this paper, we focus attention more on the explanatory power afforded by programs rather than on the broader problems of generalisation in the space of programs. While these characteristics are of course related, we take a view similar to that of @cite_0 , arguing that it is possible to build from locally valid program fragments which provide useful insight into the black-box processes generating the data. By combining gradient descent and A* search the @math -machine is able to learn informative and interpretable high-level LISP-like programs, even just from a single observation trace.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2282821441",
                "abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
                "doi": "https://doi.org/10.1145/2939672.2939778",
                "title": "\"Why Should I Trust You?\"",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1705.08180",
        "mid": "2619855679",
        "abstract": "Domain adaptation techniques address the problem of reducing the sensitivity of machine learning methods to the so-called domain shift, namely the difference between source (training) and target (test) data distributions. In particular, unsupervised domain adaptation assumes no labels are available in the target domain. To this end, aligning second order statistics (covariances) of target and source domains have proven to be an effective approach ti fill the gap between the domains. However, covariance matrices do not form a subspace of the Euclidean space, but live in a Riemannian manifold with non-positive curvature, making the usual Euclidean metric suboptimal to measure distances. In this paper, we extend the idea of training a neural network with a constraint on the covariances of the hidden layer features, by rigorously accounting for the curved structure of the manifold of symmetric positive definite matrices. The resulting loss function exploits a theoretically sound geodesic distance on such manifold. Results show indeed the suboptimal nature of the Euclidean distance. This makes us able to perform better than previous approaches on the standard Office dataset, a benchmark for domain adaptation techniques.",
        "related_work": "Domain adaptation techniques address the fundamental problem of @cite_4 between a dataset, used for training and a dataset, used for testing. This issue often arises in machine learning, especially when algorithms are to be deployed on new, possibly unlabeled domains. Domain adaptation strategies can be divided in two classes: and adaptation. The first approach is based on the assumption that a supervised algorithm (usually a classifier) can benefit not only from a fully labeled source dataset, but also from (at least) some labeled data points from the target dataset. Our method belongs instead to the second class of approaches, which assume that no labels are available for the target data. In the following we detail recent methods proposed in literature to cope with this problem.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2031342017",
                "abstract": "Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.",
                "doi": "https://doi.org/10.1109/cvpr.2011.5995347",
                "title": "Unbiased look at dataset bias",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1705.07140",
        "mid": "2894882275",
        "abstract": "Recently a deterministic method, frequent directions (FD) is proposed to solve the high dimensional low rank approximation problem. It works well in practice, but experiences high computational cost. In this paper, we establish a fast frequent directions algorithm for the low rank approximation problem, which implants a randomized algorithm, sparse subspace embedding (SpEmb) in FD. This new algorithm makes use of FD's natural block structure and sends more information through SpEmb to each block in FD. We prove that our new algorithm produces a good low rank approximation with a sketch of size linear on the rank approximated. Its effectiveness and efficiency are demonstrated by the experimental results on both synthetic and real world datasets, as well as applications in network analysis.",
        "related_work": "Since its introduction by Clarkson and Woodruff @cite_28 , SpEmb has received great attention due to its efficient time complexity. It has been extensively used as a randomization technique to solve numerical linear algebraic problems, here we look at its application on low rank approximation.",
        "ref_abstract": {
            "@cite_28": {
                "mid": "2101043704",
                "abstract": "We design a new distribution over poly(r e-1) x n matrices S so that for any fixed n x d matrix A of rank r, with probability at least 9 10, SAx2 = (1 pm e)Ax2 simultaneously for all x \u2208 Rd. Such a matrix S is called a subspace embedding. Furthermore, SA can be computed in O(nnz(A)) + O(r2e-2) time, where nnz(A) is the number of non-zero entries of A. This improves over all previous subspace embeddings, which required at least \u03a9(nd log d) time to achieve this property. We call our matrices S sparse embedding matrices. Using our sparse embedding matrices, we obtain the fastest known algorithms for overconstrained least-squares regression, low-rank approximation, approximating all leverage scores, and lp-regression: to output an x' for which Ax'-b2 \u2264 (1+e)minx Ax-b2 for an n x d matrix A and an n x 1 column vector b, we obtain an algorithm running in O(nnz(A)) + O(d3e-2) time, and another in O(nnz(A)log(1 e)) + O(d3log(1 e)) time. (Here O(f) = f \u22c5 logO(1)(f).) to obtain a decomposition of an n x n matrix A into a product of an n x k matrix L, a k x k diagonal matrix D, and a n x k matrix W, for which F A - L D W \u2264 (1+e)F A-Ak , where Ak is the best rank-k approximation, our algorithm runs in O(nnz(A)) + O(nk2 e-4log n + k3e-5log2n) time. to output an approximation to all leverage scores of an n x d input matrix A simultaneously, with constant relative error, our algorithms run in O(nnz(A) log n) + O(r3) time. to output an x' for which Ax'-bp \u2264 (1+e)minx Ax-bp for an n x d matrix A and an n x 1 column vector b, we obtain an algorithm running in O(nnz(A) log n) + poly(r e-1) time, for any constant 1 \u2264 p",
                "doi": "https://doi.org/10.1145/2488608.2488620",
                "title": "Low rank approximation and regression in input sparsity time",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1705.05278",
        "mid": "2616591008",
        "abstract": "Probability distributions produced by the cross-entropy loss for ordinal classification problems can possess undesired properties. We propose a straightforward technique to constrain discrete ordinal probability distributions to be unimodal via a combination of the Poisson probability mass function and the softmax nonlinearity. We evaluate this approach on two large ordinal image datasets and obtain promising results.",
        "related_work": "There are other ordinal techniques but which do not impose unimodal constraints. The proportional odds model (POM) and its neural network extensions (POMNN, CHNN @cite_2 ) do not suffer from the monotonicity issue due to the utilization of monotonically increasing biases in the calculation of probabilities. The stick-breaking approach by , which is a reformulation of the multinomial logit (softmax), could also be used in the ordinal case as it technically imposes an ordering on classes.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "2067001387",
                "abstract": "Threshold models are one of the most common approaches for ordinal regression, based on projecting patterns to the real line and dividing this real line in consecutive intervals, one interval for each class. However, finding such one-dimensional projection can be too harsh an imposition for some datasets. This paper proposes a multidimensional latent space representation with the purpose of relaxing this projection, where the different classes are arranged based on concentric hyperspheres, each class containing the previous classes in the ordinal scale. The proposal is implemented through a neural network model, each dimension being a linear combination of a common set of basis functions. The model is compared to a nominal neural network, a neural network based on the proportional odds model and to other state-of-the-art ordinal regression methods for a total of 12 datasets. The proposed latent space shows an improvement on the two performance metrics considered, and the model based on the three-dimensional latent space obtains competitive performance when compared to the other methods.",
                "doi": "https://doi.org/10.1016/j.neunet.2014.07.001",
                "title": "Ordinal regression neural networks based on concentric hyperspheres",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1705.03670",
        "mid": "2612399879",
        "abstract": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional time-delay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce high-quality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68 . This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames.",
        "related_work": "Our work is a direct extension of the d-vector model presented by @cite_3 . The extension is two-fold: a CNN TDNN structure that emphasizes on temporal-frequency filtering, more resemble to the traditional feature engineering; an experiment on a text-independent task demonstrated that the learned feature is independent of linguistic content and highly speaker sensitive.",
        "ref_abstract": {
            "@cite_3": {
                "mid": "2046056978",
                "abstract": "In this paper we investigate the use of deep neural networks (DNNs) for a small footprint text-dependent speaker verification task. At development stage, a DNN is trained to classify speakers at the frame-level. During speaker enrollment, the trained DNN is used to extract speaker specific features from the last hidden layer. The average of these speaker features, or d-vector, is taken as the speaker model. At evaluation stage, a d-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision. Experimental results show the DNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task. In addition, the DNN based system is more robust to additive noise and outperforms the i-vector system at low False Rejection operating points. Finally the combined system outperforms the i-vector system by 14 and 25 relative in equal error rate (EER) for clean and noisy conditions respectively.",
                "doi": "https://doi.org/10.1109/icassp.2014.6854363",
                "title": "Deep neural networks for small footprint text-dependent speaker verification",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1705.02498",
        "mid": "2949292735",
        "abstract": "Deep neural networks (DNNs) achieve excellent performance on standard classification tasks. However, under image quality distortions such as blur and noise, classification accuracy becomes poor. In this work, we compare the performance of DNNs with human subjects on distorted images. We show that, although DNNs perform better than or on par with humans on good quality images, DNN performance is still much lower than human performance on distorted images. We additionally find that there is little correlation in errors between DNNs and human subjects. This could be an indication that the internal representation of images are different between DNNs and the human visual system. These comparisons with human performance could be used to guide future development of more robust DNNs.",
        "related_work": "Comparing the performance of machine learning systems with human subjects has attracted interest because it may give insights on how machine learning systems can be improved. Borji and Itti @cite_7 compare human classification accuracy and the accuracy of several machine learning algorithms on several datasets. The study considers images as well as line drawings and jumbled images. The study, however, does not test distorted images, and does not include deep learning algorithms.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "2146215822",
                "abstract": "Several decades of research in computer and primate vision have resulted in many models (some specialized for one problem, others more general) and invaluable experimental data. Here, to help focus research efforts onto the hardest unsolved problems, and bridge computer and human vision, we define a battery of 5 tests that measure the gap between human and machine performances in several dimensions (generalization across scene categories, generalization from images to edge maps and line drawings, invariance to rotation and scaling, local global information with jumbled images, and object recognition performance). We measure model accuracy and the correlation between model and human error patterns. Experimenting over 7 datasets, where human data is available, and gauging 14 well-established models, we find that none fully resembles humans in all aspects, and we learn from each test which models and features are more promising in approaching humans in the tested dimension. Across all tests, we find that models based on local edge histograms consistently resemble humans more, while several scene statistics or \"gist\" models do perform well with both scenes and objects. While computer vision has long been inspired by human vision, we believe systematic efforts, such as this, will help better identify shortcomings of models and find new paths forward.",
                "doi": "https://doi.org/10.1109/cvpr.2014.22",
                "title": "Human vs. Computer in Scene and Object Recognition",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1704.06718",
        "mid": "2609108866",
        "abstract": "Data fusion has become an active research topic in recent years. Growing computational performance has allowed the use of redundant sensors to measure a single phenomenon. While Bayesian fusion approaches are common in general applications, the computer vision field has largely relegated this approach. Most object following algorithms have gone towards pure machine learning fusion techniques that tend to lack flexibility. Consequently, a more general data fusion scheme is needed. Within this work, a hierarchical Bayesian fusion approach is proposed, which outperforms individual trackers by using redundant measurements. The adaptive framework is achieved by relying on each measurement's local statistics and a global softened majority voting. The proposed approach was validated in a simulated application and two robotic platforms.",
        "related_work": "This section describes the different sensor fusion and adaptive sensor fusion approaches, from general algorithms tailored for fusing sensor measurements to more specific algorithms used in computer vision available in the literature. This overview covers some of the latest sensor fusion mechanisms mentioned in @cite_32 , computer vision benchmarks such as @cite_38 and the performance evaluation of some vision-based trackers @cite_7 .",
        "ref_abstract": {
            "@cite_38": {
                "mid": "2089961441",
                "abstract": "Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.",
                "doi": "https://doi.org/10.1109/cvpr.2013.312",
                "title": "Online Object Tracking: A Benchmark",
                "publication_year": 2013
            },
            "@cite_32": {
                "mid": "2038420319",
                "abstract": "There has been an ever-increasing interest in multi-disciplinary research on multisensor data fusion technology, driven by its versatility and diverse areas of application. Therefore, there seems to be a real need for an analytical review of recent developments in the data fusion domain. This paper proposes a comprehensive review of the data fusion state of the art, exploring its conceptualizations, benefits, and challenging aspects, as well as existing methodologies. In addition, several future directions of research in the data fusion community are highlighted and described.",
                "doi": "https://doi.org/10.1016/j.inffus.2011.08.001",
                "title": "Multisensor data fusion: A review of the state-of-the-art",
                "publication_year": 2013
            },
            "@cite_7": {
                "mid": "2158827467",
                "abstract": "This paper addresses the problem of single-target tracker performance evaluation. We consider the performance measures, the dataset and the evaluation system to be the most important components of tracker evaluation and propose requirements for each of them. The requirements are the basis of a new evaluation methodology that aims at a simple and easily interpretable tracker comparison. The ranking-based methodology addresses tracker equivalence in terms of statistical significance and practical differences. A fully-annotated dataset with per-frame annotations with several visual attributes is introduced. The diversity of its visual properties is maximized in a novel way by clustering a large number of videos according to their visual attributes. This makes it the most sophistically constructed and annotated dataset to date. A multi-platform evaluation system allowing easy integration of third-party trackers is presented as well. The proposed evaluation methodology was tested on the VOT2014 challenge on the new dataset and 38 trackers, making it the largest benchmark to date. Most of the tested trackers are indeed state-of-the-art since they outperform the standard baselines, resulting in a highly-challenging benchmark. An exhaustive analysis of the dataset from the perspective of tracking difficulty is carried out. To facilitate tracker comparison a new performance visualization technique is proposed.",
                "doi": "https://doi.org/10.1109/tpami.2016.2516982",
                "title": "A Novel Performance Evaluation Methodology for Single-Target Trackers",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1704.05249",
        "mid": "2608335575",
        "abstract": "To manage and maintain large-scale cellular networks, operators need to know which sectors underperform at any given time. For this purpose, they use the so-called hot spot score, which is the result of a combination of multiple network measurements and reflects the instantaneous overall performance of individual sectors. While operators have a good understanding of the current performance of a network and its overall trend, forecasting the performance of each sector over time is a challenging task, as it is affected by both regular and non-regular events, triggered by human behavior and hardware failures. In this paper, we study the spatio-temporal patterns of the hot spot score and uncover its regularities. Based on our observations, we then explore the possibility to use recent measurements' history to predict future hot spots. To this end, we consider tree-based machine learning models, and study their performance as a function of time, amount of past data, and prediction horizon. Our results indicate that, compared to the best baseline, tree-based models can deliver up to 14 better forecasts for regular hot spots and 153 better forecasts for non-regular hot spots. The latter brings strong evidence that, for moderate horizons, forecasts can be made even for sectors exhibiting isolated, non-regular behavior. Overall, our work provides insight into the dynamics of cellular sectors and their predictability. It also paves the way for more proactive network operations with greater forecasting horizons.",
        "related_work": "After an exhaustive search, we have not been able to find any work that characterizes performance of hot spots in cellular networks using real data. The only exception could be the work by Nika et al. @cite_4 , where a data set with more than 700K users is analyzed to understand temporal characteristics of data hot spots. Predictability is assessed, but considering only hot spot repetitions at the exact time and location in the forthcoming week. However, we find that the focus of the work is not on performance, but on load, with hot spots being defined on the basis of relative daily cell traffic.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2086787845",
                "abstract": "The unprecedented growth in mobile data usage is posing significant challenges to cellular operators. One key challenge is how to provide quality of service to subscribers when their residing cell is experiencing a significant amount of traffic, i.e. becoming a traffic hotspot. In this paper, we perform an empirical study on data hotspots in today's cellular networks using a 9-week cellular dataset with 734K+ users and 5327 cell sites. Our analysis examines in details static and dynamic characteristics, predictability, and causes of data hotspots, and their correlation with call hotspots. We believe the understanding of these key issues will lead to more efficient and responsive resource management and thus better QoS provision in cellular networks. To the best of our knowledge, our work is the first to characterize in detail traffic hotspots in today's cellular networks using real data.",
                "doi": "https://doi.org/10.1109/qshine.2014.6928662",
                "title": "Understanding data hotspots in cellular networks",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1704.01946",
        "mid": "2952630438",
        "abstract": "In the context of Smart Cities, indicator definitions have been used to calculate values that enable the comparison among different cities. The calculation of an indicator values has challenges as the calculation may need to combine some aspects of quality while addressing different levels of abstraction. Knowledge graphs (KGs) have been used successfully to support flexible representation, which can support improved understanding and data analysis in similar settings. This paper presents an operational description for a city KG, an indicator ontology that support indicator discovery and data visualization and an application capable of performing metadata analysis to automatically build and display dashboards according to discovered indicators. We describe our implementation in an urban mobility setting.",
        "related_work": "The process of modeling a city is complex. The intrinsic complexity of interactions between city entities make it very difficult to map relevant sets of dynamic aspects that are often used to characterize a city. Moreover, these entity interactions, along with the numerous entities and processes, differ from one city to another. Thus, the process of modeling the city is typically use-case centered, where the modeling is performed towards a specified goal. This approach, hence, streamlines the process, identifying which characteristics need to be modeled. The work in @cite_1 proposes a core conceptual model for the Domain Knowledge Model of a Smart City, which originally involves multiple domains and cities. The proposed work aims to support cross-domain and cross-city interoperability by specifying terms from different stakeholders. Ontologies play a big role in enabling cross-city comparison. The Semantic Web has been used in the Open Government Data (OGD) approach to make it possible for cities to share information and knowledge under a common vocabulary. Pushing this further, the GCI (Global City Indicators) Ontology @cite_7 is an effort for the modeling of city entities that covers the concepts used by global indicators using Semantic Web technologies.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2203273259",
                "abstract": "As the emergence of the requirement of building Smart Cities across many countries, the Domain Knowledge Model of Smart City (Short as DKM4SC) becomes necessary. The Domain Knowledge Model of Smart City has abundant contents and involves many domains and cities. In order to support cross-domain and cross-city interoperation of knowledge, we should concise the common concepts and their relationship from domains and cities and construct a core concept model (short as CCM of DKM4SC) that can specify terms from different stakeholders, support semantic understanding and give standard knowledge expression. The model is proposed to represent the core concept of Smart City for exchanging data, service. A case study is presented to illustrate the usage of the model.",
                "doi": "https://doi.org/10.1109/isc2.2015.7366228",
                "title": "Toward domain knowledge model for smart city: The core conceptual model",
                "publication_year": 2015
            },
            "@cite_7": {
                "mid": "1891528884",
                "abstract": "Abstract This paper addresses the problem of how city indicators and their supporting data are to be published on the Semantic Web so that automated analysis can be performed. With the publishing of ISO 37120, cities have a standard set of indicators that can be used to compare their performance. The problem is that no standards exist for publishing indicators on the Semantic Web. In this paper we introduce the Global City Indicator Ontology (GCIO). The GCIO addresses five issues: 1) how is meta-data associated with a single indicator value represented? 2) how are indicator definitions represented? 3) how is the data used to derive an indicator value represented? 4) how is indicator theme specific knowledge represented, and 5) how is city specific knowledge represented? The GCIO has been implemented and validated using the City of Toronto ISO 37120 indicators reported for 2013. Research continues in developing ontologies specifically for each of the indicator themes such as: Education, Shelter, Health, Transportation and Innovation.",
                "doi": "https://doi.org/10.1016/j.compenvurbsys.2015.09.009",
                "title": "The role of ontologies in publishing and analyzing city indicators",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1704.01256",
        "mid": "2949183385",
        "abstract": "Traffic congestion is a widespread problem. Dynamic traffic routing systems and congestion pricing are getting importance in recent research. Lane prediction and vehicle density estimation is an important component of such systems. We introduce a novel problem of vehicle self-positioning which involves predicting the number of lanes on the road and vehicle's position in those lanes using videos captured by a dashboard camera. We propose an integrated closed-loop approach where we use the presence of vehicles to aid the task of self-positioning and vice-versa. To incorporate multiple factors and high-level semantic knowledge into the solution, we formulate this problem as a Bayesian framework. In the framework, the number of lanes, the vehicle's position in those lanes and the presence of other vehicles are considered as parameters. We also propose a bounding box selection scheme to reduce the number of false detections and increase the computational efficiency. We show that the number of box proposals decreases by a factor of 6 using the selection approach. It also results in large reduction in the number of false detections. The entire approach is tested on real-world videos and is found to give acceptable results.",
        "related_work": "A similar problem to self-positioning has been handled in @cite_16 using spatial RAY features. It tries to predict the position of the vehicle on the road from an input video stream. However, they predict at most 3 lanes at a time either on the left or right hand side, whereas we have upto 6 lanes in our database with upto 3 lanes being on each side. In addition, there experiments are spread over only 2 days with similar road conditions. Our experiments are more rigorous, spreading over 5 days and the data includes 6 different road conditions. They have low traffic density whereas we have moderate-to-high traffic density. Thus direct comparison between these two approaches may not be possible.",
        "ref_abstract": {
            "@cite_16": {
                "mid": "2019933614",
                "abstract": "Assigning the ego-vehicle to a lane is not only beneficial for navigation but will be an essential element in future Advanced Driver Assistance Systems. This paper describes an approach for ego-lane index estimation using only a monocular camera and no additional sensing equipment like, e.g., the typically employed GPS and Inertial Measurement Unit. Key aspect of the approach are SPatial RAY (SPRAY) features which represent the spatial layout of the road in the visual scene. The proposed method perceives a variety of local visual properties of the scene by means of base classifiers operating on patches extracted from camera images. The spatial arrangement of these local visual properties are captured using SPRAY features. With a boosting classifier trained on these features the ego-lane index is obtained. The system is evaluated on low traffic density and complementary to an object-based approach suitable for heavy traffic. In the conducted experiments, the proposed approach reaches recognition rates of 93 to 97 on individual highway images without applying any kind of temporal filtering.",
                "doi": "https://doi.org/10.1109/ivs.2013.6629613",
                "title": "Visual ego-vehicle lane assignment using Spatial Ray features",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1704.00794",
        "mid": "2950782507",
        "abstract": "Similarity-based approaches represent a promising direction for time series analysis. However, many such methods rely on parameter tuning, and some have shortcomings if the time series are multivariate (MTS), due to dependencies between attributes, or the time series contain missing data. In this paper, we address these challenges within the powerful context of kernel methods by proposing the robust (TCK). The approach taken leverages the missing data handling properties of Gaussian mixture models (GMM) augmented with informative prior distributions. An ensemble learning approach is exploited to ensure robustness to parameters by combining the clustering results of many GMM to form the final kernel. We evaluate the TCK on synthetic and real data and compare to other state-of-the-art techniques. The experimental results demonstrate that the TCK is robust to parameter choices, provides competitive results for MTS without missing data and outstanding results for missing data.",
        "related_work": "The simplest possible approach is to treat the time series as vectors and apply well-known kernels such as a linear or radial basis kernel @cite_32 . While this approach works well in some circumstances, time dependencies and the relationships among multiple attributes in the MTS are not explicitly modeled.",
        "ref_abstract": {
            "@cite_32": {
                "mid": "1560724230",
                "abstract": "From the Publisher: In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs\u0097-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",
                "doi": "https://doi.org/10.7551/mitpress/4175.001.0001",
                "title": "Learning with Kernels",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1704.00399",
        "mid": "2950828162",
        "abstract": "The aggressive spatial spectrum reuse (SSR) by network densification using smaller cells has successfully driven the wireless communication industry onward in the past decades. In our future journey toward ultra-dense networks (UDNs), a fundamental question needs to be answered. Is there a limit to SSR? In other words, when we deploy thousands or millions of small cell base stations (BSs) per square kilometer, is activating all BSs on the same time frequency resource the best strategy? In this paper, we present theoretical analyses to answer such question. In particular, we find that both the signal and interference powers become bounded in practical UDNs with a non-zero BS-to-UE antenna height difference and a finite UE density, which leads to a constant capacity scaling law. As a result, there exists an optimal SSR density that can maximize the network capacity. Hence, the limit to SSR should be considered in the operation of future UDNs.",
        "related_work": "Recently, a few noteworthy studies have followed and revisited the network performance analysis of UDNs using more practical assumptions @cite_8 @cite_18 @cite_9 @cite_10 @cite_12 @cite_3 , such as a general multi-piece path loss model with probabilistic line-of-sight (LoS) and non-LoS (NLoS) transmissions, a non-zero BS-to-UE antenna height difference @math , and a non-fully-loaded network with a finite UE density @math .",
        "ref_abstract": {
            "@cite_18": {
                "mid": "2031858701",
                "abstract": "Millimeter wave (mmWave) holds promise as a carrier frequency for fifth generation cellular networks. Because mmWave signals are sensitive to blockage, prior models for cellular networks operated in the ultra high frequency (UHF) band do not apply to analyze mmWave cellular networks directly. Leveraging concepts from stochastic geometry, this paper proposes a general framework to evaluate the coverage and rate performance in mmWave cellular networks. Using a distance-dependent line-of-site (LOS) probability function, the locations of the LOS and non-LOS base stations are modeled as two independent non-homogeneous Poisson point processes, to which different path loss laws are applied. Based on the proposed framework, expressions for the signal-to-noise-and-interference ratio (SINR) and rate coverage probability are derived. The mmWave coverage and rate performance are examined as a function of the antenna geometry and base station density. The case of dense networks is further analyzed by applying a simplified system model, in which the LOS region of a user is approximated as a fixed LOS ball. The results show that dense mmWave networks can achieve comparable coverage and much higher data rates than conventional UHF cellular systems, despite the presence of blockages. The results suggest that the cell size to achieve the optimal SINR scales with the average size of the area that is LOS to a user.",
                "doi": "https://doi.org/10.1109/twc.2014.2364267",
                "title": "Coverage and Rate Analysis for Millimeter-Wave Cellular Networks",
                "publication_year": 2015
            },
            "@cite_8": {
                "mid": "2134686390",
                "abstract": "Existing cellular network analyses, and even simulations, typically use the standard path loss model where received power decays like @math over a distance @math . This standard path loss model is quite idealized, and in most scenarios the path loss exponent @math is itself a function of @math , typically an increasing one. Enforcing a single path loss exponent can lead to orders of magnitude differences in average received and interference powers versus the true values. In this paper, we study multi-slope path loss models, where different distance ranges are subject to different path loss exponents. We focus on the dual-slope path loss function, which is a piece-wise power law and continuous and accurately approximates many practical scenarios. We derive the distributions of SIR, SNR, and finally SINR before finding the potential throughput scaling, which provides insight on the observed cell-splitting rate gain. The exact mathematical results show that the SIR monotonically decreases with network density, while the converse is true for SNR, and thus the network coverage probability in terms of SINR is maximized at some finite density. With ultra-densification (network density goes to infinity), there exists a phase transition in the near-field path loss exponent @math : if @math unbounded potential throughput can be achieved asymptotically; if $ 0 , ultra-densification leads in the extreme case to zero throughput.",
                "doi": "https://doi.org/10.1109/tcomm.2015.2413412",
                "title": "Downlink Cellular Network Analysis With Multi-Slope Path Loss Models",
                "publication_year": 2015
            },
            "@cite_9": {
                "mid": "2261127636",
                "abstract": "In this paper, we introduce a sophisticated path loss model incorporating both line-of-sight (LoS) and non-line-of-sight (NLoS) transmissions to study their impact on the performance of dense small cell networks (SCNs). Analytical results are obtained for the coverage probability and the area spectral efficiency (ASE), assuming both a general path loss model and a special case with a linear LoS probability function. The performance impact of LoS and NLoS transmissions in dense SCNs in terms of the coverage probability and the ASE is significant, both quantitatively and qualitatively, compared with the previous work that does not differentiate LoS and NLoS transmissions. Our analysis demonstrates that the network coverage probability first increases with the increase of the base station (BS) density, and then decreases as the SCN becomes denser. This decrease further makes the ASE suffer from a slow growth or even a decrease with network densification. The ASE will grow almost linearly as the BS density goes ultra dense. For practical regime of the BS density, the performance results derived from our analysis are distinctively different from previous results, and thus shed new insights on the design and deployment of future dense SCNs.",
                "doi": "https://doi.org/10.1109/twc.2015.2503391",
                "title": "Performance Impact of LoS and NLoS Transmissions in Dense Cellular Networks",
                "publication_year": 2016
            },
            "@cite_3": {
                "mid": "2560910596",
                "abstract": "Very recent studies showed that in a fully loaded dense small cell network (SCN), the coverage probability performance will continuously decrease with the network densification. Such new results were captured in IEEE ComSoc Technology News with an alarming title of \"Will Densification Be the Death of 5G?\". In this paper, we revisit this issue from more practical views of realistic network deployment, such as a finite number of active base stations (BSs) and user equipments (UEs), a decreasing BS transmission power with the network densification, etc. Particularly, in dense SCNs, due to an oversupply of BSs with respect to UEs, a large number of BSs can be put into idle modes without signal transmission, if there is no active UE within their coverage areas. Setting those BSs into idle modes mitigates unnecessary inter-cell interference and reduces energy consumption. In this paper, we investigate the performance impact of such BS idle mode capability (IMC) on dense SCNs. Different from existing work, we consider a realistic path loss model incorporating both line-of-sight (LoS) and non-line-of-sight (NLoS) transmissions. Moreover, we obtain analytical results for the coverage probability, the area spectral efficiency (ASE) and the energy efficiency (EE) performance for SCNs with the BS IMC and show that the performance impact of the IMC on dense SCNs is significant. As the BS density surpasses the UE density in dense SCNs, the coverage probability will continuously increase toward one, addressing previous concerns on \"the death of 5G\". Finally, the performance improvement in terms of the EE performance is also investigated for dense SCNs using practical energy models developed in the Green-Touch project.",
                "doi": "https://doi.org/10.48550/arxiv.1609.07710",
                "title": "Performance Impact of Idle Mode Capability on Dense Small Cell Networks",
                "publication_year": 2016
            },
            "@cite_10": {
                "mid": "2963469284",
                "abstract": "The intensity matching approach for tractable performance evaluation and optimization of cellular networks is introduced. It assumes that the base stations are modeled as the points of a Poisson point process (PPP) and leverages stochastic geometry for system-level analysis. Its rationale relies on observing that system-level performance is determined by the intensity measure of transformations of the underlaying spatial PPP. By approximating the original system model with a simplified one, whose performance is determined by a mathematically convenient intensity measure, tractable yet accurate integral expressions for computing area spectral efficiency and potential throughput are provided. The considered system model accounts for many practical aspects that, for tractability, are typically neglected, e.g., line-of-sight (LOS) and non-LOS propagation, antenna radiation patterns, traffic load, practical cell associations, and general fading channels. The proposed approach, more importantly, is conveniently formulated for unveiling the impact of several system parameters, e.g., the density of base stations and blockages. The effectiveness of this novel and general methodology is validated with the aid of empirical data for the locations of base stations and for the footprints of buildings in dense urban environments.",
                "doi": "https://doi.org/10.1109/twc.2016.2574852",
                "title": "The Intensity Matching Approach: A Tractable Stochastic Geometry Approximation to System-Level Analysis of Cellular Networks",
                "publication_year": 2016
            },
            "@cite_12": {
                "mid": "2963171323",
                "abstract": "In this paper, we present a new and significant theoretical discovery. If the absolute height difference between base station (BS) antenna and user equipment (UE) antenna is larger than zero, then the network performance in terms of both the coverage probability and the area spectral efficiency will continuously decrease toward zero as the BS density increases in ultra-dense networks (UDNs). Such findings are completely different from the conclusions in the existing works, both quantitatively and qualitatively. In particular, this performance behavior has a tremendous impact on the deployment of the 5th-generation UDNs. Network operators may invest large amounts of money in deploying more network infrastructure only to obtain even less network capacity. This paper\u2019s results reveal that one way to address this issue is to lower the BS antenna height to the UE antenna height. However, this requires a revolutionized approach of the BS architecture and deployment, which is also explored in this paper.",
                "doi": "https://doi.org/10.1109/twc.2017.2757924",
                "title": "Performance Impact of Base Station Antenna Heights in Dense Cellular Networks",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1703.08314",
        "mid": "2604327526",
        "abstract": "The categorical compositional approach to meaning has been successfully applied in natural language processing, outperforming other models in mainstream empirical language processing tasks. We show how this approach can be generalized to conceptual space models of cognition. In order to do this, first we introduce the category of convex relations as a new setting for categorical compositional semantics, emphasizing the convex structure important to conceptual space applications. We then show how to construct conceptual spaces for various types such as nouns, adjectives and verbs. Finally we show by means of examples how concepts can be systematically combined to establish the meanings of composite phrases from the meanings of their constituent parts. This provides the mathematical underpinnings of a new compositional approach to cognition.",
        "related_work": "In the current work, we have restricted ourselves to grammatical composition, and in particular pregroup grammar. However, the categorical compositional scheme can be instantiated in a number of ways. The grammar can be changed from pregroup grammar to another categorial grammar, as in @cite_20 , or a compositional scheme that is not grammatically based may be used. Indeed, one of the challenges of this approach is to find a model of composition that accurately reflects human behaviour. One way of doing so would be to use an approach in which the syntactic scheme is generated by the semantics of the universe of discourse. Furthermore, since phrases and sentences are represented as sets equipped with a convex algebra, the model can in future work be extended to include logical composition.",
        "ref_abstract": {
            "@cite_20": {
                "mid": "2058503912",
                "abstract": "The Distributional Compositional Categorical (DisCoCat) model is a mathematical framework that provides compositional semantics for meanings of natural language sentences. It consists of a computational procedure for constructing meanings of sentences, given their grammatical structure in terms of compositional type-logic, and given the empirically derived meanings of their words. For the particular case that the meaning of words is modelled within a distributional vector space model, its experimental predictions, derived from real large scale data, have outperformed other empirically validated methods that could build vectors for a full sentence. This success can be attributed to a conceptually motivated mathematical underpinning, something which the other methods lack, by integrating qualitative compositional type-logic and quantitative modelling of meaning within a category-theoretic mathematical framework. The type-logic used in the DisCoCat model is Lambek\u02bcs pregroup grammar. Pregroup types form a poset al compact closed category, which can be passed, in a functorial manner, on to the compact closed structure of vector spaces, linear maps and tensor product. The diagrammatic versions of the equational reasoning in compact closed categories can be interpreted as the flow of word meanings within sentences. Pregroups simplify Lambek\u02bcs previous type-logic, the Lambek calculus. The latter and its extensions have been extensively used to formalise and reason about various linguistic phenomena. Hence, the apparent reliance of the DisCoCat on pregroups has been seen as a shortcoming. This paper addresses this concern, by pointing out that one may as well realise a functorial passage from the original type-logic of Lambek, a monoidal bi-closed category, to vector spaces, or to any other model of meaning organised within a monoidal bi-closed category. The corresponding string diagram calculus, due to Baez and Stay, now depicts the flow of word meanings, and also reflects the structure of the parse trees of the Lambek calculus.",
                "doi": "https://doi.org/10.1016/j.apal.2013.05.009",
                "title": "Lambek vs. Lambek: Functorial vector space semantics and string diagrams for Lambek calculus",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1703.06520",
        "mid": "2950143680",
        "abstract": "Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decompose text into two locally detectable elements, namely segments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining segments connected by links. Compared with previous methods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0 on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512x512 images. Moreover, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.",
        "related_work": "Our method is similar in spirit to a recent work @cite_29 , which detects text lines by finding and grouping a sequence of through a CNN coupled with recurrent neural layers. In contrast, we detect oriented segments only using convolutional layers, yielding better flexibility and faster speed. Also, we detect links explicitly using the same strong CNN features for segments, improving the robustness.",
        "ref_abstract": {
            "@cite_29": {
                "mid": "2519818067",
                "abstract": "We propose a novel Connectionist Text Proposal Network (CTPN) that accurately localizes text lines in natural image. The CTPN detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps. We develop a vertical anchor mechanism that jointly predicts location and text non-text score of each fixed-width proposal, considerably improving localization accuracy. The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model. This allows the CTPN to explore rich context information of image, making it powerful to detect extremely ambiguous text. The CTPN works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering. It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8, 35] by a large margin. The CTPN is computationally efficient with 0.14 s image, by using the very deep VGG16 model [27]. Online demo is available: http: textdet.com .",
                "doi": "https://doi.org/10.1007/978-3-319-46484-8_4",
                "title": "Detecting Text in Natural Image with Connectionist Text Proposal Network",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1703.04699",
        "mid": "2602764693",
        "abstract": "This paper addresses the problem of simultaneous 3D reconstruction and material recognition and segmentation. Enabling robots to recognise different materials (concrete, met al etc.) in a scene is important for many tasks, e.g. robotic interventions in nuclear decommissioning. Previous work on 3D semantic reconstruction has predominantly focused on recognition of everyday domestic objects (tables, chairs etc.), whereas previous work on material recognition has largely been confined to single 2D images without any 3D reconstruction. Meanwhile, most 3D semantic reconstruction methods rely on computationally expensive post-processing, using Fully-Connected Conditional Random Fields (CRFs), to achieve consistent segmentations. In contrast, we propose a deep learning method which performs 3D reconstruction while simultaneously recognising different types of materials and labeling them at the pixel level. Unlike previous methods, we propose a fully end-to-end approach, which does not require hand-crafted features or CRF post-processing. Instead, we use only learned features, and the CRF segmentation constraints are incorporated inside the fully end-to-end learned system. We present the results of experiments, in which we trained our system to perform real-time 3D semantic reconstruction for 23 different materials in a real-world application. The run-time performance of the system can be boosted to around 10Hz, using a conventional GPU, which is enough to achieve realtime semantic reconstruction using a 30fps RGB-D camera. To the best of our knowledge, this work is the first real-time end-to-end system for simultaneous 3D reconstruction and material recognition.",
        "related_work": "Recent real-time 3D reconstruction and SLAM approaches, e.g. @cite_6 (Visual-features with RANSAC), @cite_4 (Direct image alignment based on optimization), or @cite_21 (Point cloud alignment based on ICP) can effectively generate dense or semi-dense 3D maps, but they have no understanding of the observed scenes and objects.",
        "ref_abstract": {
            "@cite_21": {
                "mid": "1987648924",
                "abstract": "We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.",
                "doi": "https://doi.org/10.1109/ismar.2011.6092378",
                "title": "KinectFusion: Real-time dense surface mapping and tracking",
                "publication_year": 2011
            },
            "@cite_4": {
                "mid": "612478963",
                "abstract": "We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on ( sim (3) ), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.",
                "doi": "https://doi.org/10.1007/978-3-319-10605-2_54",
                "title": "LSD-SLAM: Large-Scale Direct Monocular SLAM",
                "publication_year": 2014
            },
            "@cite_6": {
                "mid": "2069479606",
                "abstract": "In this paper, we present a novel mapping system that robustly generates highly accurate 3-D maps using an RGB-D camera. Our approach requires no further sensors or odometry. With the availability of low-cost and light-weight RGB-D sensors such as the Microsoft Kinect, our approach applies to small domestic robots such as vacuum cleaners, as well as flying robots such as quadrocopters. Furthermore, our system can also be used for free-hand reconstruction of detailed 3-D models. In addition to the system itself, we present a thorough experimental evaluation on a publicly available benchmark dataset. We analyze and discuss the influence of several parameters such as the choice of the feature descriptor, the number of visual features, and validation methods. The results of the experiments demonstrate that our system can robustly deal with challenging scenarios such as fast camera motions and feature-poor environments while being fast enough for online operation. Our system is fully available as open source and has already been widely adopted by the robotics community.",
                "doi": "https://doi.org/10.1109/tro.2013.2279412",
                "title": "3-D Mapping With an RGB-D Camera",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1703.03098",
        "mid": "2604365427",
        "abstract": "3D scene understanding is important for robots to interact with the 3D world in a meaningful way. Most previous works on 3D scene understanding focus on recognizing geometrical or semantic properties of the scene independently. In this work, we introduce Data Associated Recurrent Neural Networks (DA-RNNs), a novel framework for joint 3D scene mapping and semantic labeling. DA-RNNs use a new recurrent neural network architecture for semantic labeling on RGB-D videos. The output of the network is integrated with mapping techniques such as KinectFusion in order to inject semantic information into the reconstructed 3D scene. Experiments conducted on a real world dataset and a synthetic dataset with RGB-D videos demonstrate the ability of our method in semantic 3D scene mapping.",
        "related_work": "In principle, our DA-RNN framework only requires dense data associations between consecutive frames. It is thus independent of the underlying representation and could be combined with any of the reconstruction techniques described above. Here, we use KinectFusion @cite_10 to achieve a volumetric representation for geometry and semantics.",
        "ref_abstract": {
            "@cite_10": {
                "mid": "1987648924",
                "abstract": "We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.",
                "doi": "https://doi.org/10.1109/ismar.2011.6092378",
                "title": "KinectFusion: Real-time dense surface mapping and tracking",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1703.00754",
        "mid": "2593723494",
        "abstract": "Simultaneous Localization and Mapping using RGB-D cameras has been a fertile research topic in the latest decade, due to the suitability of such sensors for indoor robotics. In this paper we propose a direct RGB-D SLAM algorithm with state-of-the-art accuracy and robustness at a los cost. Our experiments in the RGB-D TUM dataset [34] effectively show a better accuracy and robustness in CPU real time than direct RGB-D SLAM systems that make use of the GPU. The key ingredients of our approach are mainly two. Firstly, the combination of a semi-dense photometric and dense geometric error for the pose tracking (see Figure 1), which we demonstrate to be the most accurate alternative. And secondly, a model of the multi-view constraints and their errors in the mapping and tracking threads, which adds extra information over other approaches. We release the open-source implementation of our approach 1 . The reader is referred to a video with our results 2 for a more illustrative visualization of its performance.",
        "related_work": "One of the first approaches for direct RGB-D odometry is KinectFusion @cite_7 , which uses only the depth channel D to estimate the odometry and a dense map and discards the RGB information. As its main limitations, it is restricted to small workspaces and will probably fail if the scene does not contain enough geometric structure.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "1987648924",
                "abstract": "We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.",
                "doi": "https://doi.org/10.1109/ismar.2011.6092378",
                "title": "KinectFusion: Real-time dense surface mapping and tracking",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1702.03706",
        "mid": "2593630957",
        "abstract": "In this paper, we developed a deep neural network (DNN) that learns to solve simultaneously the three tasks of the cQA challenge proposed by the SemEval-2016 Task 3, i.e., question-comment similarity, question-question similarity and new question-comment similarity. The latter is the main task, which can exploit the previous two for achieving better results. Our DNN is trained jointly on all the three cQA tasks and learns to encode questions and comments into a single vector representation shared across the multiple tasks. The results on the official challenge test set show that our approach produces higher accuracy and faster convergence rates than the individual neural networks. Additionally, our method, which does not use any manual feature engineering, approaches the state of the art established with methods that make heavy use of it.",
        "related_work": "Finally, trained a long short-term memory model for selecting answers to TREC questions. Their model takes words from question and answer sentences as input and returns a score measuring the relevancy of an answer with respect to a given question. A recent work close to ours is @cite_5 , where the authors build a neural network for solving Task A of SemEval. However, this does not approach the problem as MTL.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2512273476",
                "abstract": "We explore the applicability of machine translation evaluation (MTE) methods to a very different problem: answer ranking in community Question Answering. In particular, we adopt a pairwise neural network (NN) architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings, and which efficiently models complex non-linear interactions. The evaluation results show state-of-the-art performance, with sizeable contribution from both the MTE features and from the pairwise NN architecture.",
                "doi": "https://doi.org/10.18653/v1/p16-2075",
                "title": "Machine Translation Evaluation Meets Community Question Answering",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1702.02530",
        "mid": "2587306891",
        "abstract": "This paper proposes a generic classification system designed to detect security threats based on the behavior of malware samples. The system relies on statistical features computed from proxy log fields to train detectors using a database of malware samples. The behavior detectors serve as basic reusable building blocks of the multi-level detection architecture. The detectors identify malicious communication exploiting encrypted URL strings and domains generated by a Domain Generation Algorithm (DGA) which are frequently used in Command and Control (C&C), phishing, and click fraud. Surprisingly, very precise detectors can be built given only a limited amount of information extracted from a single proxy log. This way, the computational requirements of the detectors are kept low which allows for deployment on a wide range of security devices and without depending on traffic context such as DNS logs, Whois records, webpage content, etc. Results on several weeks of live traffic from 100+ companies having 350k+ hosts show correct detection with a precision exceeding 95 of malicious flows, 95 of malicious URLs and 90 of infected hosts. In addition, a comparison with a signature and rule-based solution shows that our system is able to detect significant amount of new threats.",
        "related_work": "Network-based Intrusion Detection Systems (NIDS) are deployed on various networks segments, typically at the edge of the network, to monitor all incoming and outgoing network traffic @cite_2 @cite_51 . The systems use different strategies to detect malicious communication which can be broadly characterized into systems analyzing payload or payload statistics, systems processing proxy logs or Netflows, systems that build rules or signatures in a separate controlled environment, and systems relying on additional data sources such as DNS records.",
        "ref_abstract": {
            "@cite_51": {
                "mid": "433644524",
                "abstract": "With the increasing amount of network throughput and security threat, the study of intrusion detection systems (IDSs) has received a lot of attention throughout the computer science field. Current IDSs pose challenges on not only capricious intrusion categories, but also huge computational power. Though there is a number of existing literatures to IDS issues, we attempt to give a more elaborate image for a comprehensive review. Through the extensive survey and sophisticated organization, we propose the taxonomy to outline modern IDSs. In addition, tables and figures we summarized in the content contribute to easily grasp the overall picture of IDSs.",
                "doi": "https://doi.org/10.1016/j.jnca.2012.09.004",
                "title": "Intrusion detection system: A comprehensive review",
                "publication_year": 2013
            },
            "@cite_2": {
                "mid": "1489073918",
                "abstract": "In this paper, we survey different intrusions affecting availability, confidentiality and integrity of Cloud resources and services. Proposals incorporating Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS) in Cloud are examined. We recommend IDS IPS positioning in Cloud environment to achieve desired security in the next generation networks.",
                "doi": "https://doi.org/10.1016/j.jnca.2012.05.003",
                "title": "A survey of intrusion detection techniques in Cloud",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1702.01304",
        "mid": "2952374968",
        "abstract": "Predicting a person's gender based on the iris texture has been explored by several researchers. This paper considers several dimensions of experimental work on this problem, including person-disjoint train and test, and the effect of cosmetics on eyelash occlusion and imperfect segmentation. We also consider the use of multi-layer perceptron and convolutional neural networks as classifiers, comparing the use of data-driven and hand-crafted features. Our results suggest that the gender-from-iris problem is more difficult than has so far been appreciated. Estimating accuracy using a mean of N person-disjoint train and test partitions, and considering the effect of makeup - a combination of experimental conditions not present in any previous work - we find a much weaker ability to predict gender-from-iris texture than has been suggested in previous work.",
        "related_work": "The extraction of ancillary information from biometric traits is known as . As defined by Dantcheva al @cite_14 , \"[s]oft biometric traits are physical, behavioral, or material accessories, which are associated with an individual, and which can be useful for recognizing an individual.\"",
        "ref_abstract": {
            "@cite_14": {
                "mid": "297909767",
                "abstract": "Recent research has explored the possibility of extracting ancillary information from primary biometric traits viz., face, fingerprints, hand geometry, and iris. This ancillary information includes personal attributes, such as gender, age, ethnicity, hair color, height, weight, and so on. Such attributes are known as soft biometrics and have applications in surveillance and indexing biometric databases. These attributes can be used in a fusion framework to improve the matching accuracy of a primary biometric system (e.g., fusing face with gender information), or can be used to generate qualitative descriptions of an individual (e.g., young Asian female with dark eyes and brown hair). The latter is particularly useful in bridging the semantic gap between human and machine descriptions of the biometric data. In this paper, we provide an overview of soft biometrics and discuss some of the techniques that have been proposed to extract them from the image and the video data. We also introduce a taxonomy for organizing and classifying soft biometric attributes, and enumerate the strengths and limitations of these attributes in the context of an operational biometric system. Finally, we discuss open research problems in this field. This survey is intended for researchers and practitioners in the field of biometrics.",
                "doi": "https://doi.org/10.1109/tifs.2015.2480381",
                "title": "What Else Does Your Biometric Data Reveal? A Survey on Soft Biometrics",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1701.08376",
        "mid": "2952348863",
        "abstract": "In this paper we present an on-manifold sequence-to-sequence learning approach to motion estimation using visual and inertial sensors. It is to the best of our knowledge the first end-to-end trainable method for visual-inertial odometry which performs fusion of the data at an intermediate feature-representation level. Our method has numerous advantages over traditional approaches. Specifically, it eliminates the need for tedious manual synchronization of the camera and IMU as well as eliminating the need for manual calibration between the IMU and camera. A further advantage is that our model naturally and elegantly incorporates domain specific information which significantly mitigates drift. We show that our approach is competitive with state-of-the-art traditional methods when accurate calibration data is available and can be trained to outperform them in the presence of calibration and synchronization errors.",
        "related_work": "VO algorithms estimate the incremental ego-motion of a camera. A traditional VO algorithm, illustrated in Fig. a., operates by extracting features in an image, matching the features between the current and successive images and then computing the optical flow. The motion can then be computed using the optical flow. The fast semi-direct monocular visual odometry (SVO) algorithm @cite_11 is an example of a state-of-the-art VO algorithm. It is designed to be fast and robust by operating directly on the image patches, not relying on slow feature extraction. Instead, it uses probabilistic depth filters on patches of the image itself. The depth filters are then updated through whole image alignment. This visual odometry algorithm is efficient and runs in real-time on an embedded platform. Its probabilistic formulation, however, makes it difficult to tune and it also requires a bootstrapping procedure to start the process. As expected, its performance depends heavily on the hardware to prevent tracking failures - typically global shutter cameras operating at higher than 50 fps needs to be used to ensure the odometry estimates remain accurate.",
        "ref_abstract": {
            "@cite_11": {
                "mid": "1970504153",
                "abstract": "We propose a semi-direct monocular visual odometry algorithm that is precise, robust, and faster than current state-of-the-art methods. The semi-direct approach eliminates the need of costly feature extraction and robust matching techniques for motion estimation. Our algorithm operates directly on pixel intensities, which results in subpixel precision at high frame-rates. A probabilistic mapping method that explicitly models outlier measurements is used to estimate 3D points, which results in fewer outliers and more reliable points. Precise and high frame-rate motion estimation brings increased robustness in scenes of little, repetitive, and high-frequency texture. The algorithm is applied to micro-aerial-vehicle state-estimation in GPS-denied environments and runs at 55 frames per second on the onboard embedded computer and at more than 300 frames per second on a consumer laptop. We call our approach SVO (Semi-direct Visual Odometry) and release our implementation as open-source software.",
                "doi": "https://doi.org/10.1109/icra.2014.6906584",
                "title": "SVO: Fast semi-direct monocular visual odometry",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1701.08475",
        "mid": "2584781532",
        "abstract": "Nearest neighbor search is known as a challenging issue that has been studied for several decades. Recently, this issue becomes more and more imminent in viewing that the big data problem arises from various fields. In this paper, a scalable solution based on hill-climbing strategy with the support of k-nearest neighbor graph (kNN) is presented. Two major issues have been considered in the paper. Firstly, an efficient kNN graph construction method based on two means tree is presented. For the nearest neighbor search, an enhanced hill-climbing procedure is proposed, which sees considerable performance boost over original procedure. Furthermore, with the support of inverted indexing derived from residue vector quantization, our method achieves close to 100 recall with high speed efficiency in two state-of-the-art evaluation benchmarks. In addition, a comparative study on both the compressional and traditional nearest neighbor search methods is presented. We show that our method achieves the best trade-off between search quality, efficiency and memory complexity.",
        "related_work": "Recently, the hill-climbing strategy has been also explored @cite_10 @cite_6 @cite_27 . Basically, no sophisticated space partitioning is required in the original idea @cite_10 . The NNS starts from a group of random seeds (random location in the vector space). The iterative procedure tranverses over the kNN graph by depth-first search. Guided by kNN graph, the search procedure ascents closer to the true nearest neighbor in each round. It takes only few number of rounds to converge. Since the procedure starts from random position, it is likely to be trapped in local optima. To alleviate this issue, an intervention scheme is introduced in @cite_6 . However, this method turns out to be sub-optimal. Recently, this issue is addressed by indexing the reference set by multiple k-d trees, which is similar as FLANN @cite_3 . When one query comes, the search tranverses over these k-d trees. Potential candidates are collected as the seeds for hill-climbing search. Such that the search process could be faster and the possibility that is trapped in a local optima is lower. Unfortunately, such kind of indexing causes nearly one times of memory overhead to load the k-d trees, which makes it inscalable to large-scale search task.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2526367648",
                "abstract": "Approximate nearest neighbor (ANN) search is a fundamental problem in many areas of data mining, machine learning and computer vision. The performance of traditional hierarchical structure (tree) based methods decreases as the dimensionality of data grows, while hashing based methods usually lack efficiency in practice. Recently, the graph based methods have drawn considerable attention. The main idea is that , which we refer as . These methods construct a @math -nearest neighbor ( @math NN) graph offline. And at online search stage, these methods find candidate neighbors of a query point in some way ( , random selection), and then check the neighbors of these candidate neighbors for closer ones iteratively. Despite some promising results, there are mainly two problems with these approaches: 1) These approaches tend to converge to local optima. 2) Constructing a @math NN graph is time consuming. We find that these two problems can be nicely solved when we provide a good initialization for NN-expansion. In this paper, we propose EFANNA, an extremely fast approximate nearest neighbor search algorithm based on @math NN Graph. Efanna nicely combines the advantages of hierarchical structure based methods and nearest-neighbor-graph based methods. Extensive experiments have shown that EFANNA outperforms the state-of-art algorithms both on approximate nearest neighbor search and approximate nearest neighbor graph construction. To the best of our knowledge, EFANNA is the fastest algorithm so far both on approximate nearest neighbor graph construction and approximate nearest neighbor search. A library EFANNA based on this research is released on Github.",
                "doi": "https://doi.org/10.48550/arxiv.1609.07228",
                "title": "EFANNA : An Extremely Fast Approximate Nearest Neighbor Search Algorithm\n  Based on kNN Graph",
                "publication_year": 2016
            },
            "@cite_3": {
                "mid": "2086504823",
                "abstract": "For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.",
                "doi": "https://doi.org/10.1109/tpami.2014.2321376",
                "title": "Scalable Nearest Neighbor Algorithms for High Dimensional Data",
                "publication_year": 2014
            },
            "@cite_10": {
                "mid": "17346433",
                "abstract": "We introduce a new nearest neighbor search algorithm. The algorithm builds a nearest neighbor graph in an offline phase and when queried with a new point, performs hill-climbing starting from a randomly sampled node of the graph. We provide theoretical guarantees for the accuracy and the computational complexity and empirically show the effectiveness of this algorithm.",
                "doi": "https://doi.org/10.5591/978-1-57735-516-8/ijcai11-222",
                "title": "Fast approximate nearest-neighbor search with k-nearest neighbor graph",
                "publication_year": 2011
            },
            "@cite_6": {
                "mid": "2121456571",
                "abstract": "In this paper, we address the approximate nearest neighbor (ANN) search problem over large scale visual descriptors. We investigate a simple but very effective approach, neighborhood graph search, which constructs a neighborhood graph to index the data points and conducts a local search, expanding neighborhoods with a best-first manner, for ANN search. Our empirical analysis shows that neighborhood expansion is very efficient, with O(1) cost, for a new NN candidate location, and has high chances to locate true NNs and hence it usually performs well. However, it often gets sub-optimal solutions since local search only checks the neighborhood of the current solution, or conducts exhaustive and continuous neighborhood expansions to find better solutions, which deteriorates the query efficiency. In this paper, we propose a query-driven iterated neighborhood graph search approach to improve the performance. We follow the iterated local search (ILS) strategy, widely-used in combinatorial optimization, to find a solution beyond a local optimum. We handle the key challenge in making neighborhood graph search adapt to ILS, Perturbation, which generates a new pivot to restart a local search. To this end, we present a criterion to check if the local search over a neighborhood graph arrives at the local solution. Moreover, we exploit the query and search history to design the perturbation scheme, resulting in a more effective search. The major benefit is avoiding unnecessary neighborhood expansions and hence more efficiently finding true NNs. Experimental results on large scale SIFT matching, similar image search, and shape retrieval with non-metric distance measures, show that our approach performs much better than previous state-of-the-art ANN search approaches.",
                "doi": "https://doi.org/10.1145/2393347.2393378",
                "title": "Query-driven iterated neighborhood graph search for large scale indexing",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1701.07867",
        "mid": "2585665795",
        "abstract": "We introduce a space of stable meromorphic differentials with poles of prescribed orders and define its tautological cohomology ring. This space, just as the space of holomorphic differentials, is stratified according to the set of multiplicities of zeros of the differential. The main goal of this paper is to compute the Poincar 'e-dual cohomology classes of all strata. We prove that all these classes are tautological and give an algorithm to compute them. In a second part of the paper we study the Picard group of the strata. We use the tools introduced in the first part to deduce several relations in these Picard groups.",
        "related_work": "* Incidence variety compactification. The problem of the compactification of the strata is extensively studied from different approaches in a joint work of Bainbridge, Chen, Gendron, Grushevsky, and Moeller (see @cite_13 and @cite_26 ). Their compactification (called incidence variety compactification ) is slightly different from the one that we use here. We will recall their definitions in since we will make use of some of their results.",
        "ref_abstract": {
            "@cite_26": {
                "mid": "1743277569",
                "abstract": "The main goal of this work is to construct and study a reasonable compactification of the strata of the moduli space of Abelian differentials. This allows us to compute the Kodaira dimension of some strata of the moduli space of Abelian differentials. The main ingredients to study the compactifications of the strata are a version of the plumbing cylinder construction for differential forms and an extension of the parity of the connected components of the strata to the differentials on curves of compact type. We study in detail the compactifications of the hyperelliptic minimal strata and of the odd minimal stratum in genus three.",
                "doi": "https://doi.org/10.5802/aif.3187",
                "title": "The Deligne\u2013Mumford and the Incidence Variety Compactifications of the Strata of \\Omega \\protect \\mathcal{M}_{g}",
                "publication_year": 2018
            },
            "@cite_13": {
                "mid": "2342521385",
                "abstract": "We describe the closure of the strata of abelian differentials with prescribed type of zeros and poles, in the projectivized Hodge bundle over the Deligne-Mumford moduli space of stable curves with marked points. We provide an explicit characterization of pointed stable differentials in the boundary of the closure, both a complex analytic proof and a flat geometric proof for smoothing the boundary differentials, and numerous examples. The main new ingredient in our description is a global residue condition arising from a full order on the dual graph of a stable curve.",
                "doi": "https://doi.org/10.1215/00127094-2018-0012",
                "title": "Compactification of strata of Abelian differentials",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1701.01565",
        "mid": "2578423909",
        "abstract": "Reproducing experiments is an important instrument to validate previous work and build upon existing approaches. It has been tackled numerous times in different areas of science. In this paper, we introduce an empirical replicability study of three well-known algorithms for syntactic centric aspect-based opinion mining. We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details. We consider these are important threats to validity of the research on the field, specifically when compared to other problems in NLP where public datasets and code availability are critical validity components. We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances.",
        "related_work": "On the other hand, existing supervised approaches in the field are mainly based on sequence labeling. Since 2014 the SemEval workshop included a shared task on the topic @cite_2 , which has also encouraged the development of new supervised methods. We find approaches based on CRFs such as and deep learning @cite_10 @cite_3 , @cite_9 .",
        "ref_abstract": {
            "@cite_9": {
                "mid": "2252007242",
                "abstract": "Open domain targeted sentiment is the joint information extraction task that finds target mentions together with the sentiment towards each mention from a text corpus. The task is typically modeled as a sequence labeling problem, and solved using state-of-the-art labelers such as CRF. We empirically study the effect of word embeddings and automatic feature combinations on the task by extending a CRF baseline using neural networks, which have demonstrated large potentials for sentiment analysis. Results show that the neural model can give better results by significantly increasing the recall. In addition, we propose a novel integration of neural and discrete features, which combines their relative advantages, leading to significantly higher results compared to both baselines.",
                "doi": "https://doi.org/10.18653/v1/d15-1073",
                "title": "Neural Networks for Open Domain Targeted Sentiment",
                "publication_year": 2015
            },
            "@cite_10": {
                "mid": "2144012961",
                "abstract": "Recurrent neural networks (RNNs) are connectionist models of sequential data that are naturally applicable to the analysis of natural language. Recently, \u201cdepth in space\u201d \u2014 as an orthogonal notion to \u201cdepth in time\u201d \u2014 in RNNs has been investigated by stacking multiple layers of RNNs and shown empirically to bring a temporal hierarchy to the architecture. In this work we apply these deep RNNs to the task of opinion expression extraction formulated as a token-level sequence-labeling task. Experimental results show that deep, narrow RNNs outperform traditional shallow, wide RNNs with the same number of parameters. Furthermore, our approach outperforms previous CRF-based baselines, including the state-of-the-art semi-Markov CRF model, and does so without access to the powerful opinion lexicons and syntactic features relied upon by the semi-CRF, as well as without the standard layer-by-layer pre-training typically required of RNN architectures.",
                "doi": "https://doi.org/10.3115/v1/d14-1080",
                "title": "Opinion Mining with Deep Recurrent Neural Networks",
                "publication_year": 2014
            },
            "@cite_3": {
                "mid": "2252024663",
                "abstract": "The tasks in fine-grained opinion mining can be regarded as either a token-level sequence labeling problem or as a semantic compositional task. We propose a general class of discriminative models based on recurrent neural networks (RNNs) and word embeddings that can be successfully applied to such tasks without any taskspecific feature engineering effort. Our experimental results on the task of opinion target identification show that RNNs, without using any hand-crafted features, outperform feature-rich CRF-based models. Our framework is flexible, allows us to incorporate other linguistic features, and achieves results that rival the top performing systems in SemEval-2014.",
                "doi": "https://doi.org/10.18653/v1/d15-1168",
                "title": "Fine-grained Opinion Mining with Recurrent Neural Networks and Word Embeddings",
                "publication_year": 2015
            },
            "@cite_2": {
                "mid": "2251648804",
                "abstract": "Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint. The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen). SemEval2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect. The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure. It attracted 163 submissions from 32 teams.",
                "doi": "https://doi.org/10.3115/v1/s14-2004",
                "title": "SemEval-2014 Task 4: Aspect Based Sentiment Analysis",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1612.06115",
        "mid": "2585314684",
        "abstract": "Complex networks are nowadays employed in several applications. Modeling urban street networks is one of them, and in particular to analyze criminal aspects of a city. Several research groups have focused on such application, but until now, there is a lack of a well-defined methodology for employing complex networks in a whole crime analysis process, i.e. from data preparation to a deep analysis of criminal communities. Furthermore, the \u201ctoolset\u201d available for those works is not complete enough, also lacking techniques to maintain up-to-date, complete crime datasets and proper assessment measures. In this sense, we propose a threefold methodology for employing complex networks in the detection of highly criminal areas within a city. Our methodology comprises three tasks: (i) Mapping of Urban Crimes; (ii) Criminal Community Identification; and (iii) Crime Analysis. Moreover, it provides a proper set of assessment measures for analyzing intrinsic criminality of communities, especially when considering different crime types. We show our methodology by applying it to a real crime dataset from the city of San Francisco\u2014CA, USA. The results confirm its effectiveness to identify and analyze high criminality areas within a city. Hence, our contributions provide a basis for further developments on complex networks applied to crime analysis.",
        "related_work": "Mapping of Urban Crimes. Spicer @cite_0 describe a theoretical framework capable of mapping urban crimes into related locations in a city. Their work discusses the pros and cons of mapping methods. To represent a city, they use street networks and GIS software, describing methods for mapping georeferenced elements into a complex network. However, their methods associate a crime with the nearest node or edge based on address geocoding, not having any relation with graph measures. Moreover, all methods are superficially introduced, not presenting any formal fundaments.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2301635754",
                "abstract": "Abstract Street profile analysis is a new method for analyzing temporal and spatial crime patterns along major roadways in metropolitan areas. This crime mapping technique allows for the identification of crime patterns along these street segments. These are linear spaces where aggregate crime patterns merge with crime attractors generators and human movement to demonstrate how directionality is embedded in city infrastructures. Visually presenting the interplay between these criminological concepts and land use can improve police crime management strategies. This research presents how this crime mapping technique can be applied to a major roadway in Burnaby, Canada. This technique is contrasted with other crime mapping methods to demonstrate the utility of this approach when analyzing the rate and velocity of crime patterns overtime and in space.",
                "doi": "https://doi.org/10.1016/j.apgeog.2016.02.008",
                "title": "Street profile analysis: A new method for mapping crime on major roadways",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1612.04904",
        "mid": "2952136044",
        "abstract": "The 3D shapes of faces are well known to be discriminative. Yet despite this, they are rarely used for face recognition and always under controlled viewing conditions. We claim that this is a symptom of a serious but often overlooked problem with existing methods for single view 3D face reconstruction: when applied \"in the wild\", their 3D estimates are either unstable and change for different photos of the same subject or they are over-regularized and generic. In response, we describe a robust method for regressing discriminative 3D morphable face models (3DMM). We use a convolutional neural network (CNN) to regress 3DMM shape and texture parameters directly from an input photo. We overcome the shortage of training data required for this purpose by offering a method for generating huge numbers of labeled examples. The 3D estimates produced by our CNN surpass state of the art accuracy on the MICC data set. Coupled with a 3D-3D face matching pipeline, we show the first competitive face recognition results on the LFW, YTF and IJB-A benchmarks using 3D face shapes as representations, rather than the opaque deep feature vectors used by other modern systems.",
        "related_work": "Over the years, many attempts were made to estimate the 3D surface of a face appearing in a single view. Before listing them, it is important to mention recent multi image methods which use image sets for reconstruction (e.g., @cite_17 @cite_34 @cite_24 @cite_6 @cite_33 ). Although these methods produce accurate 3D reconstructions, they require many images from multiple sources to produce a single 3D face shape whereas we reconstruct faces from single images.",
        "ref_abstract": {
            "@cite_33": {
                "mid": "125358319",
                "abstract": "We present an approach that takes a single video of a person\u2019s face and reconstructs a high detail 3D shape for each video frame. We target videos taken under uncontrolled and uncalibrated imaging conditions, such as youtube videos of celebrities. In the heart of this work is a new dense 3D flow estimation method coupled with shape from shading. Unlike related works we do not assume availability of a blend shape model, nor require the person to participate in a training capturing process. Instead we leverage the large amounts of photos that are available per individual in personal or internet photo collections. We show results for a variety of video sequences that include various lighting conditions, head poses, and facial expressions.",
                "doi": "https://doi.org/10.1007/978-3-319-10593-2_52",
                "title": "Total Moving Face Reconstruction",
                "publication_year": 2014
            },
            "@cite_6": {
                "mid": "2559821077",
                "abstract": "Given a photo collection of \u201cunconstrained\u201d face images of one individual captured under a variety of unknown pose, expression, and illumination conditions, this paper presents a method for reconstructing a 3D face surface model of the individual along with albedo information. Unlike prior work on face reconstruction that requires large photo collections, we formulate an approach to adapt to photo collections with a high diversity in both the number of images and the image quality. To achieve this, we incorporate prior knowledge about face shape by fitting a 3D morphable model to form a personalized template, following by using a novel photometric stereo formulation to complete the fine details, under a coarse-to-fine scheme. Our scheme incorporates a structural similarity-based local selection step to help identify a common expression for reconstruction while discarding occluded portions of faces. The evaluation of reconstruction performance is through a novel quality measure, in the absence of ground truth 3D scans. Superior large-scale experimental results are reported on synthetic, Internet, and personal photo collections.",
                "doi": "https://doi.org/10.1109/tpami.2016.2636829",
                "title": "Adaptive 3D Face Reconstruction from Unconstrained Photo Collections",
                "publication_year": 2017
            },
            "@cite_24": {
                "mid": "1940113235",
                "abstract": "This paper presents an algorithm for unconstrained 3D face reconstruction. The input to our algorithm is an \u201cunconstrained\u201d collection of face images captured under a diverse variation of poses, expressions, and illuminations, without meta data about cameras or timing. The output of our algorithm is a true 3D face surface model represented as a watertight triangulated surface with albedo data or texture information. 3D face reconstruction from a collection of unconstrained 2D images is a long-standing computer vision problem. Motivated by the success of the state-of-the-art method, we developed a novel photometric stereo-based method with two distinct novelties. First, working with a true 3D model allows us to enjoy the benefits of using images from all possible poses, including profiles. Second, by leveraging emerging face alignment techniques and our novel normal field-based Laplace editing, a combination of landmark constraints and photometric stereo-based normals drives our surface reconstruction. Given large photo collections and a ground truth 3D surface, we demonstrate the effectiveness and strength of our algorithm both qualitatively and quantitatively.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298876",
                "title": "Unconstrained 3D face reconstruction",
                "publication_year": 2015
            },
            "@cite_34": {
                "mid": "2464650832",
                "abstract": "Automated 3D reconstruction of faces from images is challenging if the image material is difficult in terms of pose, lighting, occlusions and facial expressions, and if the initial 2D feature positions are inaccurate or unreliable. We propose a method that reconstructs individual 3D shapes from multiple single images of one person, judges their quality and then combines the best of all results. This is done separately for different regions of the face. The core element of this algorithm and the focus of our paper is a quality measure that judges a reconstruction without information about the true shape. We evaluate different quality measures, develop a method for combining results, and present a complete processing pipeline for automated reconstruction.",
                "doi": "https://doi.org/10.1109/cvpr.2016.372",
                "title": "Automated 3D Face Reconstruction from Multiple Images Using Quality Measures",
                "publication_year": 2016
            },
            "@cite_17": {
                "mid": "2518670786",
                "abstract": "3D face reconstruction from Internet photos has recently produced exciting results. A person\u2019s face, e.g., Tom Hanks, can be modeled and animated in 3D from a completely uncalibrated photo collection. Most methods, however, focus solely on face area and mask out the rest of the head. This paper proposes that head modeling from the Internet is a problem we can solve. We target reconstruction of the rough shape of the head. Our method is to gradually \u201cgrow\u201d the head mesh starting from the frontal face and extending to the rest of views using photometric stereo constraints. We call our method boundary-value growing algorithm. Results on photos of celebrities downloaded from the Internet are presented.",
                "doi": "https://doi.org/10.1007/978-3-319-46475-6_23",
                "title": "Head Reconstruction from Internet Photos",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1611.08868",
        "mid": "2393484683",
        "abstract": "Non-functional requirements (NFRs) are commonly distinguished from functional requirements by differentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also influences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions. As a result, they remain difficult to analyze and test. Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classification of 530 NFRs extracted from 11 industrial requirements specifications and analyze to which extent these NFRs describe system behavior. Our results suggest that most \"non-functional\" requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.",
        "related_work": "Previously published material. In our previously published paper @cite_4 , we presented a research proposal with the goal of analyzing natural language NFRs taken from industrial requirements specifications to better understand their nature. Our study reported here, relies on and extends our previous study design. We present the results in full detail, and provide a comprehensive discussion on the implications on software engineering disciplines.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "1898610933",
                "abstract": "Context: Seamless model-based development provides integrated chains of models, covering all software engineering phases. Non-functional requirements (NFRs), like reusability, further play a vital role in software and systems engineering, but are often neglected in research and practice. It is still unclear how to integrate NFRs in a seamless model-based development. Goal: Our long-term goal is to develop a theory on the specification of NFRs such that they can be integrated in seamless model-based development. Method: Our overall study design includes a multi-staged procedure to infer an empirically founded theory on specifying NFRs to support seamless modeling. In this short paper, we present the study design and provide a discussion of (i) preliminary results obtained from a sample, and (ii) current issues related to the design. Results: Our study already shows significant fields of improvement, e.g., the low agreement during the classification. However, the results indicate to interesting points; for example, many of commonly used NFR classes concern system modeling concepts in a way that shows how blurry the borders between functional and NFRs are. Conclusions: We conclude so far that our overall study design seems suitable to obtain the envisioned theory in the long run, but we could also show current issues that are worth discussing within the empirical software engineering community. The main goal of this contribution is not to present and discuss current results only, but to foster discussions on the issues related to the integration of NFRs in seamless modeling in general and, in particular, discussions on open methodological issues.",
                "doi": "https://doi.org/10.1109/esem.2015.7321200",
                "title": "How to Specify Non-Functional Requirements to Support Seamless Modeling? A Study Design and Preliminary Results",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1611.06362",
        "mid": "2552554784",
        "abstract": "Recent years have witnessed extensive attention in binary code learning, a.k.a. hashing, for nearest neighbor search problems. It has been seen that high-dimensional data points can be quantized into binary codes to give an efficient similarity approximation via Hamming distance. Among existing schemes, ranking-based hashing is recent promising that targets at preserving ordinal relations of ranking in the Hamming space to minimize retrieval loss. However, the size of the ranking tuples, which shows the ordinal relations, is quadratic or cubic to the size of training samples. By given a large-scale training data set, it is very expensive to embed such ranking tuples in binary code learning. Besides, it remains a dificulty to build ranking tuples efficiently for most ranking-preserving hashing, which are deployed over an ordinal graph-based setting. To handle these problems, we propose a novel ranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH), which efficiently learns the optimal hashing functions with a graph-based approximation to embed the ordinal relations. The core idea is to reduce the size of ordinal graph with ordinal constraint projection, which preserves the ordinal relations through a small data set (such as clusters or random samples). In particular, to learn such hash functions effectively, we further relax the discrete constraints and design a specific stochastic gradient decent algorithm for optimization. Experimental results on three large-scale visual search benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the proposed OCH method can achieve superior performance over the state-of-the-arts approaches.",
        "related_work": "@cite_1 aims to learn a set of hash functions as Eq. . Such functions can preserve the ordinal relations between @math and @math , where @math is the dissimilarity between the @math -th and the @math -th item. Its goal is to make sure the ordinal relation can be preserved in the produced Hamming space: @math To embed such ordinal relations, OEH first constructs a directed unweighted ordinal graph @math , where each node @math is the dissimilar degree @math , and each directed edge is defined via @math . Then the objective function is to minimize the inconsistency between the given ordinal relation graph and the ones generated from the corresponding hash codes. At last, by using the landmark-based ordinal graph, the quartic ordinal relation is transformed to the triplet ordinal relation, which transforms the target of OEH to @math where @math and @math are the landmark points.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2569015882",
                "abstract": "Binary code learning, a.k.a., hashing, has been recently popular due to its high efficiency in large-scale similarity search and recognition. It typically maps high-dimensional data points to binary codes, where data similarity can be efficiently computed via rapid Hamming distance. Most existing unsupervised hashing schemes pursue binary codes by reducing the quantization error from an original real-valued data space to a resulting Hamming space. On the other hand, most existing supervised hashing schemes constrain binary code learning to correlate with pairwise similarity labels. However, few methods consider ordinal relations in the binary code learning process, which serve as a very significant cue to learn the optimal binary codes for similarity search. In this paper, we propose a novel hashing scheme, dubbed Ordinal Embedding Hashing (OEH), which embeds given ordinal relations among data points to learn the ranking-preserving binary codes. The core idea is to construct a directed unweighted graph to capture the ordinal relations, and then train the hash functions using this ordinal graph to preserve the permutation relations in the Hamming space. To learn such hash functions effectively, we further relax the discrete constraints and design a stochastic gradient decent algorithm to obtain the optimal solution. Experimental results on two large-scale benchmark datasets demonstrate that the proposed OEH method can achieve superior performance over the state-of-the-arts approaches. At last, the evaluation on query by humming dataset demonstrates the OEH also has good performance for music retrieval by using user's humming or singing.",
                "doi": "https://doi.org/10.1609/aaai.v30i1.10167",
                "title": "Towards Optimal Binary Code Learning via Ordinal Embedding",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1611.04246",
        "mid": "2954346764",
        "abstract": "This paper proposes a learning strategy that extracts object-part concepts from a pre-trained convolutional neural network (CNN), in an attempt to 1) explore explicit semantics hidden in CNN units and 2) gradually grow a semantically interpretable graphical model on the pre-trained CNN for hierarchical object understanding. Given part annotations on very few (e.g., 3-12) objects, our method mines certain latent patterns from the pre-trained CNN and associates them with different semantic parts. We use a four-layer And-Or graph to organize the mined latent patterns, so as to clarify their internal semantic hierarchy. Our method is guided by a small number of part annotations, and it achieves superior performance (about 13 -107 improvement) in part center prediction on the PASCAL VOC and ImageNet datasets.",
        "related_work": "As reported in @cite_32 , there are two learning systems instantiated in mammalians:'' 1) the neocortex gradually acquires sophisticated knowledge representation, and 2) the hippocampus quickly learns specifics of individual experiences. CNNs are typically trained using big data, and contain rich appearance patterns of objects. If one compares CNNs to the neocortex, then the fast retrieval of latent patterns related to a semantic part can be compared to the short-term learning in hippocampus.",
        "ref_abstract": {
            "@cite_32": {
                "mid": "2424347275",
                "abstract": "We update complementary learning systems (CLS) theory, which holds that intelligent agents must possess two learning systems, instantiated in mammalians in neocortex and hippocampus. The first gradually acquires structured knowledge representations while the second quickly learns the specifics of individual experiences. We broaden the role of replay of hippocampal memories in the theory, noting that replay allows goal-dependent weighting of experience statistics. We also address recent challenges to the theory and extend it by showing that recurrent activation of hippocampal traces can support some forms of generalization and that neocortical learning can be rapid for information that is consistent with known structure. Finally, we note the relevance of the theory to the design of artificial intelligent agents, highlighting connections between neuroscience and machine learning.",
                "doi": "https://doi.org/10.1016/j.tics.2016.05.004",
                "title": "What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1611.03652",
        "mid": "2950063885",
        "abstract": "Subjective questions such as does neymar dive', or is clinton lying', or is trump a fascist', are popular queries to web search engines, as can be seen by autocompletion suggestions on Google, Yahoo and Bing. In the era of cognitive computing, beyond search, they could be handled as hypotheses issued for evaluation. Our vision is to leverage on unstructured data and metadata of the rich user-generated multimedia that is often shared as material evidence in favor or against hypotheses in social media platforms. In this paper we present two preliminary experiments along those lines and discuss challenges for a cognitive computing system that collects material evidence from user-generated multimedia towards aggregating it into some form of collective decision on the hypothesis.",
        "related_work": ". Our work is related to the so-called debating' technologies at IBM @cite_2 . Their work is strongly based on machine learning methods to track correlated text passages that may be spread over a corpus. Going beyond the observational hypotheses that are more on our sight and are typical of social media, the goal is to enable eliciting all kind of textual information that may be relevant as evidence to decide on a claim. An example is: (topic) ''; (claim) ''; (evidence) .'' To date, as far as we know this is yet a fully NLP project, which does not consider multimedia data for grounding on material evidence.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "2250762536",
                "abstract": "Engaging in a debate with oneself or others to take decisions is an integral part of our day-today life. A debate on a topic (say, use of performance enhancing drugs) typically proceeds by one party making an assertion claim (say, PEDs are bad for health) and then providing an evidence to support the claim (say, a 2006 study shows that PEDs have psychiatric side effects). In this work, we propose the task of automatically detecting such evidences from unstructured text that support a given claim. This task has many practical applications in decision support and persuasion enhancement in a wide range of domains. We first introduce an extensive benchmark data set tailored for this task, which allows training statistical models and assessing their performance. Then, we suggest a system architecture based on supervised learning to address the evidence detection task. Finally, promising experimental results are reported.",
                "doi": "https://doi.org/10.18653/v1/d15-1050",
                "title": "Show Me Your Evidence - an Automatic Method for Context Dependent Evidence Detection",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1610.09635",
        "mid": "2541164822",
        "abstract": "The Asia-Pacific Software Engineering Conference (APSEC) is a reputed and a long-running conference which has successfully completed more than two decades as of year 2015. We conduct a bibliometric and scientific publication mining based study to how the conference has evolved over the recent past six years (year 2010 to 2015). Our objective is to perform in-depth examination of the state of APSEC so that the APSEC community can identify strengths, areas of improvements and future directions for the conference. Our empirical analysis is based on various perspectives such as: paper submission acceptance rate trends, conference location, scholarly productivity and contributions from various countries, analysis of keynotes, workshops, conference organizers and sponsors, tutorials, identification of prolific authors, computation of citation impact of papers and contributing authors, internal and external collaboration, university and industry participation and collaboration, measurement of gender imbalance, topical analysis, yearly author churn and program committee characteristics.",
        "related_work": "In this Section, we present related work and the novel contributions of the study presented in this paper in context to existing work. analyze research papers published in MSR (Mining Software Repositories) series of conferences from @math to @math (a period of @math years) @cite_0 . conduct a bibliometric study consisting of mining @math papers in Requirements Engineering (RE) series of conference of @cite_6 . analyze @math years of RE papers published from the year @math to @math . They study several aspects such as: authorship numbers and scholarly productivity of various countries or regions, interdisciplinarity, topic modeling and categorization, collaboration (university and industry, internal and external) and public and proprietary dataset @cite_6 . conduct a research study on gender imbalance and low participation of women in Computer Science Research (CSR) @cite_11 . They conduct several empirical and statistical analysis consisting of mining thousands of bibliometric entries in DBLP http: dblp.uni-trier.de bibliography data @cite_11 . Their findings reveal that in the broad field of Computer Science, there is a gender balance wherein only @math 740 @math 2001 @math 2010 @math 2010 @math 2015$) through arXiv open access https: arxiv.org .",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2055253889",
                "abstract": "Mining Software Repositories (MSR) is an applied and practise-oriented field aimed at solving real problems encountered by practitioners and bringing value to Industry. We believe that empirical studies on both Open Source Software (OSS) and Closed or Proprietary Source (CSS PSS) is required in MSR research to increase generalizability or transferability of findings and reduce external (or threats) validity concerns. Furthermore, we believe that a collaboration between University and Industry is must or important in achieving the stated goals and agenda of MSR research (such as deployment and technology transfer). We analyse past five years of research papers published in MSR series of conferences (2010-2014) and count the number of studies using solely OSS data or solely CSS data or both OSS and CSS data. We also count the number of papers published by authors solely from Universities, solely from Industry and from both University and Industry. We present our findings which indicate lack of University-Industry collaboration (measured using co-authorship in scientific publications) and paucity of empirical studies on CSS PSS data. Our analysis reveals that out of 187 studies over a period of 5 years, 90:9 studies are conducted solely on OSS dataset. We present our findings which indicate that only 14:43 of the studies involve a University-Industry collaboration.",
                "doi": "https://doi.org/10.1109/swan.2015.7070489",
                "title": "University-industry collaboration and open source software (OSS) dataset in mining software repositories (MSR) research",
                "publication_year": 2015
            },
            "@cite_6": {
                "mid": "2382843593",
                "abstract": "We present insights from a bibliometric analysis and scientific paper publication mining of 551 papers in Requirements Engineering (RE) series of conference (11 years from 2005 to 2015). We study cross-disciplinary and interdisciplinary nature of RE re- search by analyzing the cited disciplines in the reference section of each paper. We apply topic modeling on a corpus consisting of 551 abstracts and extract topics as frequently co-occurring and connected terms. We use topic modeling to study the structure and composition of RE research and analyze popular topics in industry as well as research track. Co-authorship in papers is an indicator of collaboration and interaction between scientists as well as institutions and we analyze co-authorship data to in- vestigate university-industry collaboration, internal and external collaborations. We present results on the distribution of the num- ber of co-authors in each paper as well as distribution of authors across world regions. We present our analysis on the public or proprietary dataset as well as the domain of the dataset used in studies published in Requirements Engineering (RE) series of conferences.",
                "doi": "https://doi.org/10.1145/2894784.2894794",
                "title": "Insights from Mining Eleven Years of Scholarly Paper Publications in Requirements Engineering (RE) Series of Conferences",
                "publication_year": 2016
            },
            "@cite_11": {
                "mid": "2317735812",
                "abstract": "The low participation by women authors in research is an important equity issue in Computer Science Research (CSR). There are various parameters and methodologies that can be used to measure the gender imbalance. In this paper, we present a study on gender gap, imbalance and women participation in CSR. We conduct our experiments on DBLP bibliographical database and analyze several years of publication dataset across various domains of CSR. We perform Exploratory Data Analysis on the bibliographical dataset and study the trend of gender imbalance over several years. We propose eight research questions across various facets and our results shows a significant gender imbalance in different sub-fields within CSR and low rate of women participation across various regions of world.",
                "doi": "https://doi.org/10.1145/2908216.2908218",
                "title": "Women in computer science research",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1610.07882",
        "mid": "2545932337",
        "abstract": "Convolutional neural networks (CNN) are widely used in computer vision, especially in image classification. However, the way in which information and invariance properties are encoded through in deep CNN architectures is still an open question. In this paper, we propose to modify the standard convo- lutional block of CNN in order to transfer more information layer after layer while keeping some invariance within the net- work. Our main idea is to exploit both positive and negative high scores obtained in the convolution maps. This behav- ior is obtained by modifying the traditional activation func- tion step before pooling. We are doubling the maps with spe- cific activations functions, called MaxMin strategy, in order to achieve our pipeline. Extensive experiments on two classical datasets, MNIST and CIFAR-10, show that our deep MaxMin convolutional net outperforms standard CNN.",
        "related_work": "Regarding @math , it is a non-linear function that sets to zero all negative values after the convolutional layer. This filtering is supposed to facilitate the exploitation of discriminative information by de-noising filter detections. @cite_20 introduced the @math as an extension of @math . It multiplies negative values with a learnable coefficient instead of setting them to zero. This method enables to filter less information at activation layer while keeping the non linear property of @math although at the cost of additional learnable parameters.",
        "ref_abstract": {
            "@cite_20": {
                "mid": "1677182931",
                "abstract": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94 top-5 test error on the ImageNet 2012 classification dataset. This is a 26 relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66 [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1 , [26]) on this dataset.",
                "doi": "https://doi.org/10.1109/iccv.2015.123",
                "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1610.02431",
        "mid": "2530300198",
        "abstract": "In this short note we introduce ResearchDoom, an implementation of the Doom first-person shooter that can extract detailed metadata from the game. We also introduce the CocoDoom dataset, a collection of pre-recorded data extracted from Doom gaming sessions along with annotations in the MS Coco format. ResearchDoom and CocoDoom can be used to train and evaluate a variety of computer vision methods such as object recognition, detection and segmentation at the level of instances and categories, tracking, ego-motion estimation, monocular depth estimation and scene segmentation. The code and data are available at this http URL",
        "related_work": "Another recent effort similar to ResearchDoom is Unreal CV @cite_5 . This engine can also extract metadata similar to ResearchDoom and, by building on the Unreal engine, can potentially be applicable to a huge variety of modern video games. ResearchDoom is much simpler than Unreal CV as it applies to a single and relatively old game. While Doom incorporates several limitations, such as the fact that the camera can only rotate around the vertical axis, the data is nevertheless fairly complex. Furthermore, Doom provides a relatively restricted and consistent world form which data can be extracted for experiments. We leverage the latter fact to define and provide abundant pre-computed data with detailed metadata as well as with annotations using the Microsoft Coco format.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2517064749",
                "abstract": "Computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which: (i) an agent can perceive, navigate, and take actions guided by AI algorithms, (ii) properties of the worlds can be modified (e.g., material and reflectance), (iii) physical simulations can be performed, and (iv) algorithms can be learnt and evaluated. But creating realistic virtual worlds is not easy. The game industry, however, has spent a lot of effort creating 3D worlds, which a player can interact with. So researchers can build on these resources to create virtual worlds, provided we can access and modify the internal data structures of the games. To enable this we created an open-source plugin UnrealCV (this http URL) for a popular game engine Unreal Engine 4 (UE4). We show two applications: (i) a proof of concept image dataset, and (ii) linking Caffe with the virtual world to test deep network algorithms.",
                "doi": "https://doi.org/10.48550/arxiv.1609.01326",
                "title": "UnrealCV: Connecting Computer Vision to Unreal Engine",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1610.02263",
        "mid": "2530370111",
        "abstract": "The main task of HTTP Adaptive Streaming is to adapt video quality dynamically under variable network conditions. This is a key feature for multimedia delivery especially when quality of service cannot be granted network-wide and, e.g., throughput may suffer short term fluctuations. Hence, robust bitrate adaptation schemes become crucial in order to improve video quality. The objective, in this context, is to control the filling level of the playback buffer and maximize the quality of the video, while avoiding unnecessary video quality variations. In this paper we study bitrate adaptation algorithms based on Backward-Shifted Coding (BSC), a scalable video coding scheme able to greatly improve video quality. We design bitrate adaptation algorithms that balance video rate smoothness and high network capacity utilization, leveraging both on throughput-based and buffer-based adaptation mechanisms. Extensive simulations using synthetic and real-world video traffic traces show that the proposed scheme performs remarkably well even under challenging network conditions.",
        "related_work": "Backward-Shifted Coding (BSC) @cite_12 and @cite_20 are two schemes which leverage on time redundancy mechanisms for scalable video coding. In both schemes, base layer or enhancement layers of any number of future segments of a video can be delivered in advance.",
        "ref_abstract": {
            "@cite_20": {
                "mid": "2009673131",
                "abstract": "Video streaming applications are a major contributor to the recent dramatic rise of data traffic in cellular networks. Mobile users in a cellular network suffer fluctuating data rates, which almost directly reflects on the quality of video they view in a streaming service. Although replacing such video streaming services with video downloading renting services could potentially allow such mobile users to enjoy consistently higher quality videos, traditionally such services cost a lot more than video streaming services because of legal copyright pricing and management issues. We propose a novel scalable video delivery service called streamloading that can potentially allow mobile users to enjoy download quality videos, while still being legally classified as a streaming service. We describe the implementation of the service and perform extensive simulations to evaluate streamloading, in comparison to traditional streaming services.",
                "doi": "https://doi.org/10.1145/2457413.2457415",
                "title": "Streamloading",
                "publication_year": 2013
            },
            "@cite_12": {
                "mid": "2357930737",
                "abstract": "Although HTTP-based video streaming can easily penetrate firewalls and profit from Web caches, the underlying TCP may introduce large delays in case of a sudden capacity loss. To avoid an interruption of the video stream in such cases we propose the Backward-Shifted Coding (BSC). Based on Scalable Video Coding (SVC), BSC adds a time-shifted layer of redundancy to the video stream such that future frames are downloaded at any instant. This pre-fetched content maintains a fluent video stream even under highly variant network conditions and leads to high Quality of Experience (QoE). We characterize this QoE gain by analyzing initial buffering time, re-buffering time and content resolution using the Ballot theorem. The probability generating functions of the playback interruption and of the initial buffering latency are provided in closed form. We further compute the quasi-stationary distribution of the video quality, in order to compute the average quality, as well as temporal variability in video quality. Employing these analytic results to optimize QoE shows interesting trade-offs and video streaming at outstanding fluency.",
                "doi": "https://doi.org/10.48550/arxiv.1605.03815",
                "title": "Backward-Shifted Strategies Based on SVC for HTTP Adaptive Video\n  Streaming",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1609.05174",
        "mid": "2522175778",
        "abstract": "Let @math be a symmetric probability measure of finite entropy on a group @math . We show that if @math , then the pair @math has the Liouville property (all bounded @math -harmonic functions on @math are constant). Furthermore, if @math where @math , then the entropy of the @math -fold convolution power @math satisfies @math . This improves earlier results of Gournay and of Saloff-Coste and the second author. We extend the bounds to transitive graphs and illustrate their sharpness on a family of groups.",
        "related_work": "In the work of the second author with Saloff-Coste @cite_12 , an upper bound similar to Theorem was proved. More precisely, suppose @math is a symmetric probability measure of finite second moment on @math such that @math for some @math , then by [Theorem 1.7] Saloff-Coste2014 [ H_ (n) (n ^ 1+ n )^ 1- for any >0. ] In particular, this bound implies the Liouville property of @math for @math . Our results in this paper imply a sharper upper bound @math under the same assumption, and are applicable to a larger class of decay lower bounds.",
        "ref_abstract": {
            "@cite_12": {
                "mid": "2301298389",
                "abstract": "Let @math be a finitely generated group equipped with a finite symmetric generating set and the associated word length function @math . We study the behavior of the probability of return for random walks driven by symmetric measures @math that are such that @math . For this purpose we develop new relations between the isoperimetric profiles associated with different symmetric probability measures. These techniques allow us to obtain a sharp @math -version of Erschler's inequality concerning the F lner functions of wreath products. Examples and assorted applications are included.",
                "doi": "https://doi.org/10.48550/arxiv.1501.05929",
                "title": "Random walks and isoperimetric profiles under moment conditions",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1609.03438",
        "mid": "2535877628",
        "abstract": "Abstract Managed multi-context systems (mMCSs) allow for the integration of heterogeneous knowledge sources in a modular and very general way. They were, however, mainly designed for static scenarios and are therefore not well-suited for dynamic environments in which continuous reasoning over such heterogeneous knowledge with constantly arriving streams of data is necessary. In this paper, we introduce reactive multi-context systems (rMCSs), a framework for reactive reasoning in the presence of heterogeneous knowledge sources and data streams. We show that rMCSs are indeed well-suited for this purpose by illustrating how several typical problems arising in the context of stream reasoning can be handled using them, by showing how inconsistencies possibly occurring in the integration of multiple knowledge sources can be handled, and by arguing that the potential non-determinism of rMCSs can be avoided if needed using an alternative, more skeptical well-founded semantics instead with beneficial computational properties. We also investigate the computational complexity of various reasoning problems related to rMCSs. Finally, we discuss related work, and show that rMCSs do not only generalize mMCSs to dynamic settings, but also capture extend relevant approaches w.r.t. dynamics in knowledge representation and stream reasoning.",
        "related_work": "Several systems and frameworks for modeling the dynamics of knowledge and the flow of information have been developed. These systems are obviously related to reactive multi-context systems. In this section we provide a comparison with those approaches we consider most relevant, stressing differences as well as commonalities with . Note that we focus entirely on dynamic approaches and do not include systems which handle heterogeneous information in a static setting. An interesting example of the latter type are Lierler and Truszczy 'nski's abstract modular systems @cite_12 . In a nutshell, modular systems realize the communication between different reasoning modules through joint vocabularies rather than bridge rules. These systems are best viewed as alternatives to classical MCSs. It is an open question how to adapt them to a dynamic setting.",
        "ref_abstract": {
            "@cite_12": {
                "mid": "2328438203",
                "abstract": "Integrating diverse formalisms into modular knowledge representation systems offers increased expressivity, modeling convenience, and computational benefits. We introduce the concepts of abstract inference modules and abstract modular inference systems to study general principles behind the design and analysis of model generating programs, or solvers, for integrated multi-logic systems. We show how modules and modular systems give rise to transition graphs, which are a natural and convenient representation of solvers, an idea pioneered by the SAT community. These graphs lend themselves well to extensions that capture such important solver design features as learning. In the paper, we consider two flavors of learning for modular formalisms, local and global. We illustrate our approach by showing how it applies to answer set programming, propositional logic, multi-logic systems based on these two formalisms and, more generally, to satisfiability modulo theories.",
                "doi": "https://doi.org/10.1016/j.artint.2016.03.004",
                "title": "On abstract modular inference systems and solvers",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1609.00427",
        "mid": "2512809262",
        "abstract": "Although influence maximization problem has been extensively studied over the past ten years, majority of existing work adopt one of the following models: or . In the zero-feedback model, we have to commit the seed users all at once in advance, this strategy is also known as non-adaptive policy. In the full-feedback model, we select one seed at a time and wait until the diffusion completes, before selecting the next seed. Full-feedback model has better performance but potentially huge delay, zero-feedback model has zero delay but poorer performance since it does not utilize the observation that may be made during the seeding process. To fill the gap between these two models, we propose , which allows us to select a seed at any intermediate stage. We develop a novel @math -greedy policy that, for the first time, achieves a bounded approximation ratio.",
        "related_work": "The first category is non-adaptive influence maximization: we must find a set of influential customers all at once in advance subject to a budget constraint. first formalized and studied this problem under two diffusion models, namely Independent Cascade model and Linear Threshold model. study influence maximization problem under various extended models. The second category is adaptive influence maximization, which is closely related to adaptive stochastic submodular maximization @cite_3 @cite_1 @cite_0 @cite_2 . Existing studies mainly adopt full-feedback model, assuming that we can observe the full status of the previous cascade before selecting the next seed. We relax this assumption by incorporating partial-feedback and develop a novel @math -greedy adaptive policy that achieves the first bounded approximation ratio under partial-feedback model.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2962773920",
                "abstract": "Influence maximization is the problem of selecting top @math seed nodes in a social network to maximize their influence coverage under certain influence diffusion models. In this paper, we propose a novel algorithm IRIE that integrates the advantages of influence ranking (IR) and influence estimation (IE) methods for influence maximization in both the independent cascade (IC) model and its extension IC-N that incorporates negative opinion propagations. Through extensive experiments, we demonstrate that IRIE matches the influence coverage of other algorithms while scales much better than all other algorithms. Moreover IRIE is much more robust and stable than other algorithms both in running time and memory usage for various density of networks and cascade size. It runs up to two orders of magnitude faster than other state-of-the-art algorithms such as PMIA for large networks with tens of millions of nodes and edges, while using only a fraction of memory.",
                "doi": "https://doi.org/10.1109/icdm.2012.79",
                "title": "IRIE: Scalable and Robust Influence Maximization in Social Networks",
                "publication_year": 2012
            },
            "@cite_1": {
                "mid": "1424203601",
                "abstract": "The Adaptive Seeding problem is an algorithmic challenge motivated by influence maximization in social networks: One seeks to select among certain accessible nodes in a network, and then select, adaptively, among neighbors of those nodes as they become accessible in order to maximize a global objective function. More generally, adaptive seeding is a stochastic optimization framework where the choices in the first stage affect the realizations in the second stage, over which we aim to optimize. Our main result is a (1 -- 1 e)2-approximation for the adaptive seeding problem for any monotone submodular function. While adaptive policies are often approximated via non-adaptive policies, our algorithm is based on a novel method we call locally-adaptive policies. These policies combine a non-adaptive global structure, with local adaptive optimizations. This method enables the (1 -- 1 e)2-approximation for general monotone submodular functions and circumvents some of the impossibilities associated with non-adaptive policies. We also introduce a fundamental problem in submodular optimization that may be of independent interest: given a ground set of elements where every element appears with some small probability, find a set of expected size at most k that has the highest expected value over the realization of the elements. We show a surprising result: there are classes of monotone submodular functions (including coverage) that can be approximated almost optimally as the probability vanishes. For general monotone submodular functions we show via a reduction from P lanted -C lique that approximations for this problem are not likely to be obtainable. This optimization problem is an important tool for adaptive seeding via non-adaptive policies, and its hardness motivates the introduction of locally-adaptive policies we use in the main result.",
                "doi": "https://doi.org/10.5555/2884435.2884466",
                "title": "Locally adaptive optimization: adaptive seeding for monotone submodular functions",
                "publication_year": 2016
            },
            "@cite_3": {
                "mid": "2962795549",
                "abstract": "Many problems in artificial intelligence require adaptively making a sequence of decisions with uncertain outcomes under partial observability. Solving such stochastic optimization problems is a fundamental but notoriously difficult challenge. In this paper, we introduce the concept of adaptive submodularity, generalizing submodular set functions to adaptive policies. We prove that if a problem satisfies this property, a simple adaptive greedy algorithm is guaranteed to be competitive with the optimal policy. In addition to providing performance guarantees for both stochastic maximization and coverage, adaptive submodularity can be exploited to drastically speed up the greedy algorithm by using lazy evaluations. We illustrate the usefulness of the concept by giving several examples of adaptive submodular objectives arising in diverse AI applications including management of sensing resources, viral marketing and active learning. Proving adaptive submodularity for these problems allows us to recover existing results in these applications as special cases, improve approximation guarantees and handle natural generalizations.",
                "doi": "https://doi.org/10.1613/jair.3278",
                "title": "Adaptive submodularity: theory and applications in active learning and stochastic optimization",
                "publication_year": 2011
            },
            "@cite_2": {
                "mid": "2735579742",
                "abstract": "It has been reported that 40 of consumers will share an email offer with their friend and 28 of consumers will share deals via social media platforms. This motivates us to study the influence maximization discount allocation problem: given a social network and a limited marketing budget, which set of initial users should be selected to receive the discount, and how much should the discounts be worth? Our goal is to maximize the number of customers who finally adopt the target product. We investigate this problem under both non-adaptive and adaptive settings. In the first setting, we have to commit the set of initial users and corresponding discounts all at once in advance. In the latter case, given that an user has been offered a discount, we are able to know immediately her decision on whether or not to accept that discount, therefore, the decision process is performed in a sequential manner based on the feedback from previously selected users. We propose a simple greedy policy with an approximation ratio of (1-1 e) in non-adaptive setting. For the significantly more complex adaptive setting, we propose a series of adaptive policies with bounded approximation ratio in terms of expected utility.",
                "doi": "https://doi.org/10.1145/3084041.3084043",
                "title": "Adaptive Discount Allocation in Social Networks",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1608.07886",
        "mid": "2514819328",
        "abstract": "We consider crowdsourcing problems where the users are asked to provide evaluations for items; the user evaluations are then used directly, or aggregated into a consensus value. Lacking an incentive scheme, users have no motive in making effort in completing the evaluations, providing inaccurate answers instead. We propose incentive schemes that are truthful and cheap: truthful as the optimal user behavior consists in providing accurate evaluations, and cheap because the truthfulness is achieved with little overhead cost. We consider both discrete evaluation tasks, where an evaluation can be done either correctly, or incorrectly, with no degrees of approximation in between, and quantitative evaluation tasks, where evaluations are real numbers, and the error is measured as distance from the correct value. For both types of tasks, we propose hierarchical incentive schemes that can be effected with a small amount of additional evaluations, and that scale to arbitrarily large crowd sizes: they have the property that the strength of the incentive does not weaken with increasing hierarchy depth. Interestingly, we show that for these schemes to work, the only requisite is that workers know their place in the hierarchy in advance.",
        "related_work": "Providing incentives to human agents to return truthful responses is one of the central challenges for crowdsourcing algorithms and applications @cite_16 .",
        "ref_abstract": {
            "@cite_16": {
                "mid": "750688931",
                "abstract": "The success of a human computation system depends critically on the humans in the system actually behaving, or acting, as necessary for the system to function effectively. Since users have their own costs and benefits from participation, they will undertake desirable actions only if properly incentivized to do so: Indeed, while there are a vast number of human computation systems on the Web, the extent of participation and quality of contribution varies widely across systems. How can a game-theoretic approach help understand why, and provide guidance on designing systems that incentivize high participation and effort from contributors?",
                "doi": "https://doi.org/10.1007/978-1-4614-8806-4_58",
                "title": "Game Theory and Incentives in Human Computation Systems",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1608.07951",
        "mid": "2509601486",
        "abstract": "Abstract Computational color constancy refers to the problem of computing the illuminant color so that the images of a scene under varying illumination can be normalized to an image under the canonical illumination. In this paper, we adopt a deep learning framework for the illumination estimation problem. The proposed method works under the assumption of uniform illumination over the scene and aims for the accurate illuminant color computation. Specifically, we trained the convolutional neural network to solve the problem by casting the color constancy problem as an illumination classification problem. We designed the deep learning architecture so that the output of the network can be directly used for computing the color of the illumination. Experimental results show that our deep network is able to extract useful features for the illumination estimation and our method outperforms all previous color constancy methods on multiple test datasets.",
        "related_work": "Recently, understanding the mechanism of the human visual system (HVS) has been found to be useful in building the statistical assumptions by mimicking the human built-in ability of color constancy @cite_19 @cite_1 . One limitation of these approaches is that we are still far from fully understanding the mechanism of the HVS. In this regard, we believe our deep learning approach which train HVS inspired complex model (CNN) with large data to simulate color information processing on the human brain could make a breakthrough for the problem of the computational color constancy because we do not need to know how it works specifically.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "2121077932",
                "abstract": "The aim of computational color constancy is to estimate the actual surface color in an acquired scene disregarding its illuminant. Many solutions try to first estimate the illuminant and then correct the image with the illuminant estimate. Based on the linear image formation model, we propose in this work a new strategy to estimate the illuminant. Inspired by the feedback modulation from horizontal cells to the cones in the retina, we first normalize each local patch with its local maximum to obtain the so-called locally normalized reflectance estimate (LNRE). Then, we experimentally found that the ratio of the global summation of true surface reflectance to the global summation of LNRE in a scene is approximately achromatic for both indoor and outdoor scenes. Based on this substantial observation, we estimate the illuminant by computing the ratio of the global summation of the intensities to the global summation of the locally normalized intensities of the color-biased image. The proposed model has only one free parameter and requires no explicit training with learning-based approach. Experimental results on four commonly used datasets show that our model can produce competitive or even better results compared to the state-of-the-art approaches with low computational cost.",
                "doi": "https://doi.org/10.1007/978-3-319-10605-2_11",
                "title": "Efficient Color Constancy with Local Surface Reflectance Statistics",
                "publication_year": 2014
            },
            "@cite_1": {
                "mid": "2012602905",
                "abstract": "The double-opponent (DO) color-sensitive cells in the primary visual cortex (V1) of the human visual system (HVS) have long been recognized as the physiological basis of color constancy. In this work we propose a new color constancy model by imitating the functional properties of the HVS from the single-opponent (SO) cells in the retina to the DO cells in V1 and the possible neurons in the higher visual cortexes. The idea behind the proposed double-opponency based color constancy (DOCC) model originates from the substantial observation that the color distribution of the responses of DO cells to the color-biased images coincides well with the vector denoting the light source color. Then the illuminant color is easily estimated by pooling the responses of DO cells in separate channels in LMS space with the pooling mechanism of @math or @math . Extensive evaluations on three commonly used datasets, including the test with the dataset dependent optimal parameters, as well as the intra- and inter-dataset cross validation, show that our physiologically inspired DOCC model can produce quite competitive results in comparison to the state-of-the-art approaches, but with a relative simple implementation and without requiring fine-tuning of the method for each different dataset.",
                "doi": "https://doi.org/10.1109/tpami.2015.2396053",
                "title": "Color Constancy Using Double-Opponency",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1608.06749",
        "mid": "2510106892",
        "abstract": "Recently, Cloud-based Radio Access Network (C-RAN) has been proposed as a potential solution to reduce energy cost in cellular networks. C-RAN centralizes the baseband processing capabilities of Base Stations (BSs) in a cloud computing platform in the form of BaseBand Unit (BBU) pool. In C-RAN, power consumed by the traditional BS system is distributed as wireless transmission power of the Remote Radio Heads (RRHs) and baseband processing power of the BBU pool. Different from previous work where wireless transmission power and baseband processing power are optimized individually and independently, this paper focuses on joint optimization of allocation for these two kinds of power and attempts to minimize the total power consumption subject to Quality of Service (QoS) requirements from users in terms of data rates. First, we exploit the load coupling model to express the coupling relations among power, load and user data rates. Based on the load coupling mode, we formulate the joint power optimization problem in C-RAN over both wireless transmission power and baseband processing power. Second, we prove that operating at full load may not be optimal in minimizing the total power consumption in C-RAN. Finally, we propose an efficient iterative algorithm to solve the target problem. Simulations have been performed to validate our theoretical and algorithmic work. The results show that the proposed algorithm outperforms existing schemes (without joint power optimization) in terms of power consumption.",
        "related_work": "There also exists some work on optimizing baseband processing power in C-RANs. By virtualization, the baseband processing resources in the BBU pool are dynamically shared among all RRHs. In @cite_8 , a dynamic programming scheme is proposed to minimize the baseband processing power consumption in the case of dynamic cell traffic load. However, the computational complexity of the proposed scheme is exponential. In order to reduce the computational complexity, the authors in @cite_19 propose a BBU virtualization scheme that minimizes the baseband processing power consumption with a linear computational complexity order.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "2003021396",
                "abstract": "In cloud radio access networks (C-RANs), the baseband processing units (BBUs) from traditional base stations (BSs) are virtualized and moved into a single centralized location. By virtualization, the computing resources of BBUs can be dynamically shared among all cells, enabling a significant improvement in computing resource utilization and power efficiency. In this letter, we propose a BBUs virtualization scheme that minimizes the power consumption with a linear computational complexity order. The scheme is based on a heuristic simulated annealing (HSA) algorithm, which combines a bin packing algorithm with SA. Simulation results show that the HSA effectively decreases system power consumption when compared to standard approaches.",
                "doi": "https://doi.org/10.1109/lwc.2015.2393355",
                "title": "Baseband Processing Units Virtualization for Cloud Radio Access Networks",
                "publication_year": 2015
            },
            "@cite_8": {
                "mid": "2054510130",
                "abstract": "In order to reduce high construction cost and energy consumption of distributed base stations in cellular networks, the centralized radio access Cellular network infrastructure with a Super BS(CSBS) architecture has been proposed in which the centralized processing resource can be flexibly organized. However, there is sill the problem of low processing resource efficiency in protocol processing. In this paper, based on a protocol processing resource management framework, distributed load diversity based processing resource allocation (D-LDA) as well as the improved hybrid load diversity based processing resource allocation(H-LDA) are proposed, which take load discrepancy into consideration in the situation that a large number of BSs are supported. Simulation results show that D-LDA and H-LDA algorithm significantly outperforms the conventional method respectively by more than 30 and 55 in the processing resource efficiency.",
                "doi": "https://doi.org/10.1109/icc.2014.6884133",
                "title": "Load diversity based processing resource allocation for super base stations in large-scale centralized radio access networks",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1608.03415",
        "mid": "2952392184",
        "abstract": "We describe a methodology to automatically turn arbitrary ARMv8 programs into alphanumeric executable polymorphic shellcodes. Shellcodes generated in this way can evade detection and bypass filters, broadening the attack surface of ARM-powered devices such as smartphones.",
        "related_work": "Basu @cite_21 developed an algorithm for automated shellcode generation targeting the x86 architecture. The Metasploit project provides the msfvenom utility, which can turn arbitrary x86 programs into alphanumeric x86 code. Both UPX See http: upx.sf.net . and msfvenom can generate self-decrypting ARM executables, yet neither provide alphanumeric encodings for this platform.",
        "ref_abstract": {
            "@cite_21": {
                "mid": "78908061",
                "abstract": "Shellcode can be viewed as machine language code that is injected in the form of string input to exploit buffer overflows. It usually contains non-ASCII values because not all machine instructions encode into ASCII values. Many applications allow arbitrary string input, even though only strings containing characters that are ASCII or a subset of ASCII are deemed valid. Thus a common defense against shellcode injection is to discard any string input containing non-ASCII characters. Alphanumeric shellcode helps attackers bypass such character restrictions. It is non-trivial to construct alphanumeric shellcodes by hand and so tools have been created to automate the process. The alphanumeric equivalent, generated by the existing tools, is much larger than the original shellcode. This paper presents two new encoding schemes to reduce the size of the alphanumeric equivalent. A smaller shellcode is better as it can fit into smaller buffers and is even more useful in case an application restricts the input size. Results show that the size reduction of the encoded shellcode is more than 20 for many shellcodes.",
                "doi": "https://doi.org/10.1007/978-3-319-13841-1_22",
                "title": "Automatic Generation of Compact Alphanumeric Shellcodes for x86",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1607.08394",
        "mid": "2491554254",
        "abstract": "In this paper, we study cooperative cognitive radio networks consisting of a primary user (PU) and multiple secondary users (SUs). SUs transmit only when PU is sensed as silent and may interfere with primary transmission due to imperfect sensing. When primary activity is sensed correctly, SUs cooperate with PU by assisting retransmission of failed packets of PU. We analyze packet throughput of PU and SU for three variations of the proposed cooperation method. A signal flow graph-based approach is employed to obtain the closed-form expressions of packet throughput. The analysis is done for two cases: individual sensing and cooperative sensing. Furthermore, we characterize the optimal transmission probability of SUs that maximizes individual secondary packet throughput keeping all queues in the system stable. Results present a comparison of throughput performance of the proposed cooperation methods under different scenarios and show their benefits for both PU and SU throughput.",
        "related_work": "@cite_0 studied throughput and average time for packet delivery in a non-cognitive network for two cooperation scenarios-- forced cooperation where best relay retransmits a failed packet and voluntary cooperation where a user may act as relay to get higher access probability in return. In our paper, due to presence of PU, relaying capability of SUs is affected by sensing errors and interference by other users. In @cite_0 , authors modelled packet transmission process of automatic repeat request (ARQ) mechanism as Markov chain assuming that at most one retransmission per packet is allowed. We model transmission process of proposed protocols in a similar way but the signal flow graph approach employed for throughput analysis puts no restriction on number of retransmissions.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "1977313426",
                "abstract": "This paper discusses a new perspective for the application of game theory to wireless relay networks, namely, how to employ it not only as an analytical evaluation instrument, but also in constructively deriving practical network management policies. We focus on the problem of medium sharing in wireless networks, which is often seen as a case where game theory just proves the inefficiency of distributed access, without proposing any remedy. Instead, we show how, by properly modeling the agents involved in such a scenario, and enabling simple but effective incentives towards cooperation for the users, we obtain a resource allocation scheme which is meaningful from both perspectives of game theory and network engineering. Such a result is achieved by introducing throughput redistribution as a way to transfer utilities, which enables cooperation among the users. Finally, a Stackelberg formulation is proposed, involving the network access point as a further player. Our approach is also able to take into account power consumption of the terminals, still without treating it as an insurmountable hurdle to cooperation, and at the same time to drive the network allocation towards an efficient cooperation level.",
                "doi": "https://doi.org/10.1109/tcomm.2012.120512.110654",
                "title": "Promoting Cooperation in Wireless Relay Networks Through Stackelberg Dynamic Scheduling",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1607.07525",
        "mid": "2952588030",
        "abstract": "We study the problem of Salient Object Subitizing, i.e. predicting the existence and the number of salient objects in an image using holistic cues. This task is inspired by the ability of people to quickly and accurately identify the number of items within the subitizing range (1-4). To this end, we present a salient object subitizing image dataset of about 14K everyday images which are annotated using an online crowdsourcing marketplace. We show that using an end-to-end trained Convolutional Neural Network (CNN) model, we achieve prediction accuracy comparable to human performance in identifying images with zero or one salient object. For images with multiple salient objects, our model also provides significantly better than chance performance without requiring any localization process. Moreover, we propose a method to improve the training of the CNN subitizing model by leveraging synthetic images. In experiments, we demonstrate the accuracy and generalizability of our CNN subitizing model and its applications in salient object detection and image retrieval.",
        "related_work": "Salient object detection. Salient object detection aims at detecting dominant objects in a scene. Given a test image, some methods generate a saliency map that highlights the overall region of salient objects; other methods produce bounding boxes for localization. Ideally, if a salient object detection method can well localize each salient object, then the number of objects can be simply inferred by counting the detection windows. However, many existing salient object detection methods assume the existence of salient objects, and they are mainly tested and optimized for images that contain a single dominant object . Therefore, salient object detection methods often generate undesirable results on background images, and are prone to fail on images with multiple objects and complex background. Recently, @cite_1 proposed a salient object detection method for unconstrained images. Although this method can handle complex images to some extent, we will show that the counting-by-detection approach is less effective than our subitizing method in predicting the number of salient objects.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2422471819",
                "abstract": "We aim at detecting salient objects in unconstrained images. In unconstrained images, the number of salient objects (if any) varies from image to image, and is not given. We present a salient object detection system that directly outputs a compact set of detection windows, if any, for an input image. Our system leverages a Convolutional-Neural-Network model to generate location proposals of salient objects. Location proposals tend to be highly overlapping and noisy. Based on the Maximum a Posteriori principle, we propose a novel subset optimization framework to generate a compact set of detection windows out of noisy proposals. In experiments, we show that our subset optimization formulation greatly enhances the performance of our system, and our system attains 16-34 relative improvement in Average Precision compared with the state-of-the-art on three challenging salient object datasets.",
                "doi": "https://doi.org/10.1109/cvpr.2016.618",
                "title": "Unconstrained Salient Object Detection via Proposal Subset Optimization",
                "publication_year": 2016
            }
        }
    },
    {
        "aid": "1606.07154",
        "mid": "1963836406",
        "abstract": "In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9 lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.",
        "related_work": "Web environment provides content publishers with a means to track user behavior in much greater detail than in an offline settings, including capturing user's registered information and activity logs of user's clicks, page views, searches, website visits, social activities, and interactions with ads. This allows for targeting of users based on their behavior, which is typically referred to as ad targeting @cite_29 . With the rise of big data applications and platforms, machine learning approaches are heavily leveraged to automate the ad targeting process. Within the past several years, there has been a plethora of research papers that explored different aspects of online advertising, each with the goal of maximizing the benefits for advertisers, content publishers, and users.",
        "ref_abstract": {
            "@cite_29": {
                "mid": "2142534468",
                "abstract": "Historical user activity is key for building user profiles to predict the user behavior and affinities in many web applications such as targeting of online advertising, content personalization and social recommendations. User profiles are temporal, and changes in a user's activity patterns are particularly useful for improved prediction and recommendation. For instance, an increased interest in car-related web pages may well suggest that the user might be shopping for a new vehicle.In this paper we present a comprehensive statistical framework for user profiling based on topic models which is able to capture such effects in a fully fashion. Our method models topical interests of a user dynamically where both the user association with the topics and the topics themselves are allowed to vary over time, thus ensuring that the profiles remain current. We describe a streaming, distributed inference algorithm which is able to handle tens of millions of users. Our results show that our model contributes towards improved behavioral targeting of display advertising relative to baseline models that do not incorporate topical and or temporal dependencies. As a side-effect our model yields human-understandable results which can be used in an intuitive fashion by advertisers.",
                "doi": "https://doi.org/10.1145/2020408.2020433",
                "title": "Scalable distributed inference of dynamic user interests for behavioral targeting",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1606.03671",
        "mid": "2171645525",
        "abstract": "Electronic microscopy has been used for morphology evaluation of different materials structures. However, microscopy results may be affected by several factors. Image processing methods can be used to correct and improve the quality of these results. In this paper we propose an algorithm based on starlets to perform the segmentation of scanning electron microscopy images. An application is presented in order to locate gold nanoparticles in natural rubber membranes. In this application, our method showed accuracy greater than 85 for all test images. Results given by this method will be used in future studies, to computationally estimate the density distribution of gold nanoparticles in natural rubber samples and to predict reduction kinetics of gold nanoparticles at different time periods.",
        "related_work": "Image segmentation is often used in microscopy images that are currently segmented to obtain features such as number of cells @cite_18 , separation of overlapped particles @cite_7 and skull-stripping of mouse brain @cite_14 .",
        "ref_abstract": {
            "@cite_18": {
                "mid": "1481100394",
                "abstract": "Summary Cell loss and addition is an important biological event in pathology, and it usually provides central information to the changes of biological activity in the histological sections. To develop a reliable and accurate cell counting tools in tissue section, in this paper, we proposed a novel cell nuclei detecting method based on the sliding band filter which is a member of convergence index family. We evaluated the accuracy and performance of our method on density packed retinal outer nuclear layer cell confocal multivariate fluorescence microscopy image datasets. The results show our proposed method exhibited an excellent performance with its accuracy compared with human manual counting. It is worth noting that the proposed cell counting method can clearly benefit for retinal detachment and reattachment visual diagnostics close related to cell loss and addition.",
                "doi": "https://doi.org/10.1111/jmi.12015",
                "title": "A counting method for density packed cells based on sliding band filter image enhancement",
                "publication_year": 2013
            },
            "@cite_14": {
                "mid": "1980582610",
                "abstract": "Small animal magnetic resonance microscopy (MRM) has been widely used today in computational neuroanatomy. Accurate identification of brain tissue in a mouse MRM is a critical fundamental step in neuroimaging processing, which is given less attention. This study presents an automatic skull-stripping technique based on template models and histogram analysis. Results were evaluated by calculating the Jaccard similarity index (JSI) and boundary concordance ratio (BCR) between the automatically segmented and manually traced brain volumes. Results demonstrate that this technique accurately extracts the brain volume (mean JSI = 97.1 , BC = 94.4 ). The brain extraction method presented in this study will greatly facilitate analysis of neuroimaging studies of rodent animals in neurodegenerative diseases. Microsc. Res. Tech. 2013. \u00a9 2012 Wiley Periodicals, Inc.",
                "doi": "https://doi.org/10.1002/jemt.22128",
                "title": "A template-based automatic skull-stripping approach for mouse brain MR microscopy",
                "publication_year": 2012
            },
            "@cite_7": {
                "mid": "2167248649",
                "abstract": "In this paper, we present a procedure to separate aggregates of overlapped particles in digital holograms, based on a focus plane analysis applied to each particle. The method can be applied either on phase or on amplitude objects, according that each object has a border in one focus plane. Numerical simulations are performed to quantify the robustness of the process by increasing the overlapping areas between the particles. The separation algorithm is successfully demonstrated experimentally on different types of aggregates.",
                "doi": "https://doi.org/10.1364/oe.21.006466",
                "title": "Separation of overlapped particles in digital holographic microscopy",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1606.02894",
        "mid": "2410085756",
        "abstract": "Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10 of the interocular distance.",
        "related_work": "DeepFace @cite_33 is one of these outstanding networks that contains a nine-layer deep CNN model with two convolutional layers and more than 120 million parameters trained on four million facial images from over 4,000 identities. This method, through alignment of images based on a 3D model and use of an ensemble of CNNs, could achieve accuracies of 97.35 FaceNet @cite_8 is a deep CNN based on GoogLeNet @cite_23 and the network proposed in @cite_47 and trained on a face dataset with 100 to 200 million images of around eight million identities. This algorithm uses triplets of roughly aligned faces obtained from an online triplet mining approach and directly learns to map face images to a compact Euclidean space to measure face similarity. FaceNet has been evaluated on the LFW and YTF datasets and has achieved accuracies of 99.63",
        "ref_abstract": {
            "@cite_47": {
                "mid": "1849277567",
                "abstract": "Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
                "doi": "https://doi.org/10.1007/978-3-319-10590-1_53",
                "title": "Visualizing and Understanding Convolutional Networks",
                "publication_year": 2014
            },
            "@cite_33": {
                "mid": "2145287260",
                "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect => align => represent => classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35 on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27 , closely approaching human-level performance.",
                "doi": "https://doi.org/10.1109/cvpr.2014.220",
                "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
                "publication_year": 2014
            },
            "@cite_23": {
                "mid": "2097117768",
                "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298594",
                "title": "Going deeper with convolutions",
                "publication_year": 2015
            },
            "@cite_8": {
                "mid": "2096733369",
                "abstract": "Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298682",
                "title": "FaceNet: A unified embedding for face recognition and clustering",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1606.02007",
        "mid": "2951409308",
        "abstract": "Internet of Things (IoT) aims to bring every object (e.g. smart cameras, wearable, environmental sensors, home appliances, and vehicles) online, hence generating massive amounts of data that can overwhelm storage systems and data analytics applications. Cloud computing offers services at the infrastructure level that can scale to IoT storage and processing requirements. However, there are applications such as health monitoring and emergency response that require low latency, and delay caused by transferring data to the cloud and then back to the application can seriously impact their performances. To overcome this limitation, Fog computing paradigm has been proposed, where cloud services are extended to the edge of the network to decrease the latency and network congestion. To realize the full potential of Fog and IoT paradigms for real-time analytics, several challenges need to be addressed. The first and most critical problem is designing resource management techniques that determine which modules of analytics applications are pushed to each edge device to minimize the latency and maximize the throughput. To this end, we need a evaluation platform that enables the quantification of performance of resource management policies on an IoT or Fog computing infrastructure in a repeatable manner. In this paper we propose a simulator, called iFogSim, to model IoT and Fog environments and measure the impact of resource management techniques in terms of latency, network congestion, energy consumption, and cost. We describe two case studies to demonstrate modeling of an IoT environment and comparison of resource management policies. Moreover, scalability of the simulation toolkit in terms of RAM consumption and execution time is verified under different circumstances.",
        "related_work": "The FIT IoT-LAB @cite_19 is a testbed equipped with thousands of wireless nodes located in six different sites across France. It allows users to evaluate and test their novel ideas ranging from low level protocols to advanced Analytic and services in a very large scale wireless IoT environment. Major services offered by IoT-LAB include: 1) Remote access to sensors and gateways: the testbed provides users with APIs to flash any firmware, design, build, and compile applications; 2) Access to the serial ports of reserved IoT devices; 3) Internet access for nodes with end-to-end IP connection using IPv6 and 6LoWPAN; 4) Power consumption monitoring per device; 5) and robots to test and improve real-time decision making in IoT context.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "2202090419",
                "abstract": "This paper introduces the FIT IoT-LAB testbed, an open testbed composed of 2728 low-power wireless nodes and 117 mobile robots available for experimenting with large-scale wireless IoT technologies, ranging from low-level protocols to advanced Internet services. IoT-LAB is built to accelerate the development of tomorrow's IoT technology by offering an accurate open-access and open-source multi-user scientific tool. The IoT-LAB testbed is deployed in 6 sites across France. Each site features different node and hardware capabilities, but all sites are interconnected and available through the same web portal, common REST interfaces and consistent CLI tools. The result is a heterogeneous testing environment, which covers a large spectrum of IoT use cases and applications. IoT-LAB is a one-of-a-kind facility, allowing anyone to test their solution at scale, experiment and fine-tune new networking concept.",
                "doi": "https://doi.org/10.1109/wf-iot.2015.7389098",
                "title": "FIT IoT-LAB: A large scale open experimental IoT testbed",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1606.01792",
        "mid": "2413436069",
        "abstract": "In this paper, we propose phraseNet, a neural machine translator with a phrase memory which stores phrase pairs in symbolic form, mined from corpus or specified by human experts. For any given source sentence, phraseNet scans the phrase memory to determine the candidate phrase pairs and integrates tagging information in the representation of source sentence accordingly. The decoder utilizes a mixture of word-generating component and phrase-generating component, with a specifically designed strategy to generate a sequence of multiple words all at once. The phraseNet not only approaches one step towards incorporating external knowledge into neural machine translation, but also makes an effort to extend the word-by-word generation mechanism of recurrent neural network. Our empirical study on Chinese-to-English translation shows that, with carefully-chosen phrase table in memory, phraseNet yields 3.45 BLEU improvement over the generic neural machine translator.",
        "related_work": "Probably the work that is closest to phraseNet is the recently proposed Neural Generative QA (genQA) @cite_1 , where a set of triples are stored in a QA memory, and a neural network queries this memory for words to use in generating the answer. More specifically, phraseNet @math has the same gating strategy as in genQA. Still, phraseNet is different from that in several important ways: 1) phraseNet can handle multiple phrases in one sentence, and 2) phraseNet can generate multi-word expression.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2181504572",
                "abstract": "This paper presents an end-to-end neural network model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to enquire the knowledge-base, and is trained on a corpus of question-answer pairs, with their associated triples in the knowledge-base. Empirical study shows the proposed model can effectively deal with the variations of questions and answers, and generate right and natural answers by referring to the facts in the knowledge-base. The experiment on question answering demonstrates that the proposed model can outperform an embedding-based QA model as well as a neural dialogue model trained on the same data.",
                "doi": "https://doi.org/10.48550/arxiv.1512.01337",
                "title": "Neural Generative Question Answering",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1606.01481",
        "mid": "2481602277",
        "abstract": "It is well accepted that image segmentation can benefit from utilizing multilevel cues. The paper focuses on utilizing the FCNN-based dense semantic predictions in the bottom-up image segmentation, arguing to take semantic cues into account from the very beginning. By this we can avoid merging regions of similar appearance but distinct semantic categories as possible. The semantic inefficiency problem is handled. We also propose a straightforward way to use the contour cues to suppress the noise in multilevel cues, thus to improve the segmentation robustness. The evaluation on the BSDS500 shows that we obtain the competitive region and boundary performance. Furthermore, since all individual regions can be assigned with appropriate semantic labels during the computation, we are capable of extracting the adjusted semantic segmentations. The experiment on Pascal VOC 2012 shows our improvement to the original semantic segmentations which derives directly from the dense predictions.",
        "related_work": "In the bottom-up merging segmentation approaches @cite_48 @cite_27 , it is beneficial to use contour cues learned from human annotations. Then rather than treating human segmentations as only static samples, what other stuff can we learn? In @cite_46 , it is noticed that although human segmentations are of the high quality, there are surprisingly many boundary mistakes, even in the renown BSDS500 dataset, see Fig. .a-b. Easy to see, these mistakes are made intentionally instead of by casual operations.",
        "ref_abstract": {
            "@cite_46": {
                "mid": "2323866876",
                "abstract": "Given that natural image segmentation is well-known as an ill-posed problem, then how can we design an algorithm to obtain good performance as human subjects? A choice is to learn from human segmentations as MCG [3] which obtains the state-of-the-art performance and fairly high computational efficiency. Then a question arises here: what should we learn? The way all existing literatures exploit human segmentations ignores a basic fact that human segmentations are produced by human operations. The human segmentation process would inevitably fulfill the human behavior\u2019s principles including the least effort principle (LEP): a human will strive to solve his problem in such a way as to minimize the total work that he must expend [1]. The principle gives us a new insight into our problem. Suppose you are a human subject in the BSDS segmentation experiment, and are required to segment images into pieces under the ending instruction: each piece contains only one single distinguished thing. Then your total effort F of segmenting an image should including understanding images (U) to guide following operations, and tracing boundaries (T ) by hands with a mouse: F (I,S) =U (I)+T (S). Then from the viewpoint of LEP, human subjects would like to find an acceptable segmentation S by minimizing their effort, or formally,",
                "doi": "https://doi.org/10.5244/c.29.110",
                "title": "Segmenting natural images with the least effort as humans",
                "publication_year": 2015
            },
            "@cite_48": {
                "mid": "2110158442",
                "abstract": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.",
                "doi": "https://doi.org/10.1109/tpami.2010.161",
                "title": "Contour Detection and Hierarchical Image Segmentation",
                "publication_year": 2011
            },
            "@cite_27": {
                "mid": "1991367009",
                "abstract": "We propose a unified approach for bottom-up hierarchical image segmentation and object candidate generation for recognition, called Multiscale Combinatorial Grouping (MCG). For this purpose, we first develop a fast normalized cuts algorithm. We then propose a high-performance hierarchical segmenter that makes effective use of multiscale information. Finally, we propose a grouping strategy that combines our multiscale regions into highly-accurate object candidates by exploring efficiently their combinatorial space. We conduct extensive experiments on both the BSDS500 and on the PASCAL 2012 segmentation datasets, showing that MCG produces state-of-the-art contours, hierarchical regions and object candidates.",
                "doi": "https://doi.org/10.1109/cvpr.2014.49",
                "title": "Multiscale Combinatorial Grouping",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1605.06814",
        "mid": "2395091410",
        "abstract": "Verifying that a network configuration satisfies a given boolean predicate is a fundamental problem in distributed computing. Many variations of this problem have been studied, for example, in the context of proof labeling schemes (PLS), locally checkable proofs (LCP), and non-deterministic local decision (NLD). In all of these contexts, verification time is assumed to be constant. Korman, Kutten and Masuzawa [PODC 2011] presented a proof-labeling scheme for MST, with poly-logarithmic verification time, and logarithmic memory at each vertex. In this paper we introduce the notion of a @math -PLS, which allows the verification procedure to run for super-constant time. Our work analyzes the tradeoffs of @math -PLS between time, label size, message length, and computation space. We construct a universal @math -PLS and prove that it uses the same amount of total communication as a known one-round universal PLS, and @math factor smaller labels. In addition, we provide a general technique to prove lower bounds for space-time tradeoffs of @math -PLS. We use this technique to show an optimal tradeoff for testing that a network is acyclic (cycle free). Our optimal @math -PLS for acyclicity uses label size and computation space @math . We further describe a recursive @math space verifier for acyclicity which does not assume previous knowledge of the run-time @math .",
        "related_work": "The question of what properties can be verified using a constant verification time was studied in @cite_15 , and several complexity classes were presented, including LD---local decision---which includes all properties that can be decided using constant number of rounds and no additional information, and NLD---non-deterministic local decision---which includes all properties that can be decided in a constant number of rounds with additional information in the form of a certificate given to each vertex. While NLD and PLS are closely related, they differ in that NLD certificates are independent of vertex identifiers. Since PLS labels may depend on vertex identifiers, there is a PLS for every sequentially decidable property on ID based networks, while not all sequentially decidable properties are in NLD. Our lower bounds in Subsections and allow labels to depend on unique vertex identifiers, so our arguments give identical lower bounds for certificate sizes in the weaker NLD model. Nonetheless, the schemes for @math in Subsections and do not require unique identifiers.",
        "ref_abstract": {
            "@cite_15": {
                "mid": "1999120866",
                "abstract": "A central theme in distributed network algorithms concerns understanding and coping with the issue of locality. Yet despite considerable progress, research efforts in this direction have not yet resulted in a solid basis in the form of a fundamental computational complexity theory for locality. Inspired by sequential complexity theory, we focus on a complexity theory for distributed decision problems. In the context of locality, solving a decision problem requires the processors to independently inspect their local neighborhoods and then collectively decide whether a given global input instance belongs to some specified language. We consider the standard LOCAL model of computation and define LD(t) (for local decision) as the class of decision problems that can be solved in t communication rounds. We first study the intriguing question of whether randomization helps in local distributed computing, and to what extent. Specifically, we define the corresponding randomized class BPLD(t,p,q), containing all languages for which there exists a randomized algorithm that runs in t rounds, accepts correct instances with probability at least p, and rejects incorrect ones with probability at least q. We show that p2 p q = 1 is a threshold for the containment of LD(t) in BPLD(t,p,q). More precisely, we show that there exists a language that does not belong to LD(t) for any t=o(n) but does belong to BPLD(0,p,q) for any p,q \u2208 (0,1) such that p2 p q \u2264 1. On the other hand, we show that, restricted to hereditary languages, BPLD(t,p,q)=LD(O(t)), for any function t, and any p, q \u2208 (0,1) such that p2 p q > 1. In addition, we investigate the impact of nondeterminism on local decision, and establish several structural results inspired by classical computational complexity theory. Specifically, we show that nondeterminism does help, but that this help is limited, as there exist languages that cannot be decided locally nondeterministically. Perhaps surprisingly, it turns out that it is the combination of randomization with nondeterminism that enables to decide all languages in constant time. Finally, we introduce the notion of local reduction, and establish a couple of completeness results.",
                "doi": "https://doi.org/10.1145/2499228",
                "title": "Towards a complexity theory for local distributed computing",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1605.03259",
        "mid": "2361187101",
        "abstract": "The visual appearance of a person is easily affected by many factors like pose variations, viewpoint changes and camera parameter differences. This makes person Re-Identification (ReID) among multiple cameras a very challenging task. This work is motivated to learn mid-level human attributes which are robust to such visual appearance variations. And we propose a semi-supervised attribute learning framework which progressively boosts the accuracy of attributes only using a limited number of labeled data. Specifically, this framework involves a three-stage training. A deep Convolutional Neural Network (dCNN) is first trained on an independent dataset labeled with attributes. Then it is fine-tuned on another dataset only labeled with person IDs using our defined triplet loss. Finally, the updated dCNN predicts attribute labels for the target dataset, which is combined with the independent dataset for the final round of fine-tuning. The predicted attributes, namely deep attributes exhibit superior generalization ability across different datasets. By directly using the deep attributes with simple Cosine distance, we have obtained surprisingly good accuracy on four person ReID datasets. Experiments also show that a simple distance metric learning modular further boosts our method, making it significantly outperform many recent works.",
        "related_work": "Currently, many studies have applied deep learning to attributes learning @cite_18 @cite_51 . Shankar @cite_18 propose a deep-carving neural net to learn attributes for natural scene images. Chen @cite_51 use a double-path deep domain adaptation network to get the fine-grained clothing attributes. Our work differs from them in the aspects of motivation and methodology. We are motivated by how to learn attributes of the human cropped from surveillance videos from a small set of data labeled with attributes. Our semi-supervised learning framework consistently boosts the discriminative power of dCNN and attributes for person ReID.",
        "ref_abstract": {
            "@cite_18": {
                "mid": "1918392599",
                "abstract": "Most of the approaches for discovering visual attributes in images demand significant supervision, which is cumbersome to obtain. In this paper, we aim to discover visual attributes in a weakly supervised setting that is commonly encountered with contemporary image search engines.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298962",
                "title": "DEEP-CARVING: Discovering visual attributes by carving deep neural nets",
                "publication_year": 2015
            },
            "@cite_51": {
                "mid": "1946323491",
                "abstract": "We address the problem of describing people based on fine-grained clothing attributes. This is an important problem for many practical applications, such as identifying target suspects or finding missing people based on detailed clothing descriptions in surveillance videos or consumer photos. We approach this problem by first mining clothing images with fine-grained attribute labels from online shopping stores. A large-scale dataset is built with about one million images and fine-detailed attribute sub-categories, such as various shades of color (e.g., watermelon red, rosy red, purplish red), clothing types (e.g., down jacket, denim jacket), and patterns (e.g., thin horizontal stripes, houndstooth). As these images are taken in ideal pose lighting background conditions, it is unreliable to directly use them as training data for attribute prediction in the domain of unconstrained images captured, for example, by mobile phones or surveillance cameras. In order to bridge this gap, we propose a novel double-path deep domain adaptation network to model the data from the two domains jointly. Several alignment cost layers placed inbetween the two columns ensure the consistency of the two domain features and the feasibility to predict unseen attribute categories in one of the domains. Finally, to achieve a working system with automatic human body alignment, we trained an enhanced RCNN-based detector to localize human bodies in images. Our extensive experimental evaluation demonstrates the effectiveness of the proposed approach for describing people based on fine-grained clothing attributes.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7299169",
                "title": "Deep domain adaptation for describing people based on fine-grained clothing attributes",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1604.08239",
        "mid": "2345096588",
        "abstract": "The increasing prevalence of Virtual Reality technologies as a platform for gaming and video playback warrants research into how to best apply the current state of the art to challenges in data visualization. Many current VR systems are noncollaborative, while data analysis and visualization is often a multi-person process. Our goal in this paper is to address the technical and user experience challenges that arise when creating VR environments for collaborative data visualization. We focus on the integration of multiple tracking systems and the new interaction paradigms that this integration can enable, along with visual design considerations that apply specifically to collaborative network visualization in virtual reality. We demonstrate a system for collaborative interaction with large 3D layouts of Twitter friend follow networks. The system is built by combining a 'Holojam' architecture (multiple GearVR Headsets within an OptiTrack motion capture stage) and Perception Neuron motion suits, to offer an untethered, full-room multi-person visualization experience.",
        "related_work": "Work by Donalek et. al. @cite_5 uses uses Unity3D and Oculus VR headsets to visualize astronomical datasets, but has key differences in terms of capabilities and constraints. The rendering in @cite_5 occurs on a personal computer as opposed to a mobile device, and therefore is less resource bound than in our context. Furthermore, the underlying system that we build upon is designed for collaborative full-room VR, allowing users to interact with a social network in much the same way they would a real object.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2116958202",
                "abstract": "Effective data visualization is a key part of the discovery process in the era of \u201cbig data\u201d. It is the bridge between the quantitative content of the data and human intuition, and thus an essential component of the scientific path from data into knowledge and understanding. Visualization is also essential in the data mining process, directing the choice of the applicable algorithms, and in helping to identify and remove bad data from the analysis. However, a high complexity or a high dimensionality of modern data sets represents a critical obstacle. How do we visualize interesting structures and patterns that may exist in hyper-dimensional data spaces? A better understanding of how we can perceive and interact with multidimensional information poses some deep questions in the field of cognition technology and human-computer interaction. To this effect, we are exploring the use of immersive virtual reality platforms for scientific data visualization, both as software and inexpensive commodity hardware. These potentially powerful and innovative tools for multi-dimensional data visualization can also provide an easy and natural path to a collaborative data visualization and exploration, where scientists can interact with their data and their colleagues in the same visual space. Immersion provides benefits beyond the traditional \u201cdesktop\u201d visualization tools: it leads to a demonstrably better perception of a datascape geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data.",
                "doi": "https://doi.org/10.1109/bigdata.2014.7004282",
                "title": "Immersive and collaborative data visualization using virtual reality platforms",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1604.06195",
        "mid": "2337540763",
        "abstract": "With the increase number of companies focusing on commercializing Augmented Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand based input mechanism is becoming essential in order to make the experience natural, seamless and immersive. Hand pose estimation has progressed drastically in recent years due to the introduction of commodity depth cameras. Hand pose estimation based on vision is still a challenging problem due to its complexity from self-occlusion (between fingers), close similarity between fingers, dexterity of the hands, speed of the pose and the high dimension of the hand kinematic parameters. Articulated hand pose estimation is still an open problem and under intensive research from both academia and industry. The 2 approaches used for hand pose estimation are: discriminative and generative. Generative approach is a model based that tries to fit a hand model to the observed data. Discriminative approach is appearance based, usually implemented with machine learning (ML) and require a large amount of training data. Recent hand pose estimation uses hybrid approach by combining both discriminative and generative methods into a single hand pipeline. In this paper, we focus on reviewing recent progress of hand pose estimation from depth sensor. We will survey discriminative methods, generative methods and hybrid methods. This paper is not a comprehensive review of all hand pose estimation techniques, it is a subset of some of the recent state-of-the-art techniques.",
        "related_work": "A more recent paper @cite_6 in 2015 focused on comparing 13 hand pose estimation algorithms from a single depth frame and evaluated their performance on various publicly available dataset. Furthermore, @cite_6 created a new hand training dataset that is more diverse and complex to existing one. @cite_6 focus primarily on comparing the quality of each of the 13 algorithms using a common training dataset, this paper focus on reviewing the latest state-of-the-art hand pose estimation algorithms.",
        "ref_abstract": {
            "@cite_6": {
                "mid": "1517258739",
                "abstract": "Hand pose estimation has matured rapidly in recent years. The introduction of commodity depth sensors and a multitude of practical applications have spurred new advances. We provide an extensive analysis of the state-of-the-art, focusing on hand pose estimation from a single depth frame. To do so, we have implemented a considerable number of systems, and have released software and evaluation code. We summarize important conclusions here: (1) Coarse pose estimation appears viable for scenes with isolated hands. However, high precision pose estimation [required for immersive virtual reality and cluttered scenes (where hands may be interacting with nearby objects and surfaces) remain a challenge. To spur further progress we introduce a challenging new dataset with diverse, cluttered scenes. (2) Many methods evaluate themselves with disparate criteria, making comparisons difficult. We define a consistent evaluation criteria, rigorously motivated by human experiments. (3) We introduce a simple nearest-neighbor baseline that outperforms most existing systems. This implies that most systems do not generalize beyond their training sets. This also reinforces the under-appreciated point that training data is as important as the model itself. We conclude with directions for future progress.",
                "doi": "https://doi.org/10.1007/s11263-018-1081-7",
                "title": "Depth-Based Hand Pose Estimation: Methods, Data, and Challenges",
                "publication_year": 2018
            }
        }
    },
    {
        "aid": "1604.03986",
        "mid": "2341479450",
        "abstract": "Policy advice is a transfer learning method where a student agent is able to learn faster via advice from a teacher. However, both this and other reinforcement learning transfer methods have little theoretical analysis. This paper formally defines a setting where multiple teacher agents can provide advice to a student and introduces an algorithm to leverage both autonomous exploration and teacher's advice. Our regret bounds justify the intuition that good teachers help while bad teachers hurt. Using our formalization, we are also able to quantify, for the first time, when negative transfer can occur within such a reinforcement learning setting.",
        "related_work": "Few theoretical results on transfer and policy advice have been achieved. Closest to this work is that in torrey2013teaching , where the authors only provide empirical validations to their approach without drawing on any theoretical analysis. Given the theoretical derivations in this paper, we in fact note that the method @cite_17 is a special case of ours considering only one-teacher advice models.",
        "ref_abstract": {
            "@cite_17": {
                "mid": "1969685488",
                "abstract": "This article introduces a teacher\u2013student framework for reinforcement learning, synthesising and extending material that appeared in conference proceedings [Torrey, L., & Taylor, M. E. (2013)]. Teaching on a budget: Agents advising agents in reinforcement learning. Proceedings of the international conference on autonomous agents and multiagent systems ] and in a non-archival workshop paper [Carboni, N., &Taylor, M. E. (2013, May)]. Preliminary results for 1 vs. 1 tactics in StarCraft. Proceedings of the adaptive and learning agents workshop (at AAMAS-13) ]. In this framework, a teacher agent instructs a student agent by suggesting actions the student should take as it learns. However, the teacher may only give such advice a limited number of times. We present several novel algorithms that teachers can use to budget their advice effectively, and we evaluate them in two complex video games: StarCraft and Pac-Man. Our results show that the same amount of advice, given at different moments, can have differe...",
                "doi": "https://doi.org/10.1080/09540091.2014.885279",
                "title": "Reinforcement learning agents providing advice in complex video games",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1604.03178",
        "mid": "2202525882",
        "abstract": "Peer grading systems work well only if users have incentives to grade truthfully. An example of non-truthful grading, that we observed in classrooms, consists in students assigning the maximum grade to all submissions. With a naive grading scheme, such as averaging the assigned grades, all students would receive the maximum grade. In this paper, we develop three grading schemes that provide incentives for truthful peer grading. In the first scheme, the instructor grades a fraction p of the submissions, and penalizes students whose grade deviates from the instructor grade. We provide lower bounds on p to ensure truthfulness, and conclude that these schemes work only for moderate class sizes, up to a few hundred students. To overcome this limitation, we propose a hierarchical extension of this supervised scheme, and we show that it can handle classes of any size with bounded (and little) instructor work, and is therefore applicable to Massive Open Online Courses (MOOCs). Finally, we propose unsupervised incentive schemes, in which the student incentive is based on statistical properties of the grade distribution, without any grading required by the instructor. We show that the proposed unsupervised schemes provide incentives to truthful grading, at the price of being possibly unfair to individual students.",
        "related_work": "Providing incentives to human agents to return thuthful responses is a central challenge of crowdsourcing algorithms and applications @cite_9 .",
        "ref_abstract": {
            "@cite_9": {
                "mid": "750688931",
                "abstract": "The success of a human computation system depends critically on the humans in the system actually behaving, or acting, as necessary for the system to function effectively. Since users have their own costs and benefits from participation, they will undertake desirable actions only if properly incentivized to do so: Indeed, while there are a vast number of human computation systems on the Web, the extent of participation and quality of contribution varies widely across systems. How can a game-theoretic approach help understand why, and provide guidance on designing systems that incentivize high participation and effort from contributors?",
                "doi": "https://doi.org/10.1007/978-1-4614-8806-4_58",
                "title": "Game Theory and Incentives in Human Computation Systems",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1604.02694",
        "mid": "2341839464",
        "abstract": "Social status refers to the relative position within the society. It is an important notion in sociology and related research. The problem of measuring social status has been studied for many years. Various indicators are proposed to assess social status of individuals, including educational attainment, occupation, and income wealth. However, these indicators are sometimes difficult to collect or measure. We investigate social networks for alternative measures of social status. Online activities expose certain traits of users in the real world. We are interested in how these activities are related to social status, and how social status can be predicted with social network data. To the best of our knowledge, this is the first study on connecting online activities with social status in reality. In particular, we focus on the network structure of microblogs in this study. A user following another implies some kind of status. We cast the predicted social status of users to the \"status\" of real-world entities, e.g., universities, occupations, and regions, so that we can compare and validate predicted results with facts in the real world. We propose an efficient algorithm for this task and evaluate it on a dataset consisting of 3.4 million users from Sina Weibo. The result shows that it is possible to predict social status with reasonable accuracy using social network data. We also point out challenges and limitations of this approach, e.g., inconsistence between online popularity and real-world status for certain users. Our findings provide insights on analyzing online social status and future designs of ranking schemes for social networks.",
        "related_work": "Membership inference can be considered as a user profiling problem, where missing attributes are inferred from known attributes and network structure. Several works are built on the idea of propagation. ( backstrom2010 ) proposed an algorithm specialized to locations. ( mislove2010 ) infer user profiles in a university by applying a graph clustering algorithm. However, members of the detected cluster do not necessarily share the same attribute. Pennacchiotti and Popescu ( pennacchiotti2011 ) proposed a hybrid algorithm to infer binary attributes, by combining machine learning methods and propagation on networks. More complicated models are proposed to capture the correlation between different types of attributes @cite_28 , and the relationship between node and edge @cite_19 . Limitations of the user profiling problem were discussed in @cite_10 .",
        "ref_abstract": {
            "@cite_28": {
                "mid": "1968103943",
                "abstract": "Users in online social networks play a variety of social roles and statuses. For example, users in Twitter can be represented as advertiser, content contributor, information receiver, etc; users in Linkedin can be in different professional roles, such as engineer, salesperson and recruiter. Previous research work mainly focuses on using categorical and textual information to predict the attributes of users. However, it cannot be applied to a large number of users in real social networks, since much of such information is missing, outdated and non-standard. In this paper, we investigate the social roles and statuses that people act in online social networks in the perspective of network structures, since the uniqueness of social networks is connecting people. We quantitatively analyze a number of key social principles and theories that correlate with social roles and statuses. We systematically study how the network characteristics reflect the social situations of users in an online society. We discover patterns of homophily, the tendency of users to connect with users with similar social roles and statuses. In addition, we observe that different factors in social theories influence the social role status of an individual user to various extent, since these social principles represent different aspects of the network. We then introduce an optimization framework based on Factor Conditioning Symmetry, and we propose a probabilistic model to integrate the optimization framework on local structural information as well as network influence to infer the unknown social roles and statuses of online users. We will present experiment results to show the effectiveness of the inference.",
                "doi": "https://doi.org/10.1145/2487575.2487597",
                "title": "Inferring social roles and statuses in social networks",
                "publication_year": 2013
            },
            "@cite_19": {
                "mid": "2107961038",
                "abstract": "User attributes, such as occupation, education, and location, are important for many applications. In this paper, we study the problem of profiling user attributes in social network. To capture the correlation between attributes and social connections, we present a new insight that social connections are discriminatively correlated with attributes via a hidden factor -- relationship type. For example, a user's colleagues are more likely to share the same employer with him than other friends. Based on the insight, we propose to co-profile users' attributes and relationship types of their connections. To achieve co-profiling, we develop an efficient algorithm based on an optimization framework. Our algorithm captures our insight effectively. It iteratively profiles attributes by propagation via certain types of connections, and profiles types of connections based on attributes and the network structure. We conduct extensive experiments to evaluate our algorithm. The results show that our algorithm profiles various attributes accurately, which improves the state-of-the-art methods by 12 .",
                "doi": "https://doi.org/10.1145/2566486.2568045",
                "title": "User profiling in an ego network",
                "publication_year": 2014
            },
            "@cite_10": {
                "mid": "68298479",
                "abstract": "Numerous papers have reported great success at inferring the political orientation of Twitter users. This paper has some unfortunate news to deliver: while past work has been sound and often methodologically novel, we have discovered that reported accuracies have been systemically overoptimistic due to the way in which validation datasets have been collected, reporting accuracy levels nearly 30 higher than can be expected in populations of general Twitter users. Using careful and novel data collection and annotation techniques, we collected three different sets of Twitter users, each characterizing a different degree of political engagement on Twitter - from politicians (highly politically vocal) to \"normal\" users (those who rarely discuss politics). Applying standard techniques for inferring political orientation, we show that methods which previously reported greater than 90 inference accuracy, actually achieve barely 65 accuracy on normal users. We also show that classifiers cannot be used to classify users outside the narrow range of political orientation on which they were trained. While a sobering finding, our results quantify and call attention to overlooked problems in the latent attribute inference literature that, no doubt, extend beyond political orientation inference: the way in which datasets are assembled and the transferability of classifiers.",
                "doi": "https://doi.org/10.1609/icwsm.v7i1.14434",
                "title": "Classifying Political Orientation on Twitter: It\u2019s Not Easy!",
                "publication_year": 2021
            }
        }
    },
    {
        "aid": "1604.02531",
        "mid": "2337600727",
        "abstract": "We present a novel large-scale dataset and comprehensive baselines for end-to-end pedestrian detection and person recognition in raw video frames. Our baselines address three issues: the performance of various combinations of detectors and recognizers, mechanisms for pedestrian detection to help improve overall re-identification accuracy and assessing the effectiveness of different detectors for re-identification. We make three distinct contributions. First, a new dataset, PRW, is introduced to evaluate Person Re-identification in the Wild, using videos acquired through six synchronized cameras. It contains 932 identities and 11,816 frames in which pedestrians are annotated with their bounding box positions and identities. Extensive benchmarking results are presented on this dataset. Second, we show that pedestrian detection aids re-identification through two simple yet effective improvements: a discriminatively trained ID-discriminative Embedding (IDE) in the person subspace using convolutional neural network (CNN) features and a Confidence Weighted Similarity (CWS) metric that incorporates detection scores into similarity measurement. Third, we derive insights in evaluating detector performance for the particular scenario of accurate person re-identification.",
        "related_work": "Pedestrian detection. Recent pedestrian detection works feature the proposal+CNN'' approach. Pedestrian detection usually employs weak pedestrian detectors as proposals, which allows achieving relatively high recall using very few proposals @cite_51 @cite_5 @cite_13 @cite_57 . Despite the impressive recent progress in pedestrian detection, it has been rarely considered with person re-ID as an application. This paper attempts to determine how detection can help re-ID and provide insights in assessing detector performance.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "2156547346",
                "abstract": "Feature extraction, deformation handling, occlusion handling, and classification are four important components in pedestrian detection. Existing methods learn or design these components either individually or sequentially. The interaction among these components is not yet well explored. This paper proposes that they should be jointly learned in order to maximize their strengths through cooperation. We formulate these four components into a joint deep learning framework and propose a new deep network architecture. By establishing automatic, mutual interaction among components, the deep model achieves a 9 reduction in the average miss rate compared with the current best-performing pedestrian detection approaches on the largest Caltech benchmark dataset.",
                "doi": "https://doi.org/10.1109/iccv.2013.257",
                "title": "Joint Deep Learning for Pedestrian Detection",
                "publication_year": 2013
            },
            "@cite_5": {
                "mid": "2151454023",
                "abstract": "Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. The difficulty is added when several pedestrians overlap in images and occlude each other. We observe, however, that the occlusion visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation - the visibility estimation of one pedestrian facilitates the visibility estimation of another. In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the Caltech-Train dataset, the Caltech-Test dataset and the ETH dataset. Including mutual visibility leads to 4 - 8 improvements on multiple benchmark datasets.",
                "doi": "https://doi.org/10.1109/cvpr.2013.414",
                "title": "Modeling Mutual Visibility Relationship in Pedestrian Detection",
                "publication_year": 2013
            },
            "@cite_57": {
                "mid": "2084997728",
                "abstract": "In this paper, we propose a Switchable Deep Network (SDN) for pedestrian detection. The SDN automatically learns hierarchical features, salience maps, and mixture representations of different body parts. Pedestrian detection faces the challenges of background clutter and large variations of pedestrian appearance due to pose and viewpoint changes and other factors. One of our key contributions is to propose a Switchable Restricted Boltzmann Machine (SRBM) to explicitly model the complex mixture of visual variations at multiple levels. At the feature levels, it automatically estimates saliency maps for each test sample in order to separate background clutters from discriminative regions for pedestrian detection. At the part and body levels, it is able to infer the most appropriate template for the mixture models of each part and the whole body. We have devised a new generative algorithm to effectively pretrain the SDN and then fine-tune it with back-propagation. Our approach is evaluated on the Caltech and ETH datasets and achieves the state-of-the-art detection performance.",
                "doi": "https://doi.org/10.1109/cvpr.2014.120",
                "title": "Switchable Deep Network for Pedestrian Detection",
                "publication_year": 2014
            },
            "@cite_51": {
                "mid": "1986905809",
                "abstract": "Part-based models have demonstrated their merit in object detection. However, there is a key issue to be solved on how to integrate the inaccurate scores of part detectors when there are occlusions or large deformations. To handle the imperfectness of part detectors, this paper presents a probabilistic pedestrian detection framework. In this framework, a deformable part-based model is used to obtain the scores of part detectors and the visibilities of parts are modeled as hidden variables. Unlike previous occlusion handling approaches that assume independence among visibility probabilities of parts or manually define rules for the visibility relationship, a discriminative deep model is used in this paper for learning the visibility relationship among overlapping parts at multiple layers. Experimental results on three public datasets (Caltech, ETH and Daimler) and a new CUHK occlusion dataset1 specially designed for the evaluation of occlusion handling approaches show the effectiveness of the proposed approach.",
                "doi": "https://doi.org/10.1109/cvpr.2012.6248062",
                "title": "A discriminative deep model for pedestrian detection with occlusion handling",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1603.09325",
        "mid": "2323156992",
        "abstract": "The finite element methods are important and powerful tools for solving partial differential equations on complex geometries, but their high-order generalizations pose significant challenges in terms of robustness and mesh generation. In this paper, we introduce a high-order finite element method, which is insensitive to element quality and hence is more robust, and which requires only linear elements even for curved geometries and hence significantly simplifies meshing. Based on our recent work on Adaptive Extended Stencil FEM, or AES-FEM (Int. J. Num. Meth. Engrg., 2016, DOI:10.1002 nme.5246), the proposed method replaces the traditional interpolatory FEM basis functions with generalized Lagrange polynomial basis functions, constructed via local weighted least-squares approximations. In this work, we show that AES-FEM can achieve high-order accuracy using linear elements to discretize the geometry and using the standard FEM hat functions as test functions even for curved geometries. This is in contrast to most high-order generalizations of FEM, which require high-order curved elements for curved geometries. We describe the selection of stencils and the treatment of boundary conditions for high-order AES-FEM, and show that even-degree polynomial basis functions are preferred for even-order PDEs. We present numerical results in 2D and 3D for the Poisson equation and the convection-diffusion equation and demonstrate up to sixth order accuracy with AES-FEM. Overall, AES-FEM is much easier to use than FEM for curved boundaries, while being more accurate, more stable, and more efficient in terms of runtime versus error over relatively fine meshes.",
        "related_work": "Consider a partial differential equation on a bounded, simply-connected domain @math , subject to the Dirichlet and Neumann boundary conditions @math denotes a linear differential operator, @math and @math are disjoint sets of the boundary with @math , and @math denotes the outward normal to @math . A finite element method, or more generally a weighted-residual method @cite_38 , introduces a set of test functions (a.k.a. weight functions) @math , and require the residual @math to be orthogonal to @math , i.e., Eq. ) and ) are the and of the PDE, respectively. In finite element methods, @math is typically (weakly) differentiate and has a local support. In addition, @math typically forms a partition of unity, i.e., @math , so that the weighted-residual formulation is global conservative in that @math",
        "ref_abstract": {
            "@cite_38": {
                "mid": "1606119439",
                "abstract": "Preface to the classics edition Preface Acknowledgments Part I. The Method of Weighted Residuals: 1. Introduction 2. Boundary-value problems in heat and mass transfer 3. Eigenvalue and initial-value problems in heat and mass transfer 4. Applications to fluid mechanics 5. Chemical reaction systems 6. Convective instability problems Part II. Variational Principles: 7. Introduction to variational principles 8. Variational principles in fluid mechanics 9. Variational principles for heat and mass transfer problems 10. On the search for variational principles 11. Convergence and error bounds Author index Subject index.",
                "doi": "https://doi.org/10.1137/1.9781611973242",
                "title": "The Method of Weighted Residuals and Variational Principles",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1603.07641",
        "mid": "2008814765",
        "abstract": "The explosion in the availability of GPS-enabled devices has resulted in an abundance of trajectory data. In reality, however, majority of these trajectories are collected at a low sampling rate and only provide partial observations on their actually traversed routes. Consequently, they are mired with uncertainty. In this paper, we develop a technique called Infer Tra to infer uncertain trajectories from network-constrained partial observations. Rather than predicting the most likely route, the inferred uncertain trajectory takes the form of an edge-weighted graph and summarizes all probable routes in a holistic manner. For trajectory inference, Infer Tra employs Gibbs sampling by learning a Network Mobility Model (NMM) from a database of historical trajectories. Extensive experiments on real trajectory databases show that the graph-based approach of Infer Tra is up to 50 more accurate, 20 times faster, and immensely more versatile than state-of-the-art techniques.",
        "related_work": "HRIS @cite_16 is the first and the only work to infer network-constrained trajectories from partial observations. In this section, we outline how InferTra is different.",
        "ref_abstract": {
            "@cite_16": {
                "mid": "2049626361",
                "abstract": "The increasing availability of GPS-embedded mobile devices has given rise to a new spectrum of location-based services, which have accumulated a huge collection of location trajectories. In practice, a large portion of these trajectories are of low-sampling-rate. For instance, the time interval between consecutive GPS points of some trajectories can be several minutes or even hours. With such a low sampling rate, most details of their movement are lost, which makes them difficult to process effectively. In this work, we investigate how to reduce the uncertainty in such kind of trajectories. Specifically, given a low-sampling-rate trajectory, we aim to infer its possible routes. The methodology adopted in our work is to take full advantage of the rich information extracted from the historical trajectories. We propose a systematic solution, History based Route Inference System (HRIS), which covers a series of novel algorithms that can derive the travel pattern from historical data and incorporate it into the route inference process. To validate the effectiveness of the system, we apply our solution to the map-matching problem which is an important application scenario of this work, and conduct extensive experiments on a real taxi trajectory dataset. The experiment results demonstrate that HRIS can achieve higher accuracy than the existing map-matching algorithms for low-sampling-rate trajectories.",
                "doi": "https://doi.org/10.1109/icde.2012.42",
                "title": "Reducing Uncertainty of Low-Sampling-Rate Trajectories",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1603.05846",
        "mid": "2302448976",
        "abstract": "Typically, locally repairable codes (LRCs) and regenerating codes have been studied independently of each other, and it has not been clear how the parameters of one relate to those of the other. In this paper, a novel connection between locally repairable codes and exact regenerating codes is established. Via this connection, locally repairable codes are interpreted as exact regenerating codes. Further, some of these codes are shown to perform better than time-sharing codes between minimum bandwidth regenerating and minimum storage regenerating codes.",
        "related_work": "Optimal exact regenerating codes at the MBR point for all values of @math and optimal exact regenerating codes at the MSR point when @math are constructed in @cite_14 . The asymptotic achievability of the MSR point for all values of @math is shown in @cite_15 .",
        "ref_abstract": {
            "@cite_15": {
                "mid": "1977073502",
                "abstract": "The high repair bandwidth cost of (n,k) maximum distance separable (MDS) erasure codes has motivated a new class of codes that can reduce repair bandwidth over that of conventional MDS codes. In this paper, we address (n,k,d) exact repair MDS codes, which allow for any single failed node to be repaired exactly with access to any arbitrary set of d survivor nodes. We show the existence of exact repair MDS codes that achieve minimum repair bandwidth (matching the cut-set lower bound) for arbitrary admissible (n,k,d), i.e., k \u2264 d \u2264 n-1. Moreover, we extend our results to show the optimality of our codes for multiple-node failure scenarios in which an arbitrary set of r \u2264 n-k failed nodes needs to repaired. Our approach is based on asymptotic interference alignment proposed by Cadambe and Jafar. As a byproduct, we also characterize the capacity of a class of multisource nonmulticast networks.",
                "doi": "https://doi.org/10.1109/tit.2013.2237752",
                "title": "Asymptotic Interference Alignment for Optimal Repair of MDS Codes in Distributed Storage",
                "publication_year": 2013
            },
            "@cite_14": {
                "mid": "2150777202",
                "abstract": "Regenerating codes are a class of distributed storage codes that allow for efficient repair of failed nodes, as compared to traditional erasure codes. An [n, k, d] regenerating code permits the data to be recovered by connecting to any k of the n nodes in the network, while requiring that a failed node be repaired by connecting to any d nodes. The amount of data downloaded for repair is typically much smaller than the size of the source data. Previous constructions of exact-regenerating codes have been confined to the case n=d+1 . In this paper, we present optimal, explicit constructions of (a) Minimum Bandwidth Regenerating (MBR) codes for all values of [n, k, d] and (b) Minimum Storage Regenerating (MSR) codes for all [n, k, d \u2265 2k-2], using a new product-matrix framework. The product-matrix framework is also shown to significantly simplify system operation. To the best of our knowledge, these are the first constructions of exact-regenerating codes that allow the number n of nodes in the network, to be chosen independent of the other parameters. The paper also contains a simpler description, in the product-matrix framework, of a previously constructed MSR code with [n=d+1, k, d \u2265 2k-1].",
                "doi": "https://doi.org/10.1109/tit.2011.2159049",
                "title": "Optimal Exact-Regenerating Codes for Distributed Storage at the MSR and MBR Points via a Product-Matrix Construction",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1603.02844",
        "mid": "2296324964",
        "abstract": "In this paper, we aim to learn a mapping (or embedding) from images to a compact binary space in which Hamming distances correspond to a ranking measure for the image retrieval task. We make use of a triplet loss because this has been shown to be most effective for ranking problems. However, training in previous works can be prohibitively expensive due to the fact that optimization is directly performed on the triplet space, where the number of possible triplets for training is cubic in the number of training examples. To address this issue, we propose to formulate high-order binary codes learning as a multi-label classification problem by explicitly separating learning into two interleaved stages. To solve the first stage, we design a large-scale high-order binary codes inference algorithm to reduce the high-order objective to a standard binary quadratic problem such that graph cuts can be used to efficiently infer the binary code which serve as the label of each training datum. In the second stage we propose to map the original image to compact binary codes via carefully designed deep convolutional neural networks (CNNs) and the hashing function fitting can be solved by training binary CNN classifiers. An incremental interleaved optimization strategy is proffered to ensure that these two steps are interactive with each other during training for better accuracy. We conduct experiments on several benchmark datasets, which demonstrate both improved training time (by as much as two orders of magnitude) as well as producing state-of-the-art hashing for various retrieval tasks.",
        "related_work": "Recently hashing using deep learning has shown great promise. The authors of @cite_30 @cite_18 learn hash bits such that multilevel semantic similarities are kept, taking raw pixels as input and training a deep CNN. This has the effect of simultaneously learning an image feature representation (in the early layers of the network) and the hash bits, which are obtained by thresholding the outputs of the last network layer, or at 0.5. Note that these methods suffer from huge computation complexity introduced by the triplet ranking loss for hashing. In contrast, our proposed method is much more efficient in training, as shown in our experiments.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "1923967535",
                "abstract": "With the rapid growth of web images, hashing has received increasing interests in large scale image retrieval. Research efforts have been devoted to learning compact binary codes that preserve semantic similarity based on labels. However, most of these hashing methods are designed to handle simple binary similarity. The complex multi-level semantic structure of images associated with multiple labels have not yet been well explored. Here we propose a deep semantic ranking based method for learning hash functions that preserve multilevel semantic similarity between multi-label images. In our approach, deep convolutional neural network is incorporated into hash functions to jointly learn feature representations and mappings from them to hash codes, which avoids the limitation of semantic representation power of hand-crafted features. Meanwhile, a ranking list that encodes the multilevel similarity information is employed to guide the learning of such deep hash functions. An effective scheme based on surrogate loss is used to solve the intractable optimization problem of nonsmooth and multivariate ranking measures involved in the learning procedure. Experimental results show the superiority of our proposed approach over several state-of-the-art hashing methods in term of ranking evaluation metrics when tested on multi-label image datasets.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298763",
                "title": "Deep semantic ranking based hashing for multi-label image retrieval",
                "publication_year": 2015
            },
            "@cite_18": {
                "mid": "1939575207",
                "abstract": "Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual features, followed by another separate projection or quantization step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution layers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate image features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to characterize that one image is more similar to the second image than to the third one. Extensive evaluations on several benchmark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substantial improvements over other state-of-the-art supervised or unsupervised hashing methods.",
                "doi": "https://doi.org/10.1109/cvpr.2015.7298947",
                "title": "Simultaneous feature learning and hash coding with deep neural networks",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1602.02339",
        "mid": "2951907956",
        "abstract": "Autoscaling is a hallmark of cloud computing as it allows flexible just-in-time allocation and release of computational resources in response to dynamic and often unpredictable workloads. This is especially important for web applications whose workload is time dependent and prone to flash crowds. Most of them follow the 3-tier architectural pattern, and are divided into presentation, application domain and data layers. In this work we focus on the application layer. Reactive autoscaling policies of the type \"Instantiate a new Virtual Machine (VM) when the average server CPU utilisation reaches X \" have been used successfully since the dawn of cloud computing. But which VM type is the most suitable for the specific application at the moment remains an open question. In this work, we propose an approach for dynamic VM type selection. It uses a combination of online machine learning techniques, works in real time and adapts to changes in the users' workload patterns, application changes as well as middleware upgrades and reconfigurations. We have developed a prototype, which we tested with the CloudStone benchmark deployed on AWS EC2. Results show that our method quickly adapts to workload changes and reduces the total cost compared to the industry standard approach.",
        "related_work": "The area of static computing resource management has been well studied in the context of grids, clouds, and even multi-clouds @cite_28 . However, the field of dynamic resource management in response to continuously varying workloads, which is especially important for web facing applications @cite_28 , is still in its infancy. Horizontal autoscaling policies are the predominant approach for dynamic resource management and thus they have gained significant attention in recent years.",
        "ref_abstract": {
            "@cite_28": {
                "mid": "2153084067",
                "abstract": "In the past few years, we have witnessed the proliferation of a heterogeneous ecosystem of cloud providers, each one with a different infrastructure offer and pricing policy. We explore this heterogeneity in a novel cloud brokering approach that optimizes placement of virtual infrastructures across multiple clouds and also abstracts the deployment and management of infrastructure components in these clouds. The feasibility of our approach is evaluated in a high throughput computing cluster case study. Experimental results confirm that multi-cloud deployment provides better performance and lower costs compared to the usage of a single cloud only.",
                "doi": "https://doi.org/10.1016/j.future.2011.07.003",
                "title": "Cloud brokering mechanisms for optimized placement of virtual machines across multiple providers",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1601.00955",
        "mid": "2227733783",
        "abstract": "We consider the problem of learning decision rules for prediction with feature budget constraint. In particular, we are interested in pruning an ensemble of decision trees to reduce expected feature cost while maintaining high prediction accuracy for any test example. We propose a novel 0-1 integer program formulation for ensemble pruning. Our pruning formulation is general - it takes any ensemble of decision trees as input. By explicitly accounting for feature-sharing across trees together with accuracy cost trade-off, our method is able to significantly reduce feature cost by pruning subtrees that introduce more loss in terms of feature cost than benefit in terms of prediction accuracy gain. Theoretically, we prove that a linear programming relaxation produces the exact solution of the original integer program. This allows us to use efficient convex optimization tools to obtain an optimally pruned ensemble for any given budget. Empirically, we see that our pruning algorithm significantly improves the performance of the state of the art ensemble method BudgetRF.",
        "related_work": "Generally, pruning is not considered when constructing random forests as overfitting is avoided by constructing an ensemble of trees. The ensemble approach is a strong approach to avoiding overfitting, however test-time budget constraint problems require consideration of both cost and accuracy. Kulkarni and Sinha @cite_14 provide a survey of methods to prune random forests in order to reduce ensemble size. However, these methods do not explicitly account for feature costs.",
        "ref_abstract": {
            "@cite_14": {
                "mid": "2112716550",
                "abstract": "Random Forest is an ensemble supervised machine learning technique. Based on bagging and random feature selection, number of decision trees (base classifiers) is generated and majority voting is taken for classification. For effective learning and classification of Random Forest, there is need for reducing number of trees (Pruning) in Random Forest. We have presented here systematic survey of pruning efforts of Random Forest classifier along with the required theoretical background. Most of the work for pruning takes static approach while recently dynamic pruning is being targeted. We have also generated a Comparison Chart by taking relevant parameters. There is research scope for analyzing behavior of Random forest, generating accurate and diverse base decision trees, truly dynamic pruning algorithm for Random Forest classifier, and generating optimal subset of Random forest.",
                "doi": "https://doi.org/10.1109/icdse.2012.6282329",
                "title": "Pruning of Random Forest classifiers: A survey and future directions",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1512.03839",
        "mid": "2211625890",
        "abstract": "In this paper, we propose an adaptive medium access control (MAC) protocol for full-duplex (FD) cognitive radio networks in which FD secondary users (SUs) perform channel contention followed by concurrent spectrum sensing and transmission, and transmission only with maximum power in two different stages (called the FD sensing and transmission stages, respectively) in each contention and access cycle. The proposed FD cognitive MAC (FDC-MAC) protocol does not require synchronization among SUs, and it efficiently utilizes the spectrum and mitigates the self-interference in the FD transceiver. We develop a mathematical model to analyze the throughput performance of the FDC-MAC protocol, where both half-duplex (HD) transmission and FD transmission modes are considered in the transmission stage. Then, we study the FDC-MAC configuration optimization through adaptively controlling the spectrum sensing duration and transmit power level in the FD sensing stage. We prove that there exists optimal sensing time and transmit power to achieve the maximum throughput, and we develop an algorithm to configure the proposed FDC-MAC protocol. Extensive numerical results are presented to illustrate the optimal FDC-MAC configuration and the impacts of protocol parameters and the self-interference cancellation quality on the throughput performance. Moreover, we demonstrate the significant throughput gains of the FDC-MAC protocol with respect to the existing HD MAC and single-stage FD MAC protocols.",
        "related_work": "There are some recent works that propose to exploit the FD communications for MAC-level channel access in multi-user wireless networks @cite_13 -- @cite_8 . In @cite_13 , the authors develop a centralized MAC protocol to support asymmetric data traffic where network nodes may transmit data packets of different lengths, and they propose to mitigate the hidden node problem by employing a busy tone. To overcome this hidden node problem, propose to adapt the standard 802.11 MAC protocol with the RTS CTS handshake in @cite_3 . Moreover, in @cite_27 extend this study to consider interference between two nodes due to their concurrent transmissions. Different from conventional wireless networks, designing MAC protocols in CRNs is more challenging because the spectrum sensing function must be efficiently integrated into the MAC protocol. In addition, the self-interference must be carefully addressed in the simultaneous spectrum sensing and access to mitigate its negative impacts on the sensing and throughput performance.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2028186561",
                "abstract": "Recent advances in antenna and circuit design enable radios that operate in full duplex mode on a single channel with very low residual self-interference. In this paper, the use of such full duplex radios in a wireless local area network (WLAN) is explored. Different scenarios in which the full duplex transmission can be exploited are studied. A distributed full duplex MAC design based on IEEE 802.11 DCF that adopts to the traffic conditions is proposed. The proposed MAC design works for both ad hoc and infrastructure modes of WLAN and takes into consideration new interference and contention during full duplex transmissions. OPNET simulations comparing the performance of the proposed MAC with traditional half duplex based IEEE 802.11 DCF show that the new MAC protocol provides up to 88 throughput gain in a heavily loaded network.",
                "doi": "https://doi.org/10.1109/acssc.2013.6810393",
                "title": "A distributed MAC protocol for full duplex radio",
                "publication_year": 2013
            },
            "@cite_13": {
                "mid": "2049516452",
                "abstract": "In-band full-duplex (IBFD) operation has emerged as an attractive solution for increasing the throughput of wireless communication systems and networks. With IBFD, a wireless terminal is allowed to transmit and receive simultaneously in the same frequency band. This tutorial paper reviews the main concepts of IBFD wireless. One of the biggest practical impediments to IBFD operation is the presence of self-interference, i.e., the interference that the modem's transmitter causes to its own receiver. This tutorial surveys a wide range of IBFD self-interference mitigation techniques. Also discussed are numerous other research challenges and opportunities in the design and analysis of IBFD wireless systems.",
                "doi": "https://doi.org/10.1109/jsac.2014.2330193",
                "title": "In-Band Full-Duplex Wireless: Challenges and Opportunities",
                "publication_year": 2014
            },
            "@cite_3": {
                "mid": "2106543408",
                "abstract": "In this paper, we present an experiment- and simulation-based study to evaluate the use of full duplex (FD) as a potential mode in practical IEEE 802.11 networks. To enable the study, we designed a 20-MHz multiantenna orthogonal frequency-division-multiplexing (OFDM) FD physical layer and an FD media access control (MAC) protocol, which is backward compatible with current 802.11. Our extensive over-the-air experiments, simulations, and analysis demonstrate the following two results. First, the use of multiple antennas at the physical layer leads to a higher ergodic throughput than its hardware-equivalent multiantenna half-duplex (HD) counterparts for SNRs above the median SNR encountered in practical WiFi deployments. Second, the proposed MAC translates the physical layer rate gain into near doubling of throughput for multinode single-AP networks. The two results allow us to conclude that there are potentially significant benefits gained from including an FD mode in future WiFi standards.",
                "doi": "https://doi.org/10.1109/tvt.2013.2284712",
                "title": "Design and Characterization of a Full-Duplex Multiantenna System for WiFi Networks",
                "publication_year": 2014
            },
            "@cite_8": {
                "mid": "2035894106",
                "abstract": "Recent advances in signal processing have demonstrated in-band full-duplex capability at WiFi ranges. In addition to simultaneous two-way exchange between two nodes, full-duplex access points can potentially support simultaneous uplink and downlink flows. However, the atomic three-node topology, which allows simultaneous uplink and downlink, leads to inter-client interference. In this paper, we propose a random-access medium access control protocol using distributed power control to manage inter-client interference in wireless networks with full-duplex-capable access points that serve half-duplex clients. Our key contributions are two-fold. First, we identify the regimes in which power control provides sum throughput gains for the three-node atomic topology, with one uplink flow and one downlink flow. Second, we develop and benchmark PoCMAC, a full 802.11-based protocol that allows distributed selection of a three-node topology. The proposed MAC protocol is shown to achieve higher capacity as compared to an equivalent half-duplex counterpart, while maintaining similar fairness characteristics in single contention domain networks. We carried out extensive simulations and software-defined radio-based experiments to evaluate the performance of the proposed MAC protocol, which is shown to achieve a significant improvement over its half-duplex counterpart in terms of throughput performance.",
                "doi": "https://doi.org/10.1109/twc.2015.2408338",
                "title": "Power-Controlled Medium Access Control Protocol for Full-Duplex WiFi Networks",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1512.01642",
        "mid": "2192598490",
        "abstract": "Understanding human activity is very challenging even with the recently developed 3D depth sensors. To solve this problem, this work investigates a novel deep structured model, which adaptively decomposes an activity instance into temporal parts using the convolutional neural networks. Our model advances the traditional deep learning approaches in two aspects. First, we incorporate latent temporal structure into the deep model, accounting for large temporal variations of diverse human activities. In particular, we utilize the latent variables to decompose the input activity into a number of temporally segmented sub-activities, and accordingly feed them into the parts (i.e. sub-networks) of the deep architecture. Second, we incorporate a radius---margin bound as a regularization term into our deep model, which effectively improves the generalization performance for classification. For model training, we propose a principled learning algorithm that iteratively (i) discovers the optimal latent variables (i.e. the ways of activity decomposition) for all training instances, (ii) updates the classifiers based on the generated features, and (iii) updates the parameters of multi-layer neural networks. In the experiments, our approach is validated on several complex scenarios for human activity recognition and demonstrates superior performances over other state-of-the-art approaches.",
        "related_work": "In the mean time, the past few years have seen a resurgence of research in the design of deep neutral networks, and impressive progresses have been made on learning image features from raw data . To address human action recognition from videos, developed a novel deep architecture of convolutional networks, where they extracted features from both spatial and temporal dimensions. @cite_1 proposed to incorporate a new Switchable Restricted Boltzmann Machine (SRBM) to explicitly model the complex mixture of visual appearance for pedestrian detection, and train their model using an EM-type interative algorithm. Amer and Todorovic applied Sum Product Networks (SPNs) to model human activities based on variable primitive actions. Our deep model is partially motivated by these works, and we target on an more flexible and powerful solution by jointly considering the latent structure embedding, feature learning, and radius-margin classification.",
        "ref_abstract": {
            "@cite_1": {
                "mid": "2084997728",
                "abstract": "In this paper, we propose a Switchable Deep Network (SDN) for pedestrian detection. The SDN automatically learns hierarchical features, salience maps, and mixture representations of different body parts. Pedestrian detection faces the challenges of background clutter and large variations of pedestrian appearance due to pose and viewpoint changes and other factors. One of our key contributions is to propose a Switchable Restricted Boltzmann Machine (SRBM) to explicitly model the complex mixture of visual variations at multiple levels. At the feature levels, it automatically estimates saliency maps for each test sample in order to separate background clutters from discriminative regions for pedestrian detection. At the part and body levels, it is able to infer the most appropriate template for the mixture models of each part and the whole body. We have devised a new generative algorithm to effectively pretrain the SDN and then fine-tune it with back-propagation. Our approach is evaluated on the Caltech and ETH datasets and achieves the state-of-the-art detection performance.",
                "doi": "https://doi.org/10.1109/cvpr.2014.120",
                "title": "Switchable Deep Network for Pedestrian Detection",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1511.07710",
        "mid": "2277996466",
        "abstract": "To identify the location of objects of a particular class, a passive computer vision system generally processes all the regions in an image to finally output few regions. However, we can use structure in the scene to search for objects without processing the entire image. We propose a search technique that sequentially processes image regions such that the regions that are more likely to correspond to the query class object are explored earlier. We frame the problem as a Markov decision process and use an imitation learning algorithm to learn a search strategy. Since structure in the scene is essential for search, we work with indoor scene images as they contain both unary scene context information and object-object context in the scene. We perform experiments on the NYU-depth v2 dataset and show that the unary scene context features alone can achieve a significantly high average precision while processing only 20-25 of the regions for classes like bed and sofa. By considering object-object context along with the scene context features, the performance is further improved for classes like counter, lamp, pillow and sofa.",
        "related_work": "The idea of sequentially processing an image by exploiting structure is not just relevant to object localization. Sequential processing has also been explored for video event detection, where running a multitude of detectors at all spatio-temporal scales is very expensive. Amer al @cite_11 propose an explore-exploit strategy that schedules processes of top-down inference using activity context and bottom-up inference using activity parts. They use a Q-learning algorithm to learn the optimal actions to perform at a state. However, the learning algorithm needs the specification of a reward function which is difficult to obtain in many domains. We use an imitation learning algorithm that alleviates the problem of choosing a reward function.",
        "ref_abstract": {
            "@cite_11": {
                "mid": "2171544105",
                "abstract": "This paper addresses a new problem, that of multiscale activity recognition. Our goal is to detect and localize a wide range of activities, including individual actions and group activities, which may simultaneously co-occur in high-resolution video. The video resolution allows for digital zoom-in (or zoom-out) for examining fine details (or coarser scales), as needed for recognition. The key challenge is how to avoid running a multitude of detectors at all spatiotemporal scales, and yet arrive at a holistically consistent video interpretation. To this end, we use a three-layered AND-OR graph to jointly model group activities, individual actions, and participating objects. The AND-OR graph allows a principled formulation of efficient, cost-sensitive inference via an explore-exploit strategy. Our inference optimally schedules the following computational processes: 1) direct application of activity detectors --- called \u03b1 process; 2) bottom-up inference based on detecting activity parts --- called \u03b2 process; and 3) top-down inference based on detecting activity context --- called \u03b3 process. The scheduling iteratively maximizes the log-posteriors of the resulting parse graphs. For evaluation, we have compiled and benchmarked a new dataset of high-resolution videos of group and individual activities co-occurring in a courtyard of the UCLA campus.",
                "doi": "https://doi.org/10.1007/978-3-642-33765-9_14",
                "title": "Cost-Sensitive Top-Down/Bottom-Up Inference for Multiscale Activity Recognition",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1511.06522",
        "mid": "2279608790",
        "abstract": "We propose a method for integration of features extracted using deep representations of Convolutional Neural Networks (CNNs) each of which is learned using a different image dataset of objects and materials for material recognition. Given a set of representations of multiple pre-trained CNNs, we first compute activations of features using the representations on the images to select a set of samples which are best represented by the features. Then, we measure the uncertainty of the features by computing the entropy of class distributions for each sample set. Finally, we compute the contribution of each feature to representation of classes for feature selection and integration. We examine the proposed method on three benchmark datasets for material recognition. Experimental results show that the proposed method achieves state-of-the-art performance by integrating deep features. Additionally, we introduce a new material dataset called EFMD by extending Flickr Material Database (FMD). By the employment of the EFMD with transfer learning for updating the learned CNN models, we achieve 84.0 + -1.8 accuracy on the FMD dataset which is close to human performance that is 84.9 .",
        "related_work": "Surface properties of materials, and the relationship between perception of material and object categories are analyzed in @cite_5 @cite_17 . They first proposed a well-designed benchmark dataset called FMD. Then, they designed descriptors to extract hand-crafted features for representation of various surface properties such as color, texture and shape for material recognition in @cite_5 . Moreover, they analyzed the relationship between object and material recognition for accurate and fast perception of materials in @cite_17 .",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2006516328",
                "abstract": "Our world consists not only of objects and scenes but also of materials of various kinds. Being able to recognize the materials that surround us (e.g., plastic, glass, concrete) is important for humans as well as for computer vision systems. Unfortunately, materials have received little attention in the visual recognition literature, and very few computer vision systems have been designed specifically to recognize materials. In this paper, we present a system for recognizing material categories from single images. We propose a set of low and mid-level image features that are based on studies of human material recognition, and we combine these features using an SVM classifier. Our system outperforms a state-of-the-art system (Varma and Zisserman, TPAMI 31(11):2032\u20132047, 2009) on a challenging database of real-world material categories (, J Vis 9(8):784\u2013784a, 2009). When the performance of our system is compared directly to that of human observers, humans outperform our system quite easily. However, when we account for the local nature of our image features and the surface properties they measure (e.g., color, texture, local shape), our system rivals human performance. We suggest that future progress in material recognition will come from: (1) a deeper understanding of the role of non-local surface properties (e.g., extended highlights, object identity); and (2) efforts to model such non-local surface properties in images.",
                "doi": "https://doi.org/10.1007/s11263-013-0609-0",
                "title": "Recognizing Materials Using Perceptually Inspired Features",
                "publication_year": 2013
            },
            "@cite_17": {
                "mid": "2037131007",
                "abstract": "It is easy to visually distinguish a ceramic knife from one made of steel, a leather jacket from one made of denim, and a plush toy from one made of plastic. Most studies of material appearance have focused on the estimation of specific material properties such as albedo or surface gloss, and as a consequence, almost nothing is known about how we recognize material categories like leather or plastic. We have studied judgments of high-level material categories with a diverse set of real-world photographs, and we have shown (Sharan, 2009) that observers can categorize materials reliably and quickly. Performance on our tasks cannot be explained by simple differences in color, surface shape, or texture. Nor can the results be explained by observers merely performing shape-based object recognition. Rather, we argue that fast and accurate material categorization is a distinct, basic ability of the visual system.",
                "doi": "https://doi.org/10.1167/14.9.12",
                "title": "Accuracy and speed of material categorization in real-world images",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1511.05362",
        "mid": "2949318361",
        "abstract": "Kaczmarz algorithm is an efficient iterative algorithm to solve overdetermined consistent system of linear equations. During each updating step, Kaczmarz chooses a hyperplane based on an individual equation and projects the current estimate for the exact solution onto that space to get a new estimate. Many vairants of Kaczmarz algorithms are proposed on how to choose better hyperplanes. Using the property of randomly sampled data in high-dimensional space, we propose an accelerated algorithm based on clustering information to improve block Kaczmarz and Kaczmarz via Johnson-Lindenstrauss lemma. Additionally, we theoretically demonstrate convergence improvement on block Kaczmarz algorithm.",
        "related_work": "Our work mainly relies on RKA-JL algorithm @cite_9 and RKA-Block algorithm @cite_13 . We will review these algorithms below.",
        "ref_abstract": {
            "@cite_9": {
                "mid": "2036835849",
                "abstract": "The Kaczmarz method is an algorithm for finding the solution to an overdetermined consistent system of linear equations Ax?=?b by iteratively projecting onto the solution spaces. The randomized version put forth by Strohmer and Vershynin yields provably exponential convergence in expectation, which for highly overdetermined systems even outperforms the conjugate gradient method. In this article we present a modified version of the randomized Kaczmarz method which at each iteration selects the optimal projection from a randomly chosen set, which in most cases significantly improves the convergence rate. We utilize a Johnson---Lindenstrauss dimension reduction technique to keep the runtime on the same order as the original randomized version, adding only extra preprocessing time. We present a series of empirical studies which demonstrate the remarkable acceleration in convergence to the solution using this modified approach.",
                "doi": "https://doi.org/10.1007/s11075-011-9451-z",
                "title": "Acceleration of randomized Kaczmarz method via the Johnson\u2013Lindenstrauss Lemma",
                "publication_year": 2011
            },
            "@cite_13": {
                "mid": "1977769089",
                "abstract": "The block Kaczmarz method is an iterative scheme for solving overdetermined least-squares problems. At each step, the algorithm projects the current iterate onto the solution space of a subset of the constraints. This paper describes a block Kaczmarz algorithm that uses a randomized control scheme to choose the subset at each step. This algorithm is the first block Kaczmarz method with an (expected) linear rate of convergence that can be expressed in terms of the geometric properties of the matrix and its submatrices. The analysis reveals that the algorithm is most effective when it is given a good row paving of the matrix, a partition of the rows into well-conditioned blocks. The operator theory literature provides detailed information about the existence and construction of good row pavings. Together, these results yield an efficient block Kaczmarz scheme that applies to many overdetermined least-squares problem.",
                "doi": "https://doi.org/10.1016/j.laa.2012.12.022",
                "title": "Paved with good intentions: Analysis of a randomized block Kaczmarz method",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1511.04986",
        "mid": "2242657351",
        "abstract": "Finding repeated patterns or motifs in a time series is an important unsupervised task that has still a number of open issues, starting by the definition of motif. In this paper, we revise the notion of motif support, characterizing it as the number of patterns or repetitions that define a motif. We then propose GENMOTIF, a genetic algorithm to discover motifs with support which, at the same time, is flexible enough to accommodate other motif specifications and task characteristics. GENMOTIF is an anytime algorithm that easily adapts to many situations: searching in a range of segment lengths, applying uniform scaling, dealing with multiple dimensions, using different similarity and grouping criteria, etc. GENMOTIF is also parameter-friendly: it has only two intuitive parameters which, if set within reasonable bounds, do not substantially affect its performance. We demonstrate the value of our approach in a number of synthetic and real-world settings, considering traffic volume measurements, accelerometer signals, and telephone call records.",
        "related_work": "Other algorithms have been proposed for the task of time series subsequence clustering @cite_6 @cite_26 . Though this task and the one of finding motifs with support have common aspects, they are conceptually different. Time series subsequence clustering aims at obtaining a good and compact representation of the whole time series while, as shown in @cite_26 , ignoring some data. However, motif discovery aims at finding characteristic patterns (with support) that do not necessarily need to reliably and compactly represent the full time series. In fact, the segments used to derive the motifs can actually represent a tiny part of the time series (i.e., @math , as mentioned in the main text).",
        "ref_abstract": {
            "@cite_26": {
                "mid": "2083236658",
                "abstract": "Given the pervasiveness of time series data in all human endeavors, and the ubiquity of clustering as a data mining application, it is somewhat surprising that the problem of time series clustering from a single stream remains largely unsolved. Most work on time series clustering considers the clustering of individual time series, e.g., gene expression profiles, individual heartbeats or individual gait cycles. The few attempts at clustering time series streams have been shown to be objectively incorrect in some cases, and in other cases shown to work only on the most contrived datasets by carefully adjusting a large set of parameters. In this work, we make two fundamental contributions. First, we show that the problem definition for time series clustering from streams currently used is inherently flawed, and a new definition is necessary. Second, we show that the Minimum Description Length (MDL) framework offers an efficient, effective and essentially parameter-free method for time series clustering. We show that our method produces objectively correct results on a wide variety of datasets from medicine, zoology and industrial process analyses.",
                "doi": "https://doi.org/10.1109/icdm.2011.146",
                "title": "Time Series Epenthesis: Clustering Time Series Streams Requires Ignoring Some Data",
                "publication_year": 2011
            },
            "@cite_6": {
                "mid": "2049376434",
                "abstract": "In time series mining, one of the interesting tasks that attract many researchers is time series clustering which is classified into two main categories. Whole time series clustering considers how to cluster multiple time series, and the other one is Subsequence Time Series (STS) clustering, a clustering of subparts or subsequences within a single time series. Deplorably, STS clustering is not preferable even though it had widely been used as a subroutine in various mining tasks, e.g., rule discovery, anomaly detection, or classification, due to the recent finding a decade ago that STS clustering problem can produce meaningless results. There have been numerous attempts to resolve this problem but seemed to be unsuccessful. Until the two most recent attempts, they seem to accomplish in producing meaningful results; however, their approaches do need some predefined constraint values, such as the width of the subsequences that are in fact quite subjective and sensitive. Thus, we propose a novel parameter-free clustering technique to eliminate this problem by utilizing a motif discovery algorithm and some statistical principles to properly determine these parameters. Our experimental results from well-known datasets demonstrate the effectiveness of the proposed algorithm in selecting the proper subsequence width, and in turn leading to meaningful and highly accurate results.",
                "doi": "https://doi.org/10.1109/kst.2013.6512805",
                "title": "Parameter-free subsequences time series clustering with various-width clusters",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1511.01535",
        "mid": "2133749785",
        "abstract": "The focus of this paper is on the rate and power control algorithms in Dedicated Short Range Communication (DSRC) for vehicular networks. We first propose a utility maximization framework by leveraging the well-developed network congestion control, and formulate two subproblems, one on rate control with fixed transmit powers and the other on power control with fixed rates. Distributed rate control and power control algorithms are developed to solve these two subproblems, respectively, and are proved to be asymptotically optimal. Joint rate and power control can be done by using the two algorithms in an alternating fashion. The performance enhancement of our algorithms compared with a recent rate control algorithm, called EMBARC, is evaluated by using the network simulator ns2.",
        "related_work": "The design of rate and power control algorithms in DSRC is one of most critical problems in ITS. Error Model Based Adaptive Rate Control (EMBARC) @cite_15 is a recent rate control protocol which integrates several existing rate control algorithms including the Linear Integrated Message Rate Control (LIMERIC) @cite_16 , Periodically Updated Load Sensitive Adaptive Rate control (PULSAR) @cite_0 , and the InterVechicle Transmission Rate Control (IVTRC) @cite_19 . LIMERIC allocates the wireless channel equally among all vehicles that share the same bottleneck link while guaranteeing the channel load is below a given threshold. IVTRC generates messages and adapts transmission probabilities based on the Suspected Tracking Error (STE) calculated based on vehicle dynamics to avoid collisions. In EMBARC, the message rates are controlled by LIMERIC and are further modified to satisfy the STE requirement.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2002719705",
                "abstract": "Vehicle Safety Communications (VSC) is advancing rapidly towards product development and field testing. While a number of possible solutions have been proposed, the question remains open as how such a system will address the issue of scalability in its actual deployment. This paper presents a design methodology for congestion control in VSC as well as the description and evaluation of a resulting rate adaption oriented protocol named PULSAR. We start with a list of design principles reflecting the state of the art that define why and how vehicles should behave while responding to channel congestion in order to ensure fairness and support the needs of safety applications. From these principles, we derive protocol building blocks required to fulfill the defined objectives. Then, the actual protocol is described and assessed in detail, including a discussion on the intricate features of channel load assessment, rate adaptation and information sharing. A comparison with other state-of-the-art protocols shows that \u201cdetails matter\u201d with respect to the temporal and spatial dimensions of the protocol outcome.",
                "doi": "https://doi.org/10.1109/vnc.2011.6117132",
                "title": "Design methodology and evaluation of rate adaptation based congestion control for Vehicle Safety Communications",
                "publication_year": 2011
            },
            "@cite_19": {
                "mid": "2110426163",
                "abstract": "We propose an intervehicle communication framework for the cooperative active safety system (CASS) whose operation is based on the dissemination of each vehicle's state information through a wireless network. Such a CASS requires each subject vehicle to be aware of its surroundings, particularly of the motion and position of other vehicles in its proximity. In this paper, we assume that all vehicles are equipped with onboard communication devices. In such situations, the wireless channel is simultaneously shared by a large number of vehicles, and one of the most difficult challenges in designing CASS is to maintain real-time tracking accuracy of neighboring vehicles while avoiding network congestion and failure. To address this issue, we analyze the problem that multiple scalar linear time-invariant dynamical systems track each other over a multiaccess channel, and then, we propose a rate adaptation algorithm to distributively control the self-information broadcast behavior of each vehicle. The proposed algorithm uses a closed-loop control concept and accounts for the lossy channel. Simulation results show that, if the message generation rate is dynamically adjusted in an on-demand fashion, more accurate and robust tracking performance can be achieved under various traffic conditions.",
                "doi": "https://doi.org/10.1109/tits.2010.2070873",
                "title": "Intervehicle Transmission Rate Control for Cooperative Active Safety System",
                "publication_year": 2011
            },
            "@cite_15": {
                "mid": "2071515504",
                "abstract": "Channel congestion is one of the major challenges for deployment of collision avoidance systems based on DSRC (Dedicated Short Range Communication) in large scale networks. If vehicles do not adapt to congestion conditions, DSRC transmissions could encounter extensive packet losses in areas of high vehicle density, leading to degradation in the performance of safety applications. In this paper, we propose a novel congestion control algorithm called Error Model Based Adaptive Rate Control (EMBARC) which adapts a vehicle's transmission rate as a function of channel load and vehicular dynamics. In particular, we extend Linear Integrated Message Rate Control (LIMERIC) algorithm's message rate adaptation with the capability to preemptively schedule messages based on the vehicle's movement. This leads to more transmission opportunities for vehicles with higher dynamics. The determination of a preemptive scheduling event is based on a novel suspected tracking error technique. Since LIMERIC maintains the channel load around a specific value, vehicles moving less dynamically will adapt to slightly reduced transmission rates in EMBARC. The extra transmit opportunities for highly dynamic vehicles reduce incidences of large tracking error compared to a pure LIMERIC approach. At the same time, EMBARC's use of adaptive rate control provides tracking error advantages over systems that transmit largely independent of channel load. We use simulations of a road with a winding segment to compare EMBARC with algorithms that do not take both channel load and vehicle dynamics into account. The results show that EMBARC has the best tracking accuracy among these algorithms over a wide range of node densities.",
                "doi": "https://doi.org/10.1145/2482967.2482972",
                "title": "EMBARC",
                "publication_year": 2013
            },
            "@cite_16": {
                "mid": "2038940887",
                "abstract": "Wireless vehicle-to-vehicle (V2V) and vehicle-toinfrastructure (V2I) communication holds great promise for significantly reducing the human and financial costs of vehicle collisions. A common characteristic of this communication is the broadcast of a device's core state information at regular intervals (e.g., vehicle speed and location or traffic signal state and timing). Unless controlled, the aggregate of these broadcasts will congest the channel under dense traffic scenarios, reducing the effectiveness of collision avoidance applications that use transmitted information. Active congestion control using distributed techniques is a topic of great interest for establishing the scalability of this technology. This paper defines a new adaptive congestion control algorithm that can be applied to the message rate of devices in this vehicular environment. While other published approaches rely on binary control, the LInear MEssage Rate Integrated Control (LIMERIC) algorithm takes advantage of full-precision control inputs that are available on the wireless channel. The result is provable convergence to fair and efficient channel utilization in the deterministic environment, under simple criteria for setting adaptive parameters. This \u201cperfect\u201d convergence avoids the limit cycle behavior that is inherent to binary control. We also discuss several practical aspects associated with implementing LIMERIC, including guidelines for the choice of system parameters to obtain desired utilization outcomes, a gain saturation technique that maintains robust convergence under all conditions, convergence with asynchronous updates, and using channel load to determine the aggregate message rate that is observable at a receiver. This paper also extends the convergence analysis for two important cases, i.e., measurement noise in the input signal and delay in the update process. This paper illustrates key analytical results using MATLAB numerical results and employs standard NS-2 simulations to demonstrate the performance of LIMERIC in several high-density scenarios.",
                "doi": "https://doi.org/10.1109/tvt.2013.2275014",
                "title": "LIMERIC: A Linear Adaptive Message Rate Algorithm for DSRC Congestion Control",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1510.06142",
        "mid": "2951438219",
        "abstract": "Low-rank approximation of a matrix by means of random sampling has been consistently efficient in its empirical studies by many scientists who applied it with various sparse and structured multipliers, but adequate formal support for this empirical phenomenon has been missing so far. Our novel insight into the subject leads to such an elusive formal support and promises significant acceleration of the known algorithms for some fundamental problems of matrix computations and data mining and analysis. Our formal results and our numerical tests are in good accordance with each other. We also outline extensions of low-rank approximation algorithms and of our progress to the Least Squares Regression, the Fast Multipole Method, and the Conjugate Gradient algorithms.",
        "related_work": "Part (ii) of our Theorem is implied by [Theorem 10.8] HMT11 , but our specific supporting estimates are more compact, cover the case of any @math (whereas @cite_36 assumes that @math ), and we deduce them by using a shorter proof (see Remark ). Our approach, our results listed in Section , some of our techniques, e.g., expan -sion compres -sion in Section , and even the concept of factor-Gaussian matrices are new, and so are our families of multipliers and policies of their generation, combination, and application in Sections and as well. Moreover, our progress can be extended to a variety of important matrix computations. In Section (Conclusions) we outline such novel extensions to the highly popular and much studied computations for Least Squares Regression, the Fast Multipole Method and the Conjugate Gradient Algorithms. Hereafter we use the acronyms LSR\" for Least Squares Regression\", FMM\" for Fast Multipole Method\", and CG \" for Conjugate Gradient\". The extensions provide new insights and new opportunities and should motivate further effort and further progress.",
        "ref_abstract": {
            "@cite_36": {
                "mid": "2117756735",
                "abstract": "Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed\u2014either explicitly or implicitly\u2014to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the @math dominant components of the singular value decomposition of an @math matrix. (i) For a dense input matrix, randomized algorithms require @math floating-point operations (flops) in contrast to @math for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to @math passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data.",
                "doi": "https://doi.org/10.1137/090771806",
                "title": "Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1510.04031",
        "mid": "2208284149",
        "abstract": "In the last decade, the advertisement market spread significantly in the web and mobile app system. Its effectiveness is also due thanks to the possibility to target the advertisement on the specific interests of the actual user, other than on the content of the website hosting the advertisement. In this scenario, became of great value services that collect and hence can provide information about the browsing user, like Facebook and Google. In this paper, we show how to maliciously exploit the Google Targeted Advertising system to infer personal information in Google user profiles. In particular, the attack we consider is external from Google and relies on combining data from Google AdWords with other data collected from a website of the Google Display Network. We validate the effectiveness of our proposed attack, also discussing possible application scenarios. The result of our research shows a significant practical privacy issue behind such type of targeted advertising service, and call for further investigation and the design of more privacy-aware solutions, possibly without impeding the current business model involved in online advertisement.",
        "related_work": "In literature, several works deal with behavioral targeting in Online Ad Systems, highlighting their potential privacy threats @cite_13 . The novelty of the current work is that it presents and maliciously exploits a feature of the Google advertising system, which allows a remote attacker to infer user personal information, as interests and navigation behavior. In the following we review the main research works related to privacy issues and online advertisement.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "1847657293",
                "abstract": "The main goal of this chapter is to present a state-of the-art of behavioural tracking on the Internet and to highlight some of the resulting potential privacy threats. This chapter is structured as follows: First section introduces the concept of behavioural tracking. The following sections describe how tracking is performed by web sites, location-based services and social networks, respectively. Final section presents some of the existing tracking-prevention solutions. Finally, the author concludes the report and proposes some recommendations.",
                "doi": "https://doi.org/10.1007/978-94-007-2903-2_2",
                "title": "Behavioural Tracking on the Internet: A Technical Perspective",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1509.04186",
        "mid": "2221870240",
        "abstract": "We introduce an Expanded Parts Model (EPM) for recognizing human attributes (e.g., young, short hair, wearing suits) and actions (e.g., running, jumping) in still images. An EPM is a collection of part templates which are learnt discriminatively to explain specific scale-space regions in the images (in human centric coordinates). This is in contrast to current models which consist of a relatively few (i.e., a mixture of) \u2018average\u2019 templates. EPM uses only a subset of the parts to score an image and scores the image sparsely in space, i.e., it ignores redundant and random background in an image. To learn our model, we propose an algorithm which automatically mines parts and learns corresponding discriminative templates together with their respective locations from a large number of candidate parts. We validate our method on three recent challenging datasets of human attributes and actions. We obtain convincing qualitative and state-of-the-art quantitative results on the three datasets.",
        "related_work": "In the recently proposed Exemplar SVM (ESVM) work, Malisiewicz al @cite_90 propose to learn discriminative templates for each object instance of the training set independently and then combine their calibrated outputs on test images as a post-processing step. In contrast, we work at a part level and use all templates together during both training and testing. More recently, Yan al @cite_24 proposed a 2-level approach for image representation. Similar to our approach it involves sampling image regions, but while they vector quantize the region descriptors, we propose a mechanism to select discriminative regions and build discriminative part based models from them.",
        "ref_abstract": {
            "@cite_24": {
                "mid": "2147603200",
                "abstract": "We introduce a new framework for image classification that extends beyond the window sampling of fixed spatial pyramids to include a comprehensive set of windows densely sampled over location, size and aspect ratio. To effectively deal with this large set of windows, we derive a concise high-level image feature using a two-level extraction method. At the first level, window-based features are computed from local descriptors (e.g., SIFT, spatial HOG, LBP) in a process similar to standard feature extractors. Then at the second level, the new image feature is determined from the window-based features in a manner analogous to the first level. This higher level of abstraction offers both efficient handling of dense samples and reduced sensitivity to misalignment. More importantly, our simple yet effective framework can readily accommodate a large number of existing pooling coding methods, allowing them to extract features beyond the spatial pyramid representation. To effectively fuse the second level feature with a standard first level image feature for classification, we additionally propose a new learning algorithm, called Generalized Adaptive lp-norm Multiple Kernel Learning (GA-MKL), to learn an adapted robust classifier based on multiple base kernels constructed from image features and multiple sets of pre-learned classifiers of all the classes. Extensive evaluation on the object recognition (Caltech256) and scene recognition (15Scenes) benchmark datasets demonstrates that the proposed method outperforms state-of-the-art image classification algorithms under a broad range of settings.",
                "doi": "https://doi.org/10.1007/978-3-642-33765-9_34",
                "title": "Beyond Spatial Pyramids: A New Feature Extraction Framework with Dense Spatial Sampling for Image Classification",
                "publication_year": 2012
            },
            "@cite_90": {
                "mid": "1989684337",
                "abstract": "This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar-SVMs is thus defined by a single positive instance and millions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of , at only a modest computational cost increase. But the central benefit of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most detections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of overall scene understanding.",
                "doi": "https://doi.org/10.1109/iccv.2011.6126229",
                "title": "Ensemble of exemplar-SVMs for object detection and beyond",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1508.05044",
        "mid": "2949913501",
        "abstract": "CQA services are collaborative platforms where users ask and answer questions. We investigate the influence of national culture on people's online questioning and answering behavior. For this, we analyzed a sample of 200 thousand users in Yahoo Answers from 67 countries. We measure empirically a set of cultural metrics defined in Geert Hofstede's cultural dimensions and Robert Levine's Pace of Life and show that behavioral cultural differences exist in community question answering platforms. We find that national cultures differ in Yahoo Answers along a number of dimensions such as temporal predictability of activities, contribution-related behavioral patterns, privacy concerns, and power inequality.",
        "related_work": "Golder and Macy @cite_2 studied collective mood in Twitter across countries from 509 million Twitter posts by 2.4 million users over a 2-year period. Despite having different cultures, geographies, and religions, all countries (USA, Canada, UK, Australia, India, and English-speaking Africa) in their study showed similar mood rhythms---people tended to be more positive on weekends and early in the morning. @cite_0 examined the variation of Twitter users' emoticon usage patterns in cross cultures. They used Hofstede's national culture scores of 78 countries and found that collectivist cultures favor vertical and eye-oriented emoticons, where people within individualistic cultures favor horizontal and mouth-oriented emoticons. Hofstede's cultural dimensions have also been used to study whether culture of a country is associated with the way people use Twitter @cite_21 . In another study on cross-country Twitter communication, showed that cultural variables such as Hofstede's indices, language and intolerance have an impact on Twitter communication volume @cite_32 .",
        "ref_abstract": {
            "@cite_0": {
                "mid": "1748798774",
                "abstract": "Relying on Gudykunst's cultural variability in communication (CVC) framework and culture-specific facial expressions of emotion, we examined how people's use of emoticons varies cross-culturally. By merging emoticon usage patterns on Twitter with Hofstede's national culture scores and national indicators across 78 countries, this study found that people within individualistic cultures favor horizontal and mouth-oriented emoticons like :), while those within collectivistic cultures favor vertical and eye-oriented emoticons like ^_^. Our study serves to demonstrate how recent big data-driven approaches can be used to test research hypotheses in cross-cultural communication effectively from the methodological triangulation perspective. Implications and limitations regarding the findings of this study are also discussed.",
                "doi": "https://doi.org/10.1111/jcom.12086",
                "title": "Cross-Cultural Comparison of Nonverbal Cues in Emoticons on Twitter: Evidence from Big Data Analysis",
                "publication_year": 2014
            },
            "@cite_21": {
                "mid": "2282408813",
                "abstract": "Previous studies have established the link between one's actions (e.g., engaging with others vs. minding one's own business) and one's national culture (e.g., collectivist vs. individualistic), and such actions have been shown to be important as they are collectively affiliated with a country's economic outcomes (e.g., Gross Domestic Product). Hitherto there has not been any systematic study of whether one's action on Twitter (e.g., deciding when to post messages) is linked to one's culture (e.g., country's Pace of Life). To fix that, we build different network snapshots starting from 55,000 seed users on Twitter, and we do so for 10 weeks across 30 countries (after filtering those with low penetration rates) for a total of 2.34 M profiles. Based on Hofstede's theory of cultural dimensions and Levine's Pace of Life theory, we consider three behavioral patterns on Twitter (i.e., temporal predictability of tweets, engaging with others, and supporting others who are less popular) and associate them with three different dimensions derived from the two theories: Pace of Life, Individualism and Power Distance. We find the following strong correlations: activity predictability negatively correlates with Pace of Life (r=-0.62), tweets with mentions negatively correlates with Individualism (r = -0.55), and power (e.g, Twitter popularity) imbalance in relationships (between, for example, two users mentioning each other) is correlated with Power Distance (r=0.62). These three cultural dimensions matter because they are associated with a country's socio-economic aspects - with GDP per capita, income inequality, and education expenditure.",
                "doi": "https://doi.org/10.1609/icwsm.v7i1.14419",
                "title": "Cultural Dimensions in Twitter: Time, Individualism and Power",
                "publication_year": 2021
            },
            "@cite_32": {
                "mid": "2147934231",
                "abstract": "With the advent of Twitter and other lightweight social-networking services, one might think that it is easier than ever to maintain geographically dispersed, weaker social ties. By contrast, in this study we show that the international Twitter communication landscape is not only still largely predetermined by physical distance, but that it also depends on countries' social, economic, and cultural attributes. We describe a study of an international Twitter mention network of 13 million users across over 100 countries. We show that the Gravity Model, which hypothesizes that the flow between two areas is proportional to their masses (which we approximate using internet penetration) and inversely proportional to the distance between them, is correlated (r=0.68) with the international communication flow. Using this model, along with other social, economic, and cultural variables, we predict the communication volume at Adjusted R2 of @math , with trade, language and racial intolerance especially impacting communication. We discuss the implications of these barriers to communication in the contexts of collaborative work, software design, and recommendation systems.",
                "doi": "https://doi.org/10.1145/2531602.2531725",
                "title": "Twitter ain't without frontiers",
                "publication_year": 2014
            },
            "@cite_2": {
                "mid": "2008803468",
                "abstract": "We identified individual-level diurnal and seasonal mood rhythms in cultures across the globe, using data from millions of public Twitter messages. We found that individuals awaken in a good mood that deteriorates as the day progresses\u2014which is consistent with the effects of sleep and circadian rhythm\u2014and that seasonal change in baseline positive affect varies with change in daylength. People are happier on weekends, but the morning peak in positive affect is delayed by 2 hours, which suggests that people awaken later on weekends.",
                "doi": "https://doi.org/10.1126/science.1202775",
                "title": "Diurnal and Seasonal Mood Vary with Work, Sleep, and Daylength Across Diverse Cultures",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1507.08439",
        "mid": "2226205360",
        "abstract": "I present a hybrid matrix factorisation model representing users and items as linear combinations of their content features' latent factors. The model outperforms both collaborative and content-based models in cold-start or sparse interaction data scenarios (using both user and item metadata), and performs at least as well as a pure collaborative matrix factorisation model where interaction data is abundant. Additionally, feature embeddings produced by the model encode semantic information in a way reminiscent of word embedding approaches, making them useful for a range of related tasks such as tag recommendations.",
        "related_work": "Saveski @cite_12 perform joint factorisation of the user-item and item-feature matrices by using the same item latent feature matrix in both decompositions; the parameters are optimised by minimising a weighted sum of both matrices' reproduction loss functions. A weight hyperparameter governs the relative importance of accuracy in decomposing the collaborative and content matrices. A similar approach is used by McAuley @cite_4 for jointly modelling ratings and product reviews. Here, LightFM has the advantage of simplicity as its single optimisation objective is to factorise the user-item matrix.",
        "ref_abstract": {
            "@cite_4": {
                "mid": "2061873838",
                "abstract": "In order to recommend products to users we must ultimately predict how a user will respond to a new product. To do so we must uncover the implicit tastes of each user as well as the properties of each product. For example, in order to predict whether a user will enjoy Harry Potter, it helps to identify that the book is about wizards, as well as the user's level of interest in wizardry. User feedback is required to discover these latent product and user dimensions. Such feedback often comes in the form of a numeric rating accompanied by review text. However, traditional methods often discard review text, which makes user and product latent dimensions difficult to interpret, since they ignore the very text that justifies a user's rating. In this paper, we aim to combine latent rating dimensions (such as those of latent-factor recommender systems) with latent review topics (such as those learned by topic models like LDA). Our approach has several advantages. Firstly, we obtain highly interpretable textual labels for latent rating dimensions, which helps us to justify' ratings with text. Secondly, our approach more accurately predicts product ratings by harnessing the information present in review text; this is especially true for new products and users, who may have too few ratings to model their latent factors, yet may still provide substantial information from the text of even a single review. Thirdly, our discovered topics can be used to facilitate other tasks such as automated genre discovery, and to identify useful and representative reviews.",
                "doi": "https://doi.org/10.1145/2507157.2507163",
                "title": "Hidden factors and hidden topics",
                "publication_year": 2013
            },
            "@cite_12": {
                "mid": "2082927600",
                "abstract": "Recommender systems suggest to users items that they might like (e.g., news articles, songs, movies) and, in doing so, they help users deal with information overload and enjoy a personalized experience. One of the main problems of these systems is the item cold-start, i.e., when a new item is introduced in the system and no past information is available, then no effective recommendations can be produced. The item cold-start is a very common problem in practice: modern online platforms have hundreds of new items published every day. To address this problem, we propose to learn Local Collective Embeddings: a matrix factorization that exploits items' properties and past user preferences while enforcing the manifold structure exhibited by the collective embeddings. We present a learning algorithm based on multiplicative update rules that are efficient and easy to implement. The experimental results on two item cold-start use cases: news recommendation and email recipient recommendation, demonstrate the effectiveness of this approach and show that it significantly outperforms six state-of-the-art methods for item cold-start.",
                "doi": "https://doi.org/10.1145/2645710.2645751",
                "title": "Item cold-start recommendations",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1506.04940",
        "mid": "2274970128",
        "abstract": "Low-frequency words place a major challenge for automatic speech recognition (ASR). The probabilities of these words, which are often important name entities, are generally under-estimated by the language model (LM) due to their limited occurrences in the training data. Recently, we proposed a word-pair approach to deal with the problem, which borrows information of frequent words to enhance the probabilities of low-frequency words. This paper presents an extension to the word-pair method by involving multiple predicting words' to produce better estimation for low-frequency words. We also employ this approach to deal with out-of-language words in the task of multi-lingual speech recognition.",
        "related_work": "The work is an extension of the similar-pair method proposed in @cite_7 . In this approach, the probabilities of low-frequency words are enhanced and new words are supported by adding new FST transitions, both referring to the transitions of the similar and high-frequency words. Compared to the other approaches mentioned above, this method is more flexible, which supports any words instead of words limited in some pre-defined classes.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "1588936141",
                "abstract": "In practical automatic speech recognition (ASR) systems, it is difficult to recognize words that are with low-frequency in the language model (LM) training data. Ironically, these words tend to be highly important as they are often domain-specific name entities. In order to meet this challenge, we present a novel approach that enhances the weights of these words by borrowing information from some high-frequency words that are similar to the target words. Experimental results demonstrated that our method can significantly improve ASR performance on low-frequency words and does not impact performance on high-frequency words. Additionally, this method can be easily extended to deal with new words that are absent in the LM training data.",
                "doi": "https://doi.org/10.1109/chinasip.2015.7230421",
                "title": "Low-frequency word enhancement with similar pairs in speech recognition",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1506.04263",
        "mid": "1932549456",
        "abstract": "We study the necessity of predictive information in a class of queueing admission control problems, where a system manager is allowed to divert incoming jobs up to a fixed rate, in order to minimize the queueing delay experienced by the admitted jobs. (2014) show that the system's delay performance can be significantly improved by having access to future information in the form of a lookahead window, during which the times of future arrivals and services are revealed. They prove that, while delay under an optimal online policy diverges to infinity in the heavy-traffic regime, it can stay bounded by making use of future information. However, the diversion polices of (2014) require the length of the lookahead window to grow to infinity at a non-trivial rate in the heavy-traffic regime, and it remained open whether substantial performance improvement could still be achieved with less future information. We resolve this question to a large extent by establishing an asymptotically tight lower bound on how much future information is necessary to achieve superior performance, which matches the upper bound of (2014) up to a constant multiplicative factor. Our result hence demonstrates that the system's heavy-traffic delay performance is highly sensitive to the amount of future information available. Our proof is based on analyzing certain excursion probabilities of the input sample paths, and exploiting a connection between a policy's diversion decisions and subsequent server idling, which may be of independent interest for related dynamic resource allocation problems.",
        "related_work": "Unfortunately, these arguments employed in the Markov setting do not seem to carry over easily when the lookahead window is taken into account. While our setting can still be cast as an MDP by incorporating the lookahead window into the state space, the structure of the state space is now considerably more complex (and increasingly so, as @math ), and it is not so clear as to whether any monotonicity property continues to hold. Our proof techniques circumvent this complexity by focusing on the macroscopic'' sample-path characteristics of the system, instead of the more refined details of the Bellman equations. As a trade-off, our analysis is more coarse'' by nature, and it provides neither a characterization of the multiplicative in the delay scaling, nor a concrete diversion policy that achieves the lower bound of the necessary amount of future information (which, fortunately, has already been given in @cite_5 ).",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2104050194",
                "abstract": "We study an admissions control problem, where a queue with service rate @math receives incoming jobs at rate @math , and the decision maker is allowed to redirect away jobs up to a rate of @math , with the objective of minimizing the time-average queue length. We show that the amount of information about the future has a significant impact on system performance, in the heavy-traffic regime. When the future is unknown, the optimal average queue length diverges at rate @math , as @math . In sharp contrast, when all future arrival and service times are revealed beforehand, the optimal average queue length converges to a finite constant, @math , as @math . We further show that the finite limit of @math can be achieved using only a finite lookahead window starting from the current time frame, whose length scales as @math , as @math . This leads to the conjecture of an interesting duality between queuing delay and the amount of information about the future.",
                "doi": "https://doi.org/10.1214/13-aap973",
                "title": "Queuing with future information",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1505.05365",
        "mid": "269132092",
        "abstract": "The rise of smart applications has drawn interest to logical reasoning over data streams. Recently, different query languages and stream processing reasoning engines were proposed in different communities. However, due to a lack of theoretical foundations, the expressivity and semantics of these diverse approaches are given only informally. Towards clear specifications and means for analytic study, a formal framework is needed to define their semantics in precise terms. To this end, we present a first step towards an ideal semantics that allows for exact descriptions and comparisons of stream reasoning systems.",
        "related_work": "SECRET @cite_13 a model called SECRET is proposed to analyze the execution behavior of different stream processing engines (SPEs) from a practical point of view. The authors found that even the outcome of identical, simple queries vary significantly due to the different underlying processing models. There, the focus is on understanding, comparing and predicting the . In contrast, we want to provide means that allow for a similar analytical study for the of stream reasoning formalisms and engines. The two approaches are thus orthogonal and can be used together to compare stream reasoning engines based on different input feeding modes as well as different reasoning expressiveness.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "2129088288",
                "abstract": "There are many academic and commercial stream processing engines (SPEs) today, each of them with its own execution semantics. This variation may lead to seemingly inexplicable differences in query results. In this paper, we present SECRET, a model of the behavior of SPEs. SECRET is a descriptive model that allows users to analyze the behavior of systems and understand the results of window-based queries (with time- and tuple-based windows) for a broad range of heterogeneous SPEs. The model is the result of extensive analysis and experimentation with several commercial and academic engines. In the paper, we describe the types of heterogeneity found in existing engines and show with experiments on real systems that our model can explain the key differences in windowing behavior.",
                "doi": "https://doi.org/10.1007/s00778-012-0297-3",
                "title": "Modeling the execution semantics of stream processing engines with SECRET",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1504.04505",
        "mid": "1953440118",
        "abstract": "We study asymptotic behaviors of the free energy for the directed polymer in random environment. The polymer is allowed to make unbounded jumps and the environment is given by the Bernoulli variables. We first establish the existence and continuity including the negative infinity value of the coupling constant \u03b2. Our proof of existence at \u03b2=\u2212\u221e differs from existing ones in that it avoids the direct use of subadditivity. Secondly, we identify the asymptotics of the free energy at \u03b2=\u2212\u221e in the limit of the success probability of the Bernoulli variables tending to one. It is described by using the so-called time constant of a certain directed first passage percolation. Our proof relies on a certain continuity property of the time constant, which is of independent interest.",
        "related_work": "The main part of Theorem is the continuity of @math around @math , which is the zero temperature asymptotic result for the free energy. This type of problems does not seem to attract much interest in the discrete time setting since in some cases the answers are simple. For instance, consider the (nearest-neighbor) simple random walk model with an i.i.d. random environment with @math . Then it is easy to see that as @math , the free energy is asymptotic to @math times the time constant of the directed last passage percolation. However, if @math is Bernoulli distributed and we send @math , the situation is not so simple. As we mentioned at the beginning, the existence of @math proved in @cite_19 is already highly nontrivial and the continuity as @math remains an open question at the moment.",
        "ref_abstract": {
            "@cite_19": {
                "mid": "1508206940",
                "abstract": "We study the number @math of open paths of length @math in supercritical oriented percolation on @math , with @math . We prove that on the percolation event @math , @math almost surely converges to a positive deterministic constant. We also study the existence of directional limits. The proof relies on the introduction of adapted sequences of regenerating times, on subadditive arguments and on the properties of the coupled zone in supercritical oriented percolation.",
                "doi": "https://doi.org/10.1214/16-aop1158",
                "title": "The number of open paths in oriented percolation",
                "publication_year": 2017
            }
        }
    },
    {
        "aid": "1504.03306",
        "mid": "2952847610",
        "abstract": "Suppose we have a virus or one competing idea product that propagates over a multiple profile (e.g., social) network. Can we predict what proportion of the network will actually get \"infected\" (e.g., spread the idea or buy the competing product), when the nodes of the network appear to have different sensitivity based on their profile? For example, if there are two profiles @math and @math in a network and the nodes of profile @math and profile @math are susceptible to a highly spreading virus with probabilities @math and @math respectively, what percentage of both profiles will actually get infected from the virus at the end? To reverse the question, what are the necessary conditions so that a predefined percentage of the network is infected? We assume that nodes of different profiles can infect one another and we prove that under realistic conditions, apart from the weak profile (great sensitivity), the stronger profile (low sensitivity) will get infected as well. First, we focus on cliques with the goal to provide exact theoretical results as well as to get some intuition as to how a virus affects such a multiple profile network. Then, we move to the theoretical analysis of arbitrary networks. We provide bounds on certain properties of the network based on the probabilities of infection of each node in it when it reaches the steady state. Finally, we provide extensive experimental results that verify our theoretical results and at the same time provide more insight on the problem.",
        "related_work": "The majority of the presented models have studied a single epidemic in a single topology whereas in later work @cite_6 , multiple virus models are introduced. All of them require that the network is fair-play, which means that all nodes have exactly the same behavior towards the viruses. In @cite_6 , an @math model was used with two competing viruses that infect nodes of arbitrary topologies where the nodes are mutually immune. The main result is that for any topology the stronger virus (above threshold) survives and wipes out the weaker one (\"winner takes all\"). In later work, the condition for mutual immunity is removed and the focus is on the conditions where nodes are infected from both viruses @cite_12 . In both cases, the model used is SIS-like whereas in @cite_8 the authors provide a generalized model that includes the majority of known epidemiological models with appropriate parameter definition in discrete time. All the aforementioned models, are applied to simple networks whereas recent work @cite_20 , refers to competing viruses in a composite network and conjectures that the stronger one will prevail over the weaker. Once again, the problem is formulated by a non-linear dynamical system.",
        "ref_abstract": {
            "@cite_20": {
                "mid": "2102365618",
                "abstract": "If a false rumor propagates via Twitter, while the truth propagates between friends in Facebook, which one will prevail? This question captures the essence of the problem we address here. We study the intertwined propagation of two competing \"memes\" (or viruses, rumors, products etc.) in a composite network. A key novelty is the use of a composite network, which in its simplest model is defined as a single set of nodes with two distinct types of edges interconnecting them. Each meme spreads across the composite network in accordance to an SIS-like propagation model (a flu-like infection-recovery). To study the epidemic behavior of our system, we formulate it as a non-linear dynamic system (NLDS). We develop a metric for each meme that is based on the eigenvalue of an appropriately constructed matrix and argue that this metric plays a key role in determining the \"winning\" meme. First, we prove that our metric determines the tipping point at which both memes become extinct eventually. Second, we conjecture that the meme with the strongest metric will most likely prevail over the other, and we show evidence of that via simulations in both real and synthetic composite networks. Our work is among the first to study the interplay between two competing memes in composite networks.",
                "doi": "https://doi.org/10.1145/2378956.2378958",
                "title": "Competing memes propagation on networks",
                "publication_year": 2012
            },
            "@cite_12": {
                "mid": "2171031021",
                "abstract": "Suppose we have two competing ideas products viruses, that propagate over a social or other network. Suppose that they are strong virulent enough, so that each, if left alone, could lead to an epidemic. What will happen when both operate on the network? Earlier models assume that there is perfect competition: if a user buys product 'A' (or gets infected with virus 'X'), she will never buy product 'B' (or virus 'Y'). This is not always true: for example, a user could install and use both Firefox and Google Chrome as browsers. Similarly, one type of flu may give partial immunity against some other similar disease. In the case of full competition, it is known that 'winner takes all,' that is the weaker virus product will become extinct. In the case of no competition, both viruses survive, ignoring each other. What happens in-between these two extremes? We show that there is a phase transition: if the competition is harsher than a critical level, then 'winner takes all;' otherwise, the weaker virus survives. These are the contributions of this paper (a) the problem definition, which is novel even in epidemiology literature (b) the phase-transition result and (c) experiments on real data, illustrating the suitability of our results.",
                "doi": "https://doi.org/10.1145/2339530.2339601",
                "title": "Interacting viruses in networks",
                "publication_year": 2012
            },
            "@cite_6": {
                "mid": "2163595839",
                "abstract": "Given two competing products (or memes, or viruses etc.) spreading over a given network, can we predict what will happen at the end, that is, which product will 'win', in terms of highest market share? One may naively expect that the better product (stronger virus) will just have a larger footprint, proportional to the quality ratio of the products (or strength ratio of the viruses). However, we prove the surprising result that, under realistic conditions, for any graph topology, the stronger virus completely wipes-out the weaker one, thus not merely 'winning' but 'taking it all'. In addition to the proofs, we also demonstrate our result with simulations over diverse, real graph topologies, including the social-contact graph of the city of Portland OR (about 31 million edges and 1 million nodes) and internet AS router graphs. Finally, we also provide real data about competing products from Google-Insights, like Facebook-Myspace, and we show again that they agree with our analysis.",
                "doi": "https://doi.org/10.1145/2187836.2187975",
                "title": "Winner takes all",
                "publication_year": 2012
            },
            "@cite_8": {
                "mid": "2047311061",
                "abstract": "Given a network of who-contacts-whom or who-links-to-whom, will a contagious virus product meme spread and \u2018take over\u2019 (cause an epidemic) or die out quickly? What will change if nodes have partial, temporary or permanent immunity? The epidemic threshold is the minimum level of virulence to prevent a viral contagion from dying out quickly and determining it is a fundamental question in epidemiology and related areas. Most earlier work focuses either on special types of graphs or on specific epidemiological cascade models. We are the first to show the G2-threshold (twice generalized) theorem, which nicely de-couples the effect of the topology and the virus model. Our result unifies and includes as special case older results and shows that the threshold depends on the first eigenvalue of the connectivity matrix, (a) for any graph and (b) for all propagation models in standard literature (more than 25, including H.I.V.). Our discovery has broad implications for the vulnerability of real, complex networks and numerous applications, including viral marketing, blog dynamics, influence propagation, easy answers to \u2018what-if\u2019 questions, and simplified design and evaluation of immunization policies. We also demonstrate our result using extensive simulations on real networks, including on one of the biggest available social-contact graphs containing more than 31 million interactions among more than 1 million people representing the city of Portland, Oregon, USA.",
                "doi": "https://doi.org/10.1007/s10115-012-0520-y",
                "title": "Threshold conditions for arbitrary cascade models on arbitrary networks",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1504.01383",
        "mid": "2952602227",
        "abstract": "Given the extremely large pool of events and stories available, media outlets need to focus on a subset of issues and aspects to convey to their audience. Outlets are often accused of exhibiting a systematic bias in this selection process, with different outlets portraying different versions of reality. However, in the absence of objective measures and empirical evidence, the direction and extent of systematicity remains widely disputed. In this paper we propose a framework based on quoting patterns for quantifying and characterizing the degree to which media outlets exhibit systematic bias. We apply this framework to a massive dataset of news articles spanning the six years of Obama's presidency and all of his speeches, and reveal that a systematic pattern does indeed emerge from the outlet's quoting behavior. Moreover, we show that this pattern can be successfully exploited in an unsupervised prediction setting, to determine which new quotes an outlet will select to broadcast. By encoding bias patterns in a low-rank space we provide an analysis of the structure of political media coverage. This reveals a latent media bias space that aligns surprisingly well with political ideology and outlet type. A linguistic analysis exposes striking differences across these latent dimensions, showing how the different types of media outlets portray different realities even when reporting on the same events. For example, outlets mapped to the mainstream conservative side of the latent space focus on quotes that portray a presidential persona disproportionately characterized by negativity.",
        "related_work": "Media bias Our work relates to an extensive body of literature---spanning across political science, economics and communication studies---that gives theoretical and empirical accounts of media bias and its effects. We refer the reader to a recent comprehensive survey of media bias @cite_27 , and focus here on the studies that are most relevant to our approach.",
        "ref_abstract": {
            "@cite_27": {
                "mid": "2169020698",
                "abstract": "We review the burgeoning political economy literature on the influence of mass media on politics and policy. This survey, which covers both theory and empirics, is organized along four main themes: transparency, capture, informative coverage, and ideological bias. We distill some general lessons and identify some open questions.",
                "doi": "https://doi.org/10.1017/cbo9781139060028.004",
                "title": "The Political Economy of Mass Media",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1502.05680",
        "mid": "1692749124",
        "abstract": "We consider a random sparse graph with bounded average degree, in which a subset of vertices has higher connectivity than the background. In particular, the average degree inside this subset of vertices is larger than outside (but still bounded). Given a realization of such graph, we aim at identifying the hidden subset of vertices. This can be regarded as a model for the problem of finding a tightly knitted community in a social network, or a cluster in a relational dataset. In this paper we present two sets of contributions: (i) We use the cavity method from spin glass theory to derive an exact phase diagram for the reconstruction problem. In particular, as the difference in edge probability increases, the problem undergoes two phase transitions, a static phase transition and a dynamic one. (ii) We establish rigorous bounds on the dynamic phase transition and prove that, above a certain threshold, a local algorithm (belief propagation) correctly identify most of the hidden set. Below the same threshold no local algorithm can achieve this goal. However, in this regime the subset can be identified by exhaustive search. For small hidden sets and large average degree, the phase transition for local algorithms takes an intriguingly simple form. Local algorithms succeed with high probability for ( _ in - _ out > _ out e ) and fail for ( _ in - _ out < _ out e ) (with ( _ in ), ( _ out ) the average degrees inside and outside the community). We argue that spectral algorithms are also ineffective in the latter regime. It is an open problem whether any polynomial time algorithms might succeed for ( _ in - _ out < _ out e ).",
        "related_work": "The sparse graph regime studied in the present paper was also recently considered in a series of papers that analyzes community detection problems using ideas from statistical physics @cite_30 @cite_20 @cite_17 . The focus of these papers is on a setting whereby the graph @math contains @math non-overlapping communities, each of equal size @math . Using our notation, vertices within the same community are connected with probability @math and vertices belonging to different communities are connected with probability @math . Interestingly, the results of @cite_20 point at a similar phenomenon as the one studied here for @math . Namely, for a range of parameters the community structure can be identified by exhaustive search, but low complexity algorithms appear to fail.",
        "ref_abstract": {
            "@cite_30": {
                "mid": "1871641673",
                "abstract": "We present an asymptotically exact analysis of the problem of detecting communities in sparse random networks generated by stochastic block models. Using the cavity method of statistical physics and its relationship to belief propagation, we unveil a phase transition from a regime where we can infer the correct group assignments of the nodes to one where these groups are undetectable. Our approach yields an optimal inference algorithm for detecting modules, including both assortative and disassortative functional modules, assessing their significance, and learning the parameters of the underlying block model. Our algorithm is scalable and applicable to real-world networks, as long as they are well described by the block model.",
                "doi": "https://doi.org/10.1103/physrevlett.107.065701",
                "title": "Inference and Phase Transitions in the Detection of Modules in Sparse Networks",
                "publication_year": 2011
            },
            "@cite_20": {
                "mid": "2004531067",
                "abstract": "In this paper we extend our previous work on the stochastic block model, a commonly used generative model for social and biological networks, and the problem of inferring functional groups or communities from the topology of the network. We use the cavity method of statistical physics to obtain an asymptotically exact analysis of the phase diagram. We describe in detail properties of the detectability undetectability phase transition and the easy hard phase transition for the community detection problem. Our analysis translates naturally into a belief propagation algorithm for inferring the group memberships of the nodes in an optimal way, i.e., that maximizes the overlap with the underlying group memberships, and learning the underlying parameters of the block model. Finally, we apply the algorithm to two examples of real-world networks and discuss its performance.",
                "doi": "https://doi.org/10.1103/physreve.84.066106",
                "title": "Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications",
                "publication_year": 2011
            },
            "@cite_17": {
                "mid": "2024514015",
                "abstract": "Spectral algorithms are classic approaches to clustering and community detection in networks. However, for sparse networks the standard versions of these algorithms are suboptimal, in some cases completely failing to detect communities even when other algorithms such as belief propagation can do so. Here, we present a class of spectral algorithms based on a nonbacktracking walk on the directed edges of the graph. The spectrum of this operator is much better-behaved than that of the adjacency matrix or other commonly used matrices, maintaining a strong separation between the bulk eigenvalues and the eigenvalues relevant to community structure even in the sparse case. We show that our algorithm is optimal for graphs generated by the stochastic block model, detecting communities all of the way down to the theoretical limit. We also show the spectrum of the nonbacktracking operator for some real-world networks, illustrating its advantages over traditional spectral clustering.",
                "doi": "https://doi.org/10.1073/pnas.1312486110",
                "title": "Spectral redemption in clustering sparse networks",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1502.01823",
        "mid": "1517669128",
        "abstract": "In this paper we present an unsupervised method to learn the weights with which the scores of multiple classifiers must be combined in classifier fusion settings. We also introduce a novel metric for ranking instances based on an index which depends upon the rank of weighted scores of test points among the weighted scores of training points. We show that the optimized index can be used for computing measures such as average precision. Unlike most classifier fusion methods where a single weight is learned to weigh all examples our method learns instance-specific weights. The problem is formulated as learning the weight which maximizes a clarity index; subsequently the index itself and the learned weights both are used separately to rank all the test points. Our method gives an unsupervised method of optimizing performance on actual test data, unlike the well known stacking-based methods where optimization is done over a labeled training set. Moreover, we show that our method is tolerant to noisy classifiers and can be used for selecting N-best classifiers.",
        "related_work": "To the best of our knowledge, few recent works have actually looked into instance-specific weight learning @cite_10 @cite_13 . Some of the most promising results are reported in @cite_10 . The basic idea in this work is to propagate fusion weights of labeled instances to the individual unlabeled instances along a graph built on low-level features. The method has been shown to outperform other fusion methods on a variety of datasets. However, although the learned weights are instance specific, the method not only still requires a held-out set for which labels are known, it also requires knowledge of the low-level features of instances. On the other hand, our method does not require held-out data. Moreover, our solution is a meta algorithm that requires no knowledge of the low-level features of the instances. Another issue with @cite_10 is that the weights learned for different test instances are not disjoint from each other. This has the undesirable aspect that newer test instances cannot be independently introduced into the set.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "2024051019",
                "abstract": "In this paper, we propose a rank minimization method to fuse the predicted confidence scores of multiple models, each of which is obtained based on a certain kind of feature. Specifically, we convert each confidence score vector obtained from one model into a pairwise relationship matrix, in which each entry characterizes the comparative relationship of scores of two test samples. Our hypothesis is that the relative score relations are consistent among component models up to certain sparse deviations, despite the large variations that may exist in the absolute values of the raw scores. Then we formulate the score fusion problem as seeking a shared rank-2 pairwise relationship matrix based on which each original score matrix from individual model can be decomposed into the common rank-2 matrix and sparse deviation errors. A robust score vector is then extracted to fit the recovered low rank score relation matrix. We formulate the problem as a nuclear norm and l 1 norm optimization objective function and employ the Augmented Lagrange Multiplier (ALM) method for the optimization. Our method is isotonic (i.e., scale invariant) to the numeric scales of the scores originated from different models. We experimentally show that the proposed method achieves significant performance gains on various tasks including object categorization and video event detection.",
                "doi": "https://doi.org/10.1109/cvpr.2012.6248032",
                "title": "Robust late fusion with rank minimization",
                "publication_year": 2012
            },
            "@cite_10": {
                "mid": "2164507085",
                "abstract": "Late fusion addresses the problem of combining the prediction scores of multiple classifiers, in which each score is predicted by a classifier trained with a specific feature. However, the existing methods generally use a fixed fusion weight for all the scores of a classifier, and thus fail to optimally determine the fusion weight for the individual samples. In this paper, we propose a sample-specific late fusion method to address this issue. Specifically, we cast the problem into an information propagation process which propagates the fusion weights learned on the labeled samples to individual unlabeled samples, while enforcing that positive samples have higher fusion scores than negative samples. In this process, we identify the optimal fusion weights for each sample and push positive samples to top positions in the fusion score rank list. We formulate our problem as a L\u221e norm constrained optimization problem and apply the Alternating Direction Method of Multipliers for the optimization. Extensive experiment results on various visual categorization tasks show that the proposed method consistently and significantly beats the state-of-the-art late fusion methods. To the best knowledge, this is the first method supporting sample-specific fusion weight learning.",
                "doi": "https://doi.org/10.1109/cvpr.2013.109",
                "title": "Sample-Specific Late Fusion for Visual Category Recognition",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1501.00802",
        "mid": "88014687",
        "abstract": "Online Social Networks (OSNs) witness a rise in user activity whenever an event takes place. Malicious entities exploit this spur in user-engagement levels to spread malicious content that compromises system reputation and degrades user experience. It also generates revenue from advertisements, clicks, etc. for the malicious entities. Facebook, the world's biggest social network, is no exception and has recently been reported to face much abuse through scams and other type of malicious content, especially during news making events. Recent studies have reported that spammers earn $200 million just by posting malicious links on Facebook. In this paper, we characterize malicious content posted on Facebook during 17 events, and discover that existing efforts to counter malicious content by Facebook are not able to stop all malicious content from entering the social graph. Our findings revealed that malicious entities tend to post content through web and third party applications while legitimate entities prefer mobile platforms to post content. In addition, we discovered a substantial amount of malicious content generated by Facebook pages. Through our observations, we propose an extensive feature set based on entity profile, textual content, metadata, and URL features to identify malicious content on Facebook in real time and at zero-hour. This feature set was used to train multiple machine learning models and achieved an accuracy of 86.9 . The intent is to catch malicious content that is currently evading Facebook's detection techniques. Our machine learning model was able to detect more than double the number of malicious posts as compared to existing malicious content detection techniques. Finally, we built a real world solution in the form of a REST based API and a browser plug-in to identify malicious Facebook posts in real time.",
        "related_work": "Facebook has its own immune system @cite_3 to safeguard its users from unwanted malicious content. Researchers at Facebook built and deployed a coherent, scalable, and extensible real time system to protect their users and the social graph. This system performs real time checks and classifications on every read and write action. Designers of this complex system used an exhaustive set of components and techniques to differentiate between legitimate actions and spam. These components were standard classifiers like Random Forest, Support Vector Machines, Logistic Regression, a feature extraction language, dynamic model loading, a policy engine, and feature loops. Interestingly, despite this complex immune system deployed by Facebook, unwanted spam, phishing, and other malicious content continues to exist and thrive on Facebook. Although the immune system deployed by Facebook utilizes a variety of techniques to safeguard its users, authors did not present an evaluation of the system to suggest how accurately and efficiently the system is able to capture malicious content.",
        "ref_abstract": {
            "@cite_3": {
                "mid": "2156836300",
                "abstract": "Popular Internet sites are under attack all the time from phishers, fraudsters, and spammers. They aim to steal user information and expose users to unwanted spam. The attackers have vast resources at their disposal. They are well-funded, with full-time skilled labor, control over compromised and infected accounts, and access to global botnets. Protecting our users is a challenging adversarial learning problem with extreme scale and load requirements. Over the past several years we have built and deployed a coherent, scalable, and extensible realtime system to protect our users and the social graph. This Immune System performs realtime checks and classifications on every read and write action. As of March 2011, this is 25B checks per day, reaching 650K per second at peak. The system also generates signals for use as feedback in classifiers and other components. We believe this system has contributed to making Facebook the safest place on the Internet for people and their information. This paper outlines the design of the Facebook Immune System, the challenges we have faced and overcome, and the challenges we continue to face.",
                "doi": "https://doi.org/10.1145/1989656.1989664",
                "title": "Facebook immune system",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1412.3128",
        "mid": "2950988471",
        "abstract": "We present an accurate, real-time approach to robotic grasp detection based on convolutional neural networks. Our network performs single-stage regression to graspable bounding boxes without using standard sliding window or region proposal techniques. The model outperforms state-of-the-art approaches by 14 percentage points and runs at 13 frames per second on a GPU. Our network can simultaneously perform classification so that in a single step it recognizes the object and finds a good grasp rectangle. A modification to this model predicts multiple grasps per object by using a locally constrained prediction mechanism. The locally constrained model performs significantly better, especially on objects that can be grasped in a variety of ways.",
        "related_work": "Robotic systems increasingly leverage RGB-D sensors and data for tasks like object recognition @cite_15 , detection @cite_5 @cite_17 , and mapping @cite_20 @cite_13 . RGB-D sensors like the Kinect are cheap, and the extra depth information is invaluable for robots that interact with a 3-D environment.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2075274454",
                "abstract": "We propose a view-based approach for labeling objects in 3D scenes reconstructed from RGB-D (color+depth) videos. We utilize sliding window detectors trained from object views to assign class probabilities to pixels in every RGB-D frame. These probabilities are projected into the reconstructed 3D scene and integrated using a voxel representation. We perform efficient inference on a Markov Random Field over the voxels, combining cues from view-based detection and 3D shape, to label the scene. Our detection-based approach produces accurate scene labeling on the RGB-D Scenes Dataset and improves the robustness of object detection.",
                "doi": "https://doi.org/10.1109/icra.2012.6225316",
                "title": "Detection-based object labeling in 3D scenes",
                "publication_year": 2012
            },
            "@cite_15": {
                "mid": "2156222070",
                "abstract": "Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. In this paper, we introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology. This paper describes the dataset collection procedure and introduces techniques for RGB-D based object recognition and detection, demonstrating that combining color and depth information substantially improves quality of results.",
                "doi": "https://doi.org/10.1109/icra.2011.5980382",
                "title": "A large-scale hierarchical multi-view RGB-D object dataset",
                "publication_year": 2011
            },
            "@cite_13": {
                "mid": "2073660060",
                "abstract": "We present an approach to simultaneous localization and mapping (SLAM) for RGB-D cameras like the Microsoft Kinect. Our system concurrently estimates the trajectory of a hand-held Kinect and generates a dense 3D model of the environment. We present the key features of our approach and evaluate its performance thoroughly on a recently published dataset, including a large set of sequences of different scenes with varying camera speeds and illumination conditions. In particular, we evaluate the accuracy, robustness, and processing time for three different feature descriptors (SIFT, SURF, and ORB). The experiments demonstrate that our system can robustly deal with difficult data in common indoor scenarios while being fast enough for online operation. Our system is fully available as open-source.",
                "doi": "https://doi.org/10.1109/icra.2012.6225199",
                "title": "An evaluation of the RGB-D SLAM system",
                "publication_year": 2012
            },
            "@cite_20": {
                "mid": "1187244281",
                "abstract": "RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. In this paper we investigate how such cameras can be used in the context of robotics, specifically for building dense 3D maps of indoor environments. Such maps have applications in robot navigation, manipulation, semantic mapping, and telepresence. We present RGB-D Mapping, a full 3D mapping system that utilizes a novel joint optimization algorithm combining visual features and shape-based alignment. Visual and depth information are also combined for view-based loop closure detection, followed by pose optimization to achieve globally consistent maps.We evaluate RGB-D Mapping on two large indoor environments, and show that it effectively combines the visual and shape information available from RGB-D cameras.",
                "doi": "https://doi.org/10.1007/978-3-642-28572-1_33",
                "title": "RGB-D Mapping: Using Depth Cameras for Dense 3D Modeling of Indoor Environments",
                "publication_year": 2014
            },
            "@cite_17": {
                "mid": "2070961462",
                "abstract": "In this work we address the problem of feature extraction for object recognition in the context of cameras providing RGB and depth information (RGB-D data). We consider this problem in a bag of features like setting and propose a new, learned, local feature descriptor for RGB-D images, the convolutional k-means descriptor. The descriptor is based on recent results from the machine learning community. It automatically learns feature responses in the neighborhood of detected interest points and is able to combine all available information, such as color and depth into one, concise representation. To demonstrate the strength of this approach we show its applicability to different recognition problems. We evaluate the quality of the descriptor on the RGB-D Object Dataset where it is competitive with previously published results and propose an embedding into an image processing pipeline for object recognition and pose estimation.",
                "doi": "https://doi.org/10.1109/icra.2012.6225188",
                "title": "A learned feature descriptor for object recognition in RGB-D data",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1411.6114",
        "mid": "1921080889",
        "abstract": "With the advancement of Cloud Computing over the past few years, there has been a massive shift from traditional data centers to cloud enabled data centers. The enterprises with cloud data centers are focusing their attention on energy savings through effective utilization of resources. In this work, we propose algorithms which try to minimize the energy consumption in the data center duly maintaining the SLA guarantees. The algorithms try to utilize least number of physical machines in the data center by dynamically rebalancing the physical machines based on their resource utilization. The algorithms also perform an optimal consolidation of virtual machines on a physical machine, minimizing SLA violations. In extensive simulation, our algorithms achieve savings of about 21 in terms of energy consumption and in terms of maintaining the SLAs, it performs 60 better than Single Threshold algorithm.",
        "related_work": "Ching-Chi Lin et. al in @cite_9 presented an improved version of Round Robin algorithm used in Eucalyptus. According to Dynamic Round Robin algorithm, if a virtual machine has finished its execution and there are still other virtual machines running on the same physical machine, this physical machine will not accept any new virtual machine requests. Such physical machines are referred to as being in retirement' state, meaning that after the execution of the remaining virtual machines, this physical machine could be shutdown. And if a physical machine is in the retirement' state for a sufficiently long period of time, the currently running virtual machines are forced to migrate on to other physical machines and shutdown after the migration operation is finished. This waiting time threshold is denoted as retirement threshold'. So, a physical machine which is in the retirement state beyond this threshold will be forced to migrate its virtual machines and shutdown.",
        "ref_abstract": {
            "@cite_9": {
                "mid": "2122618394",
                "abstract": "Power consumption is one of the most critical problems in data centers. One effective way to reduce power consumption is to consolidate the hosting workloads and shut down physical machines which become idle after consolidation. Server consolidation is a NP-hard problem. In this paper, a new algorithms Dynamic Round-Robin (DRR), is proposed for energy-aware virtual machine scheduling and consolidation. We compare this strategy with the GREEDY, ROUNDROBIN and POWERSAVE scheduling strategies implemented in the Eucalyptus Cloud system. Our experiment results show that the Dynamic Round-Robin algorithm reduce a significant amount of power consumption compared with the three strategies in Eucalyptus.",
                "doi": "https://doi.org/10.1109/cloud.2011.94",
                "title": "Energy-Aware Virtual Machine Dynamic Provision and Scheduling for Cloud Computing",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1411.1076",
        "mid": "2164071215",
        "abstract": "We consider the Principal Component Analysis problem for large tensors of arbitrary order @math under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio @math becomes larger than @math (and in particular @math can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.",
        "related_work": "As mentioned above, power iteration is a natural approach to tensor factorization and was studied in several earlier papers. Most recently, interest within machine learning was spurred by @cite_22 .",
        "ref_abstract": {
            "@cite_22": {
                "mid": "2950741027",
                "abstract": "This work considers a computationally and statistically efficient parameter estimation method for a wide class of latent variable models---including Gaussian mixture models, hidden Markov models, and latent Dirichlet allocation---which exploits a certain tensor structure in their low-order observable moments (typically, of second- and third-order). Specifically, parameter estimation is reduced to the problem of extracting a certain (orthogonal) decomposition of a symmetric tensor derived from the moments; this decomposition can be viewed as a natural generalization of the singular value decomposition for matrices. Although tensor decompositions are generally intractable to compute, the decomposition of these specially structured tensors can be efficiently obtained by a variety of approaches, including power iterations and maximization approaches (similar to the case of matrices). A detailed analysis of a robust tensor power method is provided, establishing an analogue of Wedin's perturbation theorem for the singular vectors of matrices. This implies a robust and computationally tractable estimation approach for several popular latent variable models.",
                "doi": "https://doi.org/10.21236/ada604494",
                "title": "Tensor Decompositions for Learning Latent Variable Models",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1410.7270",
        "mid": "1626106296",
        "abstract": "Our traditional notion of a cell is changing dramatically given the increasing degree of heterogeneity in 4G and emerging 5G systems. Rather than belonging to a specific cell, a device would choose the most suitable connection from the plethora of connections available. In such a setting, given the transmission powers differ significantly between downlink (DL) and uplink (UL), a wireless device that sees multiple Base Stations (BSs) may access the infrastructure in a way that it receives the downlink (DL) traffic from one BS and sends uplink (UL) traffic through another BS. This situation is referred to as Downlink and Uplink Decoupling (DUDe). In this paper, the capacity and throughput gains brought by decoupling are rigorously derived using stochastic geometry. Theoretical findings are then corroborated by means of simulation results. A further constituent of this paper is the verification of the theoretically derived results by means of a real-world system simulation platform. Despite theoretical assumptions differing from the very complete system simulator, the trends in the association probabilities and capacity gains are similar. Based on the promising results, we then outline architectural changes needed to facilitate the decoupling of DL and UL.",
        "related_work": "The loosening and decoupling of the DL and UL association has been indicated in @cite_2 , and has been studied in a few selected pioneering contributions @cite_15 @cite_21 @cite_3 @cite_9 .",
        "ref_abstract": {
            "@cite_9": {
                "mid": "2103972037",
                "abstract": "New research directions will lead to fundamental changes in the design of future fifth generation (5G) cellular networks. This article describes five technologies that could lead to both architectural and component disruptive design changes: device-centric architectures, millimeter wave, massive MIMO, smarter devices, and native support for machine-to-machine communications. The key ideas for each technology are described, along with their potential impact on 5G and the research challenges that remain.",
                "doi": "https://doi.org/10.1109/mcom.2014.6736746",
                "title": "Five disruptive technology directions for 5G",
                "publication_year": 2014
            },
            "@cite_21": {
                "mid": "2007996888",
                "abstract": "Wireless cellular networks evolve towards a heterogeneous infrastructure, featuring multiple types of Base Stations (BSs), such as Femto BSs (FBSs) and Macro BSs (MBSs). A wireless device observes multiple points (BSs) through which it can access the infrastructure and it may choose to receive the downlink (DL) traffic from one BS and send uplink (UL) traffic through another BS. Such a situation is referred to as decoupled DL UL access . Using the framework of stochastic geometry, we derive the association probability for DL UL. To maximize the average received power, as the relative density of FBSs initially increases, a large fraction of devices chooses decoupled access, i.e., receive from a MBS in DL and transmit through a FBS in UL. We analyze the impact that this type of association has on the average throughput in the system.",
                "doi": "https://doi.org/10.1109/lwc.2015.2388676",
                "title": "Analysis of the Decoupled Access for Downlink and Uplink in Wireless Heterogeneous Networks",
                "publication_year": 2015
            },
            "@cite_3": {
                "mid": "2171373667",
                "abstract": "As the specification of Release 11 of the LTE standards is approaching its completion, 3GPP is gradually moving its focus toward the next major step in the evolution of LTE. The drivers of the LTE evolution include the increasing demand for mobile broadband services and traffic volumes as well as emerging usage scenarios involving short-range and machine-type communications. In this article we provide an overview of the key technology areas components that are currently considered by 3GPP for Rel-12, including support for further enhanced local area access by tight interaction between the wide area and local area layers, signaling solutions for wireless local area network integration, multi-antenna enhancements, improved support for massive MTC, and direct device-to-device communications.",
                "doi": "https://doi.org/10.1109/mcom.2013.6553692",
                "title": "LTE release 12 and beyond [Accepted From Open Call]",
                "publication_year": 2013
            },
            "@cite_2": {
                "mid": "1994080576",
                "abstract": "Imagine a world with more base stations than cell phones: this is where cellular technology is headed in 10-20 years. This mega-trend requires many fundamental differences in visualizing, modeling, analyzing, simulating, and designing cellular networks vs. the current textbook approach. In this article, the most important shifts are distilled down to seven key factors, with the implications described and new models and techniques proposed for some, while others are ripe areas for future exploration.",
                "doi": "https://doi.org/10.1109/mcom.2013.6476878",
                "title": "Seven ways that HetNets are a cellular paradigm shift",
                "publication_year": 2013
            },
            "@cite_15": {
                "mid": "2063348098",
                "abstract": "Cell association in cellular networks has traditionally been based on the downlink received signal power only, despite the fact that uplink and downlink transmission powers and interference levels differed significantly. This approach was adequate in homogeneous networks with macro base stations all having similar transmission power levels. However, with the growth of heterogeneous networks where there is a big disparity in the transmit power of the different base station types, this approach is highly inefficient. In this paper, we study the notion of Downlink and Uplink Decoupling (DUDe) where the downlink cell association is based on the downlink received power while the uplink is based on the pathloss. We present the motivation and assess the gains of this 5G design approach with simulations that are based on Vodafone's LTE field trial network in a dense urban area, employing a high resolution ray-tracing pathloss prediction and realistic traffic maps based on live network measurements.",
                "doi": "https://doi.org/10.1109/glocom.2014.7037069",
                "title": "Downlink and Uplink Decoupling: A disruptive architectural design for 5G networks",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1409.1284",
        "mid": "2950295006",
        "abstract": "We propose an approach to index raster images of dictionary pages which in turn would require very little manual effort to enable direct access to the appropriate pages of the dictionary for lookup. Accessibility is further improved by feedback and crowdsourcing that enables highlighting of the specific location on the page where the lookup word is found, annotation, digitization, and fielded searching. This approach is equally applicable on simple scripts as well as complex writing systems. Using our proposed approach, we have built a Web application called \"Dictionary Explorer\" which supports word indexes in various languages and every language can have multiple dictionaries associated with it. Word lookup gives direct access to appropriate pages of all the dictionaries of that language simultaneously. The application has exploration features like searching, pagination, and navigating the word index through a tree-like interface. The application also supports feedback, annotation, and digitization features. Apart from the scanned images, \"Dictionary Explorer\" aggregates results from various sources and user contributions in Unicode. We have evaluated the time required for indexing dictionaries of different sizes and complexities in the Urdu language and examined various trade-offs in our implementation. Using our approach, a single person can make a dictionary of 1,000 pages searchable in less than an hour.",
        "related_work": "HathiTrust @cite_48 is a collaborative repository of digital content from various research institutions and libraries to preserve cultural records and to ensure accessibility and availability of the preserved content in future.",
        "ref_abstract": {
            "@cite_48": {
                "mid": "2106203112",
                "abstract": "Academic libraries are increasingly looking to provide services that allow their users to work with digital collections in innovative ways, for example, to analyze large volumes of digitized collections. The HathiTrust Research Center (HTRC) is a large collaborative that provides an innovative research infrastructure for dealing with massive amounts of digital texts. In this poster, we report on the technical progress of the HTRC as well as on the efforts to build a user community around our cyberinfrastructure.",
                "doi": "https://doi.org/10.1145/2467696.2467767",
                "title": "HathiTrust research center",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1409.0347",
        "mid": "2022000911",
        "abstract": "Many tensor-based data completion methods aim to solve image and video in-painting problems. But, all methods were only developed for a single dataset. In most of real applications, we can usually obtain more than one dataset to reflect one phenomenon, and all the datasets are mutually related in some sense. Thus one question raised whether such the relationship can improve the performance of data completion or not? In the paper, we proposed a novel and efficient method by exploiting the relationship among datasets for multi-video data completion. Numerical results show that the proposed method significantly improve the performance of video in-painting, particularly in the case of very high missing percentage.",
        "related_work": "Many tensor based methods for completion have been developed @cite_5 @cite_7 @cite_9 @cite_3 @cite_8 , where @cite_5 and @cite_8 are based on tensor decomposition to predict missing values while @cite_7 @cite_9 @cite_3 minimize nuclear norm of a tensor directly. But all of them just focus on a single tensor data. It is worthwhile to notice that TMac proposed in @cite_3 used a similar optimization method to our method. However, we use the Frobenius norm regularization on factor matrices to control the rank of unfolding of tensors but TMac only justifies the number of column of latent matrices to control the nuclear norm.",
        "ref_abstract": {
            "@cite_7": {
                "mid": "2091449379",
                "abstract": "In this paper, we propose an algorithm to estimate missing values in tensors of visual data. The values can be missing due to problems in the acquisition process or because the user manually identified unwanted outliers. Our algorithm works even with a small amount of samples and it can propagate structure to fill larger missing regions. Our methodology is built on recent studies about matrix completion using the matrix trace norm. The contribution of our paper is to extend the matrix case to the tensor case by proposing the first definition of the trace norm for tensors and then by building a working algorithm. First, we propose a definition for the tensor trace norm that generalizes the established definition of the matrix trace norm. Second, similarly to matrix completion, the tensor completion is formulated as a convex optimization problem. Unfortunately, the straightforward problem extension is significantly harder to solve than the matrix case because of the dependency among multiple constraints. To tackle this problem, we developed three algorithms: simple low rank tensor completion (SiLRTC), fast low rank tensor completion (FaLRTC), and high accuracy low rank tensor completion (HaLRTC). The SiLRTC algorithm is simple to implement and employs a relaxation technique to separate the dependant relationships and uses the block coordinate descent (BCD) method to achieve a globally optimal solution; the FaLRTC algorithm utilizes a smoothing scheme to transform the original nonsmooth problem into a smooth one and can be used to solve a general tensor trace norm minimization problem; the HaLRTC algorithm applies the alternating direction method of multipliers (ADMMs) to our problem. Our experiments show potential applications of our algorithms and the quantitative evaluation indicates that our methods are more accurate and robust than heuristic approaches. The efficiency comparison indicates that FaLTRC and HaLRTC are more efficient than SiLRTC and between FaLRTC and HaLRTC the former is more efficient to obtain a low accuracy solution and the latter is preferred if a high-accuracy solution is desired.",
                "doi": "https://doi.org/10.1109/tpami.2012.39",
                "title": "Tensor Completion for Estimating Missing Values in Visual Data",
                "publication_year": 2013
            },
            "@cite_8": {
                "mid": "2147512299",
                "abstract": "CANDECOMP PARAFAC (CP) tensor factorization of incomplete data is a powerful technique for tensor completion through explicitly capturing the multilinear latent factors. The existing CP algorithms require the tensor rank to be manually specified, however, the determination of tensor rank remains a challenging problem especially for CP rank . In addition, existing approaches do not take into account uncertainty information of latent factors, as well as missing entries. To address these issues, we formulate CP factorization using a hierarchical probabilistic model and employ a fully Bayesian treatment by incorporating a sparsity-inducing prior over multiple latent factors and the appropriate hyperpriors over all hyperparameters, resulting in automatic rank determination. To learn the model, we develop an efficient deterministic Bayesian inference algorithm, which scales linearly with data size. Our method is characterized as a tuning parameter-free approach, which can effectively infer underlying multilinear factors with a low-rank constraint, while also providing predictive distributions over missing entries. Extensive simulations on synthetic data illustrate the intrinsic capability of our method to recover the ground-truth of CP rank and prevent the overfitting problem, even when a large amount of entries are missing. Moreover, the results from real-world applications, including image inpainting and facial image synthesis, demonstrate that our method outperforms state-of-the-art approaches for both tensor factorization and tensor completion in terms of predictive performance.",
                "doi": "https://doi.org/10.1109/tpami.2015.2392756",
                "title": "Bayesian CP Factorization of Incomplete Tensors with Automatic Rank Determination",
                "publication_year": 2015
            },
            "@cite_9": {
                "mid": "2030628896",
                "abstract": "We present a framework based on convex optimization and spectral regularization to perform learning when feature observations are multidimensional arrays (tensors). We give a mathematical characterization of spectral penalties for tensors and analyze a unifying class of convex optimization problems for which we present a provably convergent and scalable template algorithm. We then specialize this class of problems to perform learning both in a transductive as well as in an inductive setting. In the transductive case one has an input data tensor with missing features and, possibly, a partially observed matrix of labels. The goal is to both infer the missing input features as well as predict the missing labels. For induction, the goal is to determine a model for each learning task to be used for out of sample prediction. Each training pair consists of a multidimensional array and a set of labels each of which corresponding to related but distinct tasks. In either case the proposed technique exploits precise low multilinear rank assumptions over unknown multidimensional arrays; regularization is based on composite spectral penalties and connects to the concept of Multilinear Singular Value Decomposition. As a by-product of using a tensor-based formalism, our approach allows one to tackle the multi-task case in a natural way. Empirical studies demonstrate the merits of the proposed methods.",
                "doi": "https://doi.org/10.1007/s10994-013-5366-3",
                "title": "Learning with tensors: a framework based on convex optimization and spectral regularization",
                "publication_year": 2013
            },
            "@cite_3": {
                "mid": "2038891887",
                "abstract": "Higher-order low-rank tensors naturally arise in many applications including hyperspectral data recovery, video inpainting, seismic data reconstruction, and so on. We propose a new model to recover a low-rank tensor by simultaneously performing low-rank matrix factorizations to the all-mode matricizations of the underlying tensor. An alternating minimization algorithm is applied to solve the model, along with two adaptive rank-adjusting strategies when the exact rank is not known. &nbsp Phase transition plots reveal that our algorithm can recover a variety of synthetic low-rank tensors from significantly fewer samples than the compared methods, which include a matrix completion method applied to tensor recovery and two state-of-the-art tensor completion methods. Further tests on real-world data show similar advantages. Although our model is non-convex, our algorithm performs consistently throughout the tests and gives better results than the compared methods, some of which are based on convex models. In addition, subsequence convergence of our algorithm can be established in the sense that any limit point of the iterates satisfies the KKT condtions.",
                "doi": "https://doi.org/10.3934/ipi.2015.9.601",
                "title": "Parallel matrix factorization for low-rank tensor completion",
                "publication_year": 2015
            },
            "@cite_5": {
                "mid": "1814521481",
                "abstract": "Abstract The problem of incomplete data \u2013 i.e., data with missing or unknown values \u2013 in multi-way arrays is ubiquitous in biomedical signal processing, network traffic analysis, bibliometrics, social network analysis, chemometrics, computer vision, communication networks, etc. We consider the problem of how to factorize data sets with missing values with the goal of capturing the underlying latent structure of the data and possibly reconstructing missing values (i.e., tensor completion). We focus on one of the most well-known tensor factorizations that captures multi-linear structure, CANDECOMP PARAFAC (CP). In the presence of missing data, CP can be formulated as a weighted least squares problem that models only the known entries. We develop an algorithm called CP-WOPT (CP Weighted OPTimization) that uses a first-order optimization approach to solve the weighted least squares problem. Based on extensive numerical experiments, our algorithm is shown to successfully factorize tensors with noise and up to 99 missing data. A unique aspect of our approach is that it scales to sparse large-scale data, e.g., 1000 \u00d7 1000 \u00d7 1000 with five million known entries (0.5 dense). We further demonstrate the usefulness of CP-WOPT on two real-world applications: a novel EEG (electroencephalogram) application where missing data is frequently encountered due to disconnections of electrodes and the problem of modeling computer network traffic where data may be absent due to the expense of the data collection process.",
                "doi": "https://doi.org/10.1016/j.chemolab.2010.08.004",
                "title": "Scalable tensor factorizations for incomplete data",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1408.2154",
        "mid": "2950947447",
        "abstract": "Let @math be a simple connected graph and @math an ordered subset of vertices. The metric representation of a vertex @math with respect to @math is the @math -vector @math , where @math represents the length of a shortest @math path in @math . The set @math is called a resolving set for @math if @math implies @math for every @math . The smallest cardinality of a resolving set is the metric dimension of @math . In this article we propose, to the best of our knowledge, a new problem in Graph Theory that resembles to the aforementioned metric dimension problem. We call @math a @math -antiresolving set if @math is the largest positive integer such that for every vertex @math there exist other @math different vertices @math with @math , , @math and @math have the same metric representation with respect to @math . The @math -metric antidimension of @math is the minimum cardinality among all the @math -antiresolving sets for @math . In this article, we introduce a novel privacy measure, named @math -anonymity and based on the @math -metric antidimension problem, aimed at evaluating the resistance of social graphs to active attacks. We, therefore, propose a true-biased algorithm for computing the @math -metric antidimension of random graphs. The success rate of our algorithm, according to empirical results, is above @math and @math when looking for a @math -antiresolving basis and a @math -antiresolving set respectively. We also investigate theoretical properties of the @math -antiresolving sets and the @math -metric antidimension of graphs. In particular, we focus on paths, cycles, complete bipartite graphs and trees.",
        "related_work": "Other types of active attacks exist. For instance, the maximal vertex coverage (MVC) attack consists in attacking a few nodes so as to delete as many edges of the network as possible. In this attack, the attacker tries to convince some users to leave the social network in order to reduce the number of residual social ties. Metrics to quantify the impact of MVC attacks have been studied in @cite_20 . MVC is not a privacy attack, though.",
        "ref_abstract": {
            "@cite_20": {
                "mid": "2137313157",
                "abstract": "Measuring the impact of network attack is an important issue in network science. In this paper, we study the impact of maximal vertex coverage (MVC) attack in large complex networks, where the attacker aims at deleting as many edges of the network as possible by attacking a small fraction of nodes. First, we present two metrics to measure the impact of MVC attack. To compute these metrics, we propose an efficient randomized greedy algorithm with near-optimal performance guarantee. Second, we generalize the MVC attack into an uncertain setting, in which a node is deleted by the attacker with a prior probability. We refer to the MVC attack under such uncertain environment as the probabilistic MVC attack. Based on the probabilistic MVC attack, we propose two adaptive metrics, and then present an adaptive greedy algorithm for calculating such metrics accurately and efficiently. Finally, we conduct extensive experiments on 20 real datasets. The results show that P2P and co-authorship networks are extremely robust under the MVC attack while both the online social networks and the Email communication networks exhibit vulnerability under the MVC attack. In addition, the results demonstrate the efficiency and effectiveness of the proposed algorithms for computing the proposed metrics.",
                "doi": "https://doi.org/10.1016/j.ins.2014.03.085",
                "title": "Measuring the impact of MVC attack in large complex networks",
                "publication_year": 2014
            }
        }
    },
    {
        "aid": "1407.0423",
        "mid": "1951791596",
        "abstract": "Since the beginning of the digital area, privacy and anonymity have been impacted drastically (both, positively and negatively), by the different technologies developed for communications purposes. The broad possibilities that the Internet offers since its conception, makes it a mandatory target for those entities that are aiming to know and control the different channels of communication and the information that flows through. In this paper, we address the current threats against privacy and anonymity on the Internet, together with the methods applied against them. In addition, we enumerate the publicly known entities behind those threats and their motivations. Finally, we analyze the state of the art concerning the protection of the privacy and anonymity on the Internet; introducing future lines of research.",
        "related_work": "As of today, there is still no single solution available that is complaint with the definitions of privacy and anonymity introduced in Section 1. Nowadays, every communication done through the Internet that aims to be private and anonymous requires the use of cryptography and hard-trace routing techniques. Proxies and TOR are among the most used anonymity technologies @cite_13 and public-key cryptography is the most popular method aiming for privacy. Nevertheless, it is still possible to perform different attacks over these techniques, leading always to some result that invalidate the required properties for privacy and anonymity mentioned in Section 1. We introduce some of the possible improvements that current lines of research are proposing in order to enforce better privacy and anonymity.",
        "ref_abstract": {
            "@cite_13": {
                "mid": "2021549941",
                "abstract": "Anonymity technologies enable Internet users to maintain a level of privacy that prevents the collection of identifying information such as the IP address. Understanding the deployment of anonymity technologies on the Internet is important to analyze the current and future trends. In this paper, we provide a tutorial survey and a measurement study to understand the anonymity technology usage on the Internet from multiple perspectives and platforms. First, we review currently utilized anonymity technologies and assess their usage levels. For this, we cover deployed contemporary anonymity technologies including proxy servers, remailers, JAP, I2P, and Tor with the geo-location of deployed servers. Among these systems, proxy servers, Tor and I2P are actively used, while remailers and JAP have minimal usage. Then, we analyze application-level protocol usage and anonymity technology usage with different applications. For this, we preform a measurement study by collecting data from a Tor exit node, a P2P client, a large campus network, a departmental email server, and publicly available data on spam sources to assess the utilization of anonymizer technologies from various perspectives. Our results confirm previous findings regarding application usage and server geo-location distribution where certain countries utilize anonymity networks significantly more than others. Moreover, our application analysis reveals that Tor and proxy servers are used more than other anonymity techniques.",
                "doi": "https://doi.org/10.1016/j.comcom.2013.04.009",
                "title": "An overview of anonymity technology usage",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1405.6630",
        "mid": "1499412938",
        "abstract": "We consider the problem of predicting winners in elections, for the case where we are given complete knowledge about all possible candidates, all possible voters (together with their preferences), but where it is uncertain either which candidates exactly register for the election or which voters cast their votes. Under reasonable assumptions, our problems reduce to counting variants of election control problems. We either give polynomial-time algorithms or prove #P-completeness results for counting variants of control by adding deleting candidates voters for Plurality, k-Approval, Approval, Condorcet, and Maximin voting rules. We consider both the general case, where voters' preferences are unrestricted, and the case where voters' preferences are single-peaked.",
        "related_work": "Another model of predicting election outcomes is that of @cite_0 . They consider a situation where each voter is undecided regarding several possible votes. That is, for each voter we are given several possible preference orders and a probability distribution over these votes. The question is, what is the probability that a designated candidate wins.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2140055348",
                "abstract": "We investigate the extent to which it is possible to compute the probability of a particular candidate winning an election, given imperfect information about the preferences of the electorate. We assume that for each voter, we have a probability distribution over a set of preference orderings. Thus, for each voter, we have a number of possible preference orderings - we do not know which of these orderings actually represents the preferences of the voter, but for each ordering, we know the probability that it does. For the case where the number of candidates is a constant, we are able to give a polynomial time algorithm to compute the probability that a given candidate will win. We present experimental results obtained with an implementation of the algorithm, illustrating how the [email protected]?s performance in practice is better than its predicted theoretical bound. However, when the number of candidates is not bounded, we prove that the problem becomes #P-hard for the Plurality, k-approval, Borda, Copeland, and Bucklin voting rules. We further show that even evaluating if a candidate has any chance of winning is NP-complete for the Plurality voting rule in the case where voters may have different weights. With unweighted voters, we give a polynomial algorithm for Plurality, and show that the problem is hard for many other voting rules. Finally, we give a Monte Carlo approximation algorithm for computing the probability of a candidate winning in any settings, with an error that is as small as desired.",
                "doi": "https://doi.org/10.1016/j.artint.2012.04.009",
                "title": "On the evaluation of election outcomes under uncertainty",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1404.2366",
        "mid": "2088312047",
        "abstract": "Device-to-device (D2D) communications in cellular networks are promising technologies for improving network throughput, spectrum efficiency, and transmission delay. In this paper, we first introduce the concept of guard distance to explore a proper system model for enabling multiple concurrent D2D pairs in the same cell. Considering the Signal to Interference Ratio (SIR) requirements for both macro-cell and D2D communications, a geometrical method is proposed to obtain the guard distances from a D2D user equipment (DUE) to the base station (BS), to the transmitting cellular user equipment (CUE), and to other communicating D2D pairs, respectively, when the uplink resource is reused. By utilizing the guard distances, we then derive the bounds of the maximum throughput improvement provided by D2D communications in a cell. Extensive simulations are conducted to demonstrate the impact of different parameters on the optimal maximum throughput. We believe that the obtained results can provide useful guidelines for the deployment of future cellular networks with underlaying D2D communications.",
        "related_work": "For the theoretical analysis of the achievable performance bounds, available results are relatively fewer. In @cite_8 , the uplink capacity gain was derived when one D2D link was enabled in an FDD CDMA-based cellular cell. In @cite_20 , an interference-limited area (ILA) control scheme was proposed to manage the interference from CUEs to a D2D transaction when multiple antennas were used by the BS. By analyzing the coverage of ILA, a lower bound of the ergodic capacity was also obtained for DUEs using uplink cellular radio resources. After that, a similar approach was extended to the downlink resources sharing scenario in @cite_25 . In @cite_9 , the maximum achievable transmission capacity, which was defined as the spatial density of successful transmissions per unit area, was analyzed for the hybrid D2D and cellular network through stochastic geometry. However, due to the inevitable interference accumulated at the BS, most of the existing analytical results assuming a single D2D pair in a cellular network cannot be directly extended to a scenario with multiple coexisting D2D pairs. Therefore, the performance of D2D communications in the latter is still an under-developed issue, which could further improve the spectrum efficiency and increase the cellular network throughput.",
        "ref_abstract": {
            "@cite_9": {
                "mid": "1977993370",
                "abstract": "This paper analyzes the maximum achievable transmission capacity of the D2D communication system under heterogeneous networks. The heterogeneous networks contain two primary systems working on independent bands and D2D communication guarantees the target outage probabilities of both systems on each band. By utilizing stochastic geometry, the effects of the spatial densities and the transmission power allocation ratio on the achievable transmission capacity are presented. Moreover, the optimal transmission density of D2D pairs and the optimal power allocation ratio are derived. The maximum capacity of D2D communication is defined based on the former optimal value from theoretical results. It is shown that the optimal power allocation ratio is proportional to the product of bandwidth, node density and transmission power of two primary systems.",
                "doi": "https://doi.org/10.4108/icst.crowncom.2012.249072",
                "title": "Transmission Capacity of D2D communication under heterogeneous networks with Dual Bands",
                "publication_year": 2012
            },
            "@cite_25": {
                "mid": "2056867735",
                "abstract": "It is quite important for Device-to-Device (D2D) communication to make decision in selecting which part of the celluar radio resource, namely uplink (UL) resource or downlink (DL) resource, to be reused for the purpose of higher system capacity. In this paper, we propose an interference control scheme in DL mode to limit the base station's (BS) interference to D2D user equipments (DUEs) by selecting cellular user equipments (CUEs) nearer to BS. By comparing the interference control scheme used in UL situation, we derived the two lower bounds of ergodic capacity of reusing UL and DL resource in D2D communication. Numeric results show that the system capacity can be optimized by utilizing effective reusing mode selection strategies.",
                "doi": "https://doi.org/10.1109/comnetsat.2012.6380778",
                "title": "Capacity analysis of Device-to-Device resource reusing modes for cellular networks",
                "publication_year": 2012
            },
            "@cite_20": {
                "mid": "2066106876",
                "abstract": "A new interference management strategy is proposed to enhance the overall capacity of cellular networks (CNs) and device-to-device (D2D) systems. We consider M out of K cellular user equipments (CUEs) and one D2D pair exploiting the same resources in the uplink (UL) period under the assumption of M multiple antennas at the base station (BS). First, we use the conventional mechanism which limits the maximum transmit power of the D2D transmitter so as not to generate harmful interference from D2D systems to CNs. Second, we propose a \u03b4D-interference limited area (ILA) control scheme to manage interference from CNs to D2D systems. The method does not allow the coexistence (i.e., use of the same resources) of CUEs and a D2D pair if the CUEs are located in the \u03b4D-ILA defined as the area in which the interference to signal ratio (ISR) at the D2D receiver is greater than the predetermined threshold, \u03b4D. Next, we analyze the coverage of the \u03b4D-ILA and derive the lower bound of the ergodic capacity as a closed form. Numerical results show that the \u03b4D-ILA based D2D gain is much greater than the conventional D2D gain, whereas the capacity loss to the CNs caused by using the \u03b4D-ILA is negligibly small.",
                "doi": "https://doi.org/10.1109/twc.2011.100611.101684",
                "title": "Capacity Enhancement Using an Interference Limited Area for Device-to-Device Uplink Underlaying Cellular Networks",
                "publication_year": 2011
            },
            "@cite_8": {
                "mid": "2037623102",
                "abstract": "We study the uplink capacity gain when Device-to-Device (D2D) links are enabled in a FDD CDMA-based cellular system. In such a system, a direct link for data traffic is established if a communication pair is closely located, and better link quality is expected due to the short D2D transmission distance. We consider a single cell scenario, and assume uplink resource is utilized by D2D communications. We assume the D2D transmitter transmits at a power level no more than it transmits in a cellular mode; in this case no extra interference would be introduced into the CDMA system. Optimal transmit power for benefited D2D links is derived to improve system performance in terms of uplink capacity. Extensive simulations are conducted, which demonstrate that a significant capacity gain is obtained from D2D links. Subsequently, we discuss the performance gain and its affecting factors and try to derive some insights on practical implementation.",
                "doi": "https://doi.org/10.1109/iccsn.2011.6013627",
                "title": "Device-to-Device communication in CDMA-based cellular systems &#x2014; Uplink capacity analysis",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1403.7654",
        "mid": "1530114379",
        "abstract": "The Olympic Games are an important sporting event with notable consequences for the general economic landscape of the host city. Traditional economic assessments focus on the aggregated impact of the event on the national income, but fail to provide micro-scale insights on why local businesses will benefit from the increased activity during the Games. In this paper we provide a novel approach to modeling the impact of the Olympic Games on local retailers by analyzing a dataset mined from a large location-based social service, Foursquare. We hypothesize that the spatial positioning of businesses as well as the mobility trends of visitors are primary indicators of whether retailers will rise their popularity during the event. To confirm this we formulate a retail winners prediction task in the context of which we evaluate a set of geographic and mobility metrics. We find that the proximity to stadiums, the diversity of activity in the neighborhood, the nearby area sociability, as well as the probability of customer flows from and to event places such as stadiums and parks are all vital factors. Through supervised learning techniques we demonstrate that the success of businesses hinges on a combination of both geographic and mobility factors. Our results suggest that location-based social networks, where crowdsourced information about the dynamic interaction of users with urban spaces becomes publicly available, present an alternative medium to assess the economic impact of large scale events in a city.",
        "related_work": "The power of social media for the automated analysis of real-world events has been universally recognized @cite_11 . The large volumes of timely user-generated content in response to public events such as election campaigns allow the extraction of event insights not easily obtainable via alternative means @cite_18 @cite_13 . Social media users act as sensors that empower the development of real-time event detection methodologies @cite_19 @cite_16 . We follow the trends of employing social media for our analysis, and take advantage of location-based services data to automate the impact assessment of a major sports event.",
        "ref_abstract": {
            "@cite_18": {
                "mid": "2180109290",
                "abstract": "Social media channels such as Twitter have emerged as platforms for crowds to respond to public and televised events such as speeches and debates. However, the very large volume of responses presents challenges for attempts to extract sense from them. In this work, we present an analytical method based on joint statistical modeling of topical influences from the events and associated Twitter feeds. The model enables the auto-segmentation of the events and the characterization of tweets into two categories: (1) episodic tweets that respond specifically to the content in the segmentsof the events, and (2) steady tweets that respond generally about the events. By applying our method to two large sets of tweets in response to President Obama's speech on the Middle East in May 2011 and a Republican Primary debate in September 2011, we present what these tweets were about. We also reveal the nature and magnitude of the influences of the event on the tweets over its timeline. In a user study, we further show that users find the topics and the episodic tweets discovered by our method to be of higher quality and more interesting as compared to the state-of-the-art, with improvements in the range of 18-41 .",
                "doi": "https://doi.org/10.1609/icwsm.v6i1.14264",
                "title": "What Were the Tweets About? Topical Associations between Public Events and Twitter Feeds",
                "publication_year": 2021
            },
            "@cite_19": {
                "mid": "11244355",
                "abstract": "Twitter, as a form of social media, is fast emerging in recent years. Users are using Twitter to report real-life events. This paper focuses on detecting those events by analyzing the text stream in Twitter. Although event detection has long been a research topic, the characteristics of Twitter make it a non-trivial task. Tweets reporting such events are usually overwhelmed by high flood of meaningless \u201cbabbles\u201d. Moreover, event detection algorithm needs to be scalable given the sheer amount of tweets. This paper attempts to tackle these challenges with EDCoW (Event Detection with Clustering of Wavelet-based Signals). EDCoW builds signals for individual words by applying wavelet analysis on the frequencybased raw signals of the words. It then filters away the trivial words by looking at their corresponding signal autocorrelations. The remaining words are then clustered to form events with a modularity-based graph partitioning technique. Experimental results show promising result of EDCoW.",
                "doi": "https://doi.org/10.1609/icwsm.v5i1.14102",
                "title": "Event Detection in Twitter",
                "publication_year": 2021
            },
            "@cite_16": {
                "mid": "85157849",
                "abstract": "In this paper we examine the effectiveness of using a filtered stream of tweets from Twitter to automatically identify events of interest within the video of live sports transmissions. We show that using just the volume of tweets generated at any moment of a game actually provides a very accurate means of event detection, as well as an automatic method for tagging events with representative words from the tweet stream. We compare this method with an alternative approach that uses complex audio-visual content analysis of the video, showing that it provides near-equivalent accuracy for major event detection at a fraction of the computational cost. Using community tweets and discussion also provides a sense of what the audience themselves found to be the talking points of a video.",
                "doi": "https://doi.org/10.1609/icwsm.v5i1.14170",
                "title": "Using Twitter to Detect and Tag Important Events in Sports Media",
                "publication_year": 2021
            },
            "@cite_13": {
                "mid": "105055686",
                "abstract": "In this work, we study the use of Twitter by House, Senate and gubernatorial candidates during the midterm (2010) elections in the U.S. Our data includes almost 700 candidates and over 690k documents that they produced and cited in the 3.5 years leading to the elections. We utilize graph and text mining techniques to analyze differences between Democrats, Republicans and Tea Party candidates, and suggest a novel use of language modeling for estimating content cohesiveness. Our findings show significant differences in the usage patterns of social media, and suggest conservative candidates used this medium more effectively, conveying a coherent message and maintaining a dense graph of connections. Despite the lack of party leadership, we find Tea Party members display both structural and language-based cohesiveness. Finally, we investigate the relation between network structure, content and election results by creating a proof-of-concept model that predicts candidate victory with an accuracy of 88.0 .",
                "doi": "https://doi.org/10.1609/icwsm.v5i1.14129",
                "title": "The Party Is Over Here: Structure and Content in the 2010 Election",
                "publication_year": 2021
            },
            "@cite_11": {
                "mid": "37473190",
                "abstract": "Twitter has become exceedingly popular, with hundreds of millions of tweets being posted every day on a wide variety of topics. This has helped make real-time search applications possible with leading search engines routinely displaying relevant tweets in response to user queries. Recent research has shown that a considerable fraction of these tweets are about \"events,\" and the detection of novel events in the tweet-stream has attracted a lot of research interest. However, very little research has focused on properly displaying this real-time information about events. For instance, the leading search engines simply display all tweets matching the queries in reverse chronological order. In this paper we argue that for some highly structured and recurring events, such as sports, it is better to use more sophisticated techniques to summarize the relevant tweets. We formalize the problem of summarizing event-tweets and give a solution based on learning the underlying hidden state representation of the event via Hidden Markov Models. In addition, through extensive experiments on real-world data we show that our model significantly outperforms some intuitive and competitive baselines.",
                "doi": "https://doi.org/10.1609/icwsm.v5i1.14138",
                "title": "Event Summarization Using Tweets",
                "publication_year": 2021
            }
        }
    },
    {
        "aid": "1403.5177",
        "mid": "1642442794",
        "abstract": "We present a supervised-learning algorithm from graph data (a set of graphs) for arbitrary twice-differentiable loss functions and sparse linear models over all possible subgraph features. To date, it has been shown that under all possible subgraph features, several types of sparse learning, such as Adaboost, LPBoost, LARS LASSO, and sparse PLS regression, can be performed. Particularly emphasis is placed on simultaneous learning of relevant features from an infinite set of candidates. We first generalize techniques used in all these preceding studies to derive an unifying bounding technique for arbitrary separable functions. We then carefully use this bounding to make block coordinate gradient descent feasible over infinite subgraph features, resulting in a fast converging algorithm that can solve a wider class of sparse learning problems over graph data. We also empirically study the differences from the existing approaches in convergence property, selected subgraph features, and search-space sizes. We further discuss several unnoticed issues in sparse learning over all possible subgraph features.",
        "related_work": "These two existing, popular approaches are based on subgraph features, which are limited to specific type ones only. In contrast, a series of inspiring studies for has been made . These approaches involve automatic selection of relevant features from during the learning process. Triggered by the seminal paper by @cite_2 , it has been shown that we can perform simultaneous learning of not only the model parameters but also relevant subgraph features from all possible subgraphs in several machine-learning problems such as Adaboost , LARS LASSO , sparse partial-least-squares (PLS) regression , sparse principal component analysis (PCA) , and LPBoost . This paper aims to give a coherent and unifying view to understand how these existing methods work well, and then present a more general framework, which is applicable to a wider class of learning problems.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "1988703566",
                "abstract": "Combinatorial chemistry has generated chemical libraries and databases with a huge number of chemical compounds, which include prospective drugs. Chemical structures of compounds can be molecular graphs, to which a variety of graph-based techniques in computer science, specifically graph mining, can be applied. The most basic way for analyzing molecular graphs is using structural fragments, so-called subgraphs in graph theory. The mainstream technique in graph mining is frequent subgraph mining, by which we can retrieve essential subgraphs in given molecular graphs. In this article we explain the idea and procedure of mining frequent subgraphs from given molecular graphs, raising some real applications, and we describe the recent advances of graph mining.",
                "doi": "https://doi.org/10.1016/j.drudis.2012.07.016",
                "title": "Graph mining: procedure, application to drug discovery and recent advances",
                "publication_year": 2013
            }
        }
    },
    {
        "aid": "1310.7552",
        "mid": "2953161065",
        "abstract": "We explore a fundamental problem of super-resolving a signal of interest from a few measurements of its low-pass magnitudes. We propose a 2-stage tractable algorithm that, in the absence of noise, admits perfect super-resolution of an @math -sparse signal from @math low-pass magnitude measurements. The spike locations of the signal can assume any value over a continuous disk, without increasing the required sample size. The proposed algorithm first employs a conventional super-resolution algorithm (e.g. the matrix pencil approach) to recover unlabeled sets of signal correlation coefficients, and then applies a simple sorting algorithm to disentangle and retrieve the true parameters in a deterministic manner. Our approach can be adapted to multi-dimensional spike models and random Fourier sampling by replacing its first step with other harmonic retrieval algorithms.",
        "related_work": "In particular, our recovery algorithm coincides with @cite_8 @cite_0 @cite_24 when applied to simultaneously sparse and low-rank matrices. When the signal is further assumed to be exactly sparse of sparsity level @math , the pioneering work @cite_2 showed that @math measurements suffice; this result is extended to accommodate sub-Gaussian measurements and approximately sparse signals by our work using a much simpler approach.",
        "ref_abstract": {
            "@cite_0": {
                "mid": "2102019642",
                "abstract": "This paper develops a novel framework for phase retrieval, a problem which arises in X-ray crystallography, diffraction imaging, astronomical imaging, and many other applications. Our approach, called PhaseLift, combines multiple structured illuminations together with ideas from convex programming to recover the phase from intensity measurements, typically from the modulus of the diffracted wave. We demonstrate empirically that a complex-valued object can be recovered from the knowledge of the magnitude of just a few diffracted patterns by solving a simple convex optimization problem inspired by the recent literature on matrix completion. More importantly, we also demonstrate that our noise-aware algorithms are stable in the sense that the reconstruction degrades gracefully as the signal-to-noise ratio decreases. Finally, we introduce some theory showing that one can design very simple structured illumination patterns such that three diffracted figures uniquely determine the phase of the object we wish to...",
                "doi": "https://doi.org/10.1137/151005099",
                "title": "Phase Retrieval via Matrix Completion",
                "publication_year": 2015
            },
            "@cite_2": {
                "mid": "2963855280",
                "abstract": "In this paper we consider a system of quadratic equations @math , where @math is unknown while normal random vectors @math and quadratic measurements @math are known. The system is assumed to be underdetermined, i.e., @math . We prove that if there exists a sparse solution @math , i.e., at most @math components of @math are nonzero, then by solving a convex optimization program, we can solve for @math up to a multiplicative constant with high probability, provided that @math . On the other hand, we prove that @math is necessary for a class of natural convex relaxations to be exact.",
                "doi": "https://doi.org/10.1137/120893707",
                "title": "Sparse Signal Recovery from Quadratic Measurements via Convex Programming",
                "publication_year": 2013
            },
            "@cite_24": {
                "mid": "2169501582",
                "abstract": "This note shows that we can recover any complex vector @math exactly from on the order of n quadratic equations of the form |?a i ,x 0?|2=b i , i=1,?,m, by using a semidefinite program known as PhaseLift. This improves upon earlier bounds in (Commun. Pure Appl. Math. 66:1241---1274, 2013), which required the number of equations to be at least on the order of nlogn. Further, we show that exact recovery holds for all input vectors simultaneously, and also demonstrate optimal recovery results from noisy quadratic measurements; these results are much sharper than previously known results.",
                "doi": "https://doi.org/10.1007/s10208-013-9162-z",
                "title": "Solving Quadratic Equations via PhaseLift When There Are About as Many Equations as Unknowns",
                "publication_year": 2013
            },
            "@cite_8": {
                "mid": "2078397124",
                "abstract": "Suppose we wish to recover a signal amssym @math from m intensity measurements of the form , ; that is, from data in which phase information is missing. We prove that if the vectors are sampled independently and uniformly at random on the unit sphere, then the signal x can be recovered exactly (up to a global phase factor) by solving a convenient semidefinite program\u2013-a trace-norm minimization problem; this holds with large probability provided that m is on the order of , and without any assumption about the signal whatsoever. This novel result demonstrates that in some instances, the combinatorial phase retrieval problem can be solved by convex programming techniques. Finally, we also prove that our methodology is robust vis-a-vis additive noise. \u00a9 2012 Wiley Periodicals, Inc.",
                "doi": "https://doi.org/10.1002/cpa.21432",
                "title": "PhaseLift: Exact and Stable Signal Recovery from Magnitude Measurements via Convex Programming",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1306.6729",
        "mid": "2951313522",
        "abstract": "Recent studies have shown that a significant number of mobile applications, often handling sensitive data such as bank accounts and login credentials, suffers from SSL vulnerabilities. Most of the time, these vulnerabilities are due to improper use of the SSL protocol (in particular, in its phase), resulting in applications exposed to man-in-the-middle attacks. In this paper, we present MITHYS, a system able to: (i) detect applications vulnerable to man-in-the-middle attacks, and (ii) protect them against these attacks. We demonstrate the feasibility of our proposal by means of a prototype implementation in Android, named MITHYSApp. A thorough set of experiments assesses the validity of our solution in detecting and protecting mobile applications from man-in-the-middle attacks, without introducing significant overheads. Finally, MITHYSApp does not require any special permissions nor OS modifications, as it operates at the application level. These features make MITHYSApp immediately deployable on a large user base.",
        "related_work": "Various misuses of the SSL protocol are spread both in the desktop environment and in the mobile environment, exposing private data (potentially sensible) to malicious attacks. In particular, @cite_18 analysed the SSL usage across various environments, only to find out that this protocol's implementation is completely broken in many security-critical applications and libraries''. Meanwhile, @cite_8 analysed the SSL usage on 13,500 Android applications, and found out that a large percentage of them suffer from SSL vulnerabilities, which expose them to dangerous man-in-the-middle attacks. To add it up, some of these applications (such as PayPal and Facebook) are very popular, covering up to 185 million users. Both studies just gave some advices to developers, but did not mention any solution to the SSL usage problem.",
        "ref_abstract": {
            "@cite_18": {
                "mid": "2145994642",
                "abstract": "SSL (Secure Sockets Layer) is the de facto standard for secure Internet communications. Security of SSL connections against an active network attacker depends on correctly validating public-key certificates presented when the connection is established. We demonstrate that SSL certificate validation is completely broken in many security-critical applications and libraries. Vulnerable software includes Amazon's EC2 Java library and all cloud clients based on it; Amazon's and PayPal's merchant SDKs responsible for transmitting payment details from e-commerce sites to payment gateways; integrated shopping carts such as osCommerce, ZenCart, Ubercart, and PrestaShop; AdMob code used by mobile websites; Chase mobile banking and several other Android apps and libraries; Java Web-services middleware including Apache Axis, Axis 2, Codehaus XFire, and Pusher library for Android and all applications employing this middleware. Any SSL connection from any of these programs is insecure against a man-in-the-middle attack. The root causes of these vulnerabilities are badly designed APIs of SSL implementations (such as JSSE, OpenSSL, and GnuTLS) and data-transport libraries (such as cURL) which present developers with a confusing array of settings and options. We analyze perils and pitfalls of SSL certificate validation in software based on these APIs and present our recommendations.",
                "doi": "https://doi.org/10.1145/2382196.2382204",
                "title": "The most dangerous code in the world",
                "publication_year": 2012
            },
            "@cite_8": {
                "mid": "2103370348",
                "abstract": "Many Android apps have a legitimate need to communicate over the Internet and are then responsible for protecting potentially sensitive data during transit. This paper seeks to better understand the potential security threats posed by benign Android apps that use the SSL TLS protocols to protect data they transmit. Since the lack of visual security indicators for SSL TLS usage and the inadequate use of SSL TLS can be exploited to launch Man-in-the-Middle (MITM) attacks, an analysis of 13,500 popular free apps downloaded from Google's Play Market is presented. We introduce MalloDroid, a tool to detect potential vulnerability against MITM attacks. Our analysis revealed that 1,074 (8.0 ) of the apps examined contain SSL TLS code that is potentially vulnerable to MITM attacks. Various forms of SSL TLS misuse were discovered during a further manual audit of 100 selected apps that allowed us to successfully launch MITM attacks against 41 apps and gather a large variety of sensitive data. Furthermore, an online survey was conducted to evaluate users' perceptions of certificate warnings and HTTPS visual security indicators in Android's browser, showing that half of the 754 participating users were not able to correctly judge whether their browser session was protected by SSL TLS or not. We conclude by considering the implications of these findings and discuss several countermeasures with which these problems could be alleviated.",
                "doi": "https://doi.org/10.1145/2382196.2382205",
                "title": "Why eve and mallory love android",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1303.1651",
        "mid": "2952052839",
        "abstract": "Achieving high efficiency with numerical kernels for sparse matrices is of utmost importance, since they are part of many simulation codes and tend to use most of the available compute time and resources. In addition, especially in large scale simulation frameworks the readability and ease of use of mathematical expressions are essential components for the continuous maintenance, modification, and extension of software. In this context, the sparse matrix-matrix multiplication is of special interest. In this paper we thoroughly analyze the single-core performance of sparse matrix-matrix multiplication kernels in the Blaze Smart Expression Template (SET) framework. We develop simple models for estimating the achievable maximum performance, and use them to assess the efficiency of our implementations. Additionally, we compare these kernels with several commonly used SET-based C++ libraries, which, just as Blaze, aim at combining the requirements of high performance with an elegant user interface. For the different sparse matrix structures considered here, we show that our implementations are competitive or faster than those of the other SET libraries for most problem sizes on a current Intel multicore processor.",
        "related_work": "@cite_4 we have analyzed several of these ET implementations in detail and have introduced the notion of SETs and our SET library in particular. @cite_16 we have extended our analysis to more ET-based libraries, focused on the optimization and vectorization capabilities for dense arithmetic, and presented performance results for the CG algorithm, which is fundamental for many applications. In this work we expand our analysis to sparse arithmetic and the sparse matrix-matrix multiplication (spMMM) in particular.",
        "ref_abstract": {
            "@cite_16": {
                "mid": "2043609983",
                "abstract": "Performance is of utmost importance for linear algebra libraries since they usually are the core of numerical and simulation packages and use most of the available compute time and resources. However, especially in large scale simulation frameworks the readability and ease of use of mathematical expressions is essential for a continuous maintenance, modification, and extension of the software framework. Based on these requirements, in the last decade C++ Expression Templates have gained a reputation as a suitable means to combine an elegant, domain-specific, and intuitive user interface with \u201cHPC-grade\u201d performance. Unfortunately, many of the available ET-based frameworks fall short of the expectation to deliver high performance, adding to the general mistrust towards C++ math libraries. In this paper we present performance results for Smart Expression Template libraries, demonstrating that by proper combination of high-level C++ code and low-level compute kernels both requirements, an elegant interface and high performance, can be achieved.",
                "doi": "https://doi.org/10.1109/hpcsim.2012.6266939",
                "title": "High performance smart expression template math libraries",
                "publication_year": 2012
            },
            "@cite_4": {
                "mid": "1665019326",
                "abstract": "In the last decade, expression templates (ETs) have gained a reputation as an efficient performance optimization tool for C++ codes. This reputation builds on several ET-based linear algebra frameworks focused on combining both elegant and high-performance C++ code. However, on closer examination the assumption that ETs are a performance optimization technique cannot be maintained. In this paper we compare the performance of several generations of ET-based frameworks. We analyze different ET methodologies and explain the inability of some ET implementations to deliver high performance for dense and sparse linear algebra operations. Additionally, we introduce the notion of \u201csmart\u201d ETs, which truly allow for a combination of high performance code with the elegance and maintainability of a domain-specific language.",
                "doi": "https://doi.org/10.1137/110830125",
                "title": "Expression Templates Revisited: A Performance Analysis of Current Methodologies",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1204.5443",
        "mid": "2949692813",
        "abstract": "We consider the problem of managing a bounded size First-In-First-Out (FIFO) queue buffer, where each incoming unit-sized packet requires several rounds of processing before it can be transmitted out. Our objective is to maximize the total number of successfully transmitted packets. We consider both push-out (when the policy is permitted to drop already admitted packets) and non-push-out cases. In particular, we provide analytical guarantees for the throughput performance of our algorithms. We further conduct a comprehensive simulation study which experimentally validates the predicted theoretical behaviour.",
        "related_work": "@cite_2 were the first to consider buffer management and scheduling in the context of network processors with heterogeneous processing requirements for the arriving traffic. They study both SRPT (shortest remaining processing time) and FIFO (first-in-first-out) schedulers with recycles, in both push-out and non-push-out buffer management cases, where a packet is recycled after processing according to the priority policy (FIFO or SRPT). They showed competitive algorithms and worst-case lower bounds for such settings. Although they considered a different architecture (FIFO with recycles) than the one we consider in this paper, they provided only a lower bound for the push-out FIFO case, and it remains unknown if it can be attained.",
        "ref_abstract": {
            "@cite_2": {
                "mid": "2141282318",
                "abstract": "Current network processors (NPs) increasingly deal with packets with heterogeneous processing times. In such an environment, packets that require many processing cycles delay low-latency traffic because the common approach in today's NPs is to employ run-to-completion processing. These difficulties have led to the emergence of the Multipass NP architecture, where after a processing cycle ends, all processed packets are recycled into the buffer and recompete for processing resources. In this paper, we provide a model that captures many of the characteristics of this architecture, and we consider several scheduling and buffer management algorithms that are specially designed to optimize the performance of multipass network processors. In particular, we provide analytical guarantees for the throughput performance of our algorithms. We further conduct a comprehensive simulation study, which validates our results.",
                "doi": "https://doi.org/10.1109/infcom.2011.5935167",
                "title": "Providing performance guarantees in multipass network processors",
                "publication_year": 2011
            }
        }
    },
    {
        "aid": "1105.5236",
        "mid": "2952132893",
        "abstract": "Traceroute measurements are one of our main instruments to shed light onto the structure and properties of today's complex networks such as the Internet. This paper studies the feasibility and infeasibility of inferring the network topology given traceroute data from a worst-case perspective, i.e., without any probabilistic assumptions on, e.g., the nodes' degree distribution. We attend to a scenario where some of the routers are anonymous, and propose two fundamental axioms that model two basic assumptions on the traceroute data: (1) each trace corresponds to a real path in the network, and (2) the routing paths are at most a factor 1 alpha off the shortest paths, for some parameter alpha in (0,1]. In contrast to existing literature that focuses on the cardinality of the set of (often only minimal) inferrable topologies, we argue that a large number of possible topologies alone is often unproblematic, as long as the networks have a similar structure. We hence seek to characterize the set of topologies inferred with our axioms. We introduce the notion of star graphs whose colorings capture the differences among inferred topologies; it also allows us to construct inferred topologies explicitly. We find that in general, inferrable topologies can differ significantly in many important aspects, such as the nodes' distances or the number of triangles. These negative results are complemented by a discussion of a scenario where the trace set is best possible, i.e., \"complete\". It turns out that while some properties such as the node degrees are still hard to measure, a complete trace set can help to determine global properties such as the connectivity.",
        "related_work": "In the field of , topologies are explored using pairwise end-to-end measurements, without the cooperation of nodes along these paths. This approach is quite flexible and applicable in various contexts, e.g., in social networks @cite_6 . For a good discussion of this approach as well as results for a routing model along shortest and second shortest paths see @cite_6 . For example, @cite_6 shows that for sparse random graphs, a relatively small number of cooperating participants is sufficient to discover a network fairly well.",
        "ref_abstract": {
            "@cite_6": {
                "mid": "2568950526",
                "abstract": "We consider the task of topology discovery of sparse random graphs using end-to-end random measurements (e.g., delay) between a subset of nodes, referred to as the participants. The rest of the nodes are hidden, and do not provide any information for topology discovery. We consider topology discovery under two routing models: (a) the participants exchange messages along the shortest paths and obtain end-to-end measurements, and (b) additionally, the participants exchange messages along the second shortest path. For scenario (a), our proposed algorithm results in a sub-linear edit-distance guarantee using a sub-linear number of uniformly selected participants. For scenario (b), we obtain a much stronger result, and show that we can achieve consistent reconstruction when a sub-linear number of uniformly selected nodes participate. This implies that accurate discovery of sparse random graphs is tractable using an extremely small number of participants. We finally obtain a lower bound on the number of participants required by any algorithm to reconstruct the original random graph up to a given edit distance. We also demonstrate that while consistent discovery is tractable for sparse random graphs using a small number of participants, in general, there are graphs which cannot be discovered by any algorithm even with a significant number of participants, and with the availability of end-to-end information along all the paths between the participants. \u00a9 2012 Wiley Periodicals, Inc. Random Struct. Alg., 2013",
                "doi": "https://doi.org/10.1002/rsa.20420",
                "title": "Topology discovery of sparse random graphs with few participants",
                "publication_year": 2012
            }
        }
    },
    {
        "aid": "1105.5032",
        "mid": "2951177312",
        "abstract": "Many electoral bribery, control, and manipulation problems (which we will refer to in general as \"manipulative actions\" problems) are NP-hard in the general case. It has recently been noted that many of these problems fall into polynomial time if the electorate is single-peaked (i.e., is polarized along some axis issue). However, real-world electorates are not truly single-peaked. There are usually some mavericks, and so real-world electorates tend to merely be nearly single-peaked. This paper studies the complexity of manipulative-action algorithms for elections over nearly single-peaked electorates, for various notions of nearness and various election systems. We provide instances where even one maverick jumps the manipulative-action complexity up to @math -hardness, but we also provide many instances where a reasonable number of mavericks can be tolerated without increasing the manipulative-action complexity.",
        "related_work": "The four papers most related to the present one are the following. insightfully raised the idea that general complexity results may change in single-peaked societies. His manipulative-action example (STV) actually provides a case where single-peakedness fails to lower manipulation complexity, but in a different context he did find a lowering of complexity for single-peakedness. The papers , the first of which was in the TARK conference, then broadly explored the effect of single-peakedness on manipulative actions. These three papers are all in the model of (perfect) single-peakedness. , in the context of preference elicitation, raised and experimentally studied the issue of single-peaked societies. also discussed nearness to single-peakedness, and the papers @cite_3 @cite_2 both raise as open issues whether shield-evaporation (complexity) results for single-peakedness will withstand near-single-peakedness. The present paper seeks to bring the nearly single-peaked'' lens to the study of manipulative actions.",
        "ref_abstract": {
            "@cite_3": {
                "mid": "1983312130",
                "abstract": "Much work has been devoted, during the past 20years, to using complexity to protect elections from manipulation and control. Many ''complexity shield'' results have been obtained-results showing that the attacker's task can be made NP-hard. Recently there has been much focus on whether such worst-case hardness protections can be bypassed by frequently correct heuristics or by approximations. This paper takes a very different approach: We argue that when electorates follow the canonical political science model of societ al preferences the complexity shield never existed in the first place. In particular, we show that for electorates having single-peaked preferences, many existing NP-hardness results on manipulation and control evaporate.",
                "doi": "https://doi.org/10.1016/j.ic.2010.09.001",
                "title": "The shield that never was: Societies with single-peaked preferences are more open to manipulation and control",
                "publication_year": 2011
            },
            "@cite_2": {
                "mid": "2233988666",
                "abstract": "For many election systems, bribery (and related) attacks have been shown NP-hard using constructions on combinatorially rich structures such as partitions and covers. This paper shows that for voters who follow the most central political-science model of electorates-- single-peaked preferences--those hardness protections vanish. By using single-peaked preferences to simplify combinatorial covering challenges, we for the first time show that NP-hard bribery problems--including those for Kemeny and Llull elections--fall to polynomial time for single-peaked electorates. By using single-peaked preferences to simplify combinatorial partition challenges, we for the first time show that NP-hard partition-of-voters problems fall to polynomial time for single-peaked electorates. We show that for single-peaked electorates, the winner problems for Dodgson and Kemeny elections, though \u03982p-complete in the general case, fall to polynomial time. And we completely classify the complexity of weighted coalition manipulation for scoring protocols in single-peaked electorates.",
                "doi": "https://doi.org/10.1613/jair.4647",
                "title": "Bypassing Combinatorial Protections: Polynomial-Time Algorithms for Single-Peaked Electorates",
                "publication_year": 2015
            }
        }
    },
    {
        "aid": "1906.00410",
        "mid": "2947469668",
        "abstract": "Domain randomization (DR) is a successful technique for learning robust policies for robot systems, when the dynamics of the target robot system are unknown. The success of policies trained with domain randomization however, is highly dependent on the correct selection of the randomization distribution. The majority of success stories typically use real world data in order to carefully select the DR distribution, or incorporate real world trajectories to better estimate appropriate randomization distributions. In this paper, we consider the problem of finding good domain randomization parameters for simulation, without prior access to data from the target system. We explore the use of gradient-based search methods to learn a domain randomization with the following properties: 1) The trained policy should be successful in environments sampled from the domain randomization distribution 2) The domain randomization distribution should be wide enough so that the experience similar to the target robot system is observed during training, while addressing the practicality of training finite capacity models. These two properties aim to ensure the trajectories encountered in the target system are close to those observed during training, as existing methods in machine learning are better suited for interpolation than extrapolation. We show how adapting the domain randomization distribution while training context-conditioned policies results in improvements on jump-start and asymptotic performance when transferring a learned policy to the target environment.",
        "related_work": "@cite_5 present an empirical study of generalization in Deep-RL, testing interpolation and extrapolation performance of state-of-the-art algorithms when varying simulation parameters in control tasks. The authors provide an experimental assessment of generalization under varying training and testing distributions. Our work extends these results by providing results for the case when the training distribution parameters are learned and change during policy training.",
        "ref_abstract": {
            "@cite_5": {
                "mid": "2898436992",
                "abstract": "Deep reinforcement learning (RL) has achieved breakthrough results on many tasks, but has been shown to be sensitive to system changes at test time. As a result, building deep RL agents that generalize has become an active research area. Our aim is to catalyze and streamline community-wide progress on this problem by providing the first benchmark and a common experimental protocol for investigating generalization in RL. Our benchmark contains a diverse set of environments and our evaluation methodology covers both in-distribution and out-of-distribution generalization. To provide a set of baselines for future research, we conduct a systematic evaluation of deep RL algorithms, including those that specifically tackle the problem of generalization.",
                "doi": "https://doi.org/10.48550/arxiv.1810.12282",
                "title": "Assessing Generalization in Deep Reinforcement Learning",
                "publication_year": 2018
            }
        }
    }
]